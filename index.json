[{"categories":["pytorch","python","distributed_training"],"content":"Distributed Training ","date":"08-11-2022","objectID":"/blog/distributed_training/:0:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Why? Need more compute power to process large batches in parallel (DDP) Uses collective communication Large model that couldn’t be fit in memory of one GPU (RPC) Uses P2P communication All of the above XD ","date":"08-11-2022","objectID":"/blog/distributed_training/:1:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DDP in Pytorch Every GPU has a model replica, controlled by a process. Every process fetches different batch of data. Forward. Overlapping between computation of and communication(broadcast - allreduced) of gradient. Validation ","date":"08-11-2022","objectID":"/blog/distributed_training/:2:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"4 steps-recipe to Distributed Training ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Initialize Distributed Group init_process_group(backend='nccl') ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Data Local Training # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) Distributed Training # Create distributed sampler pinned to rank sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) # May be True # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, # Must be False! sampler=sampler) ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Model Local Training # Initialize the model model = create_model() Distributed Training # Initialize the model model = create_model() # Create CUDA device device = torch.device(f'cuda:{rank}') # Send model parameters to the device model = model.to(device) # Wrap the model in DDP wrapper model = DistributedDataParallel(model, device_ids=[rank], output_device=rank) ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:3","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Saving and Logging if rank == 0: torch.save(model.module.state_dict(), 'model.pt') ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:4","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DP vs DDP ","date":"08-11-2022","objectID":"/blog/distributed_training/:4:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DP: Can’t scale to multiple machines Single Process, multiple threads Module is replicated on each device, and the gradients are all summed into the original module Doesn’t give the best performance, as a result of the GIL problem with multi-thread applications in python ","date":"08-11-2022","objectID":"/blog/distributed_training/:4:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DDP: Can be used for Single machine, multiple GPUs training, or for multi-node training It initiates process for every device (eg. 2 nodes, with 4 GPUs each = 8 processes) Gradients are gathered using “all_reduce” operation It’s advised to use DDP for any distributed training ","date":"08-11-2022","objectID":"/blog/distributed_training/:4:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Torch.Distributed.Launch vs Torchrun ","date":"08-11-2022","objectID":"/blog/distributed_training/:5:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Distributes Launch scripts: We need to run the script on every node, with the correct ranks We have to pass it all necessary environment variables Example #!/bin/bash MASTER=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1) #Launch the pytorch processes, first on master (first in $HOSTLIST) then on the slaves RANK=0 for node in $HOSTLIST; do echo \"RAnk is \"$RANK srun --nodes=1 --gpus=$GPUS_PER_NODE \\ python -m torch.distributed.launch \\ --nproc_per_node=$GPUS_PER_NODE \\ --nnodes=$NNODES \\ --use_env \\ --node_rank=$RANK \\ --master_addr=${MASTER} \\ --master_port=$PORT \\ train.py \\ arg1 \\ arg2 \u0026 sleep 1 echo \"node is\" $node echo \"node id is \"$RANK RANK=$((RANK+1)) done wait ***Watch out for “\u0026” *** ","date":"08-11-2022","objectID":"/blog/distributed_training/:5:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Torchrun: We run the script only once, and it runs it on all nodes It adds all environment variables, and we can use them directly in the code Example: srun torchrun \\ --nnodes=2 \\ --nproc_per_node=$GPUS \\ --rdzv_id=123456 \\ // worker group ID --rdzv_backend=c10d \\ // Communication library --rdzv_endpoint=${MASTER} \\ test.py Overall, Torchrun removes a lot of mundane steps. ","date":"08-11-2022","objectID":"/blog/distributed_training/:5:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Hugging Face ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Option 1: using HF trainer You can use HF trainer, but in this case, you need to manually run the training script on every node ( torch.distributed launch with for loop, or using torchrun once) ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Option 2: using HF accelerator You just need to run the script using the accelerate library. You need to create the training loop manually, and can’t use HF trainer then ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"PyTorch Lightning PyTorch Lightning is the easiest in running distributed training We pass in the number of nodes, and number of GPUs per node to the trainer Calls your script internally multiple times with the correct environment variables ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:3","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Common errors: ","date":"08-11-2022","objectID":"/blog/distributed_training/:7:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Dict names issue: Problem: When we wrap our model with DDP, pytorch adds (module.dict_key) for all keys in the state_dict Solution: We need to add a function, that detect if we are running distributed training or not, and add or delete “module” from all keys accordingly ","date":"08-11-2022","objectID":"/blog/distributed_training/:7:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["life","python","programming"],"content":"Some facts that I learned by time","date":"05-23-2022","objectID":"/blog/programming_facts/","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Intro this is intended to be some kind or reference to go through whenever I face some kind of bug, or error, that I don’t know how to solve.usually these kinds of error that doesn’t make sense, or we don’t know the cause of them, and the worst it, that we don’t find many ppl facing the same issue, so internet can’t be so useful then. I learned theses facts the hard way, spending so much time trying to figure out the root of the issue. Debugging Facts here are some tips, to help whenever you are facing some error, trying get a package to work, or you these kinds of error that takes you life few days, you know. ","date":"05-23-2022","objectID":"/blog/programming_facts/:0:0","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Package installation issues ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:0","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Python version this might sound trivial, but every package version, only works with some python versions so you should run pip install package-name== to get the package versions supported by your python version ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:1","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Update Pip whenever there’s a dependency conflict, or versions conflict, and you can’t install a package, then check your pip and update it if possible I spent like 4 days clueless why a simple package that I installed a week before, won’t install now, like out of a sudden, it won’t install anymore, due to dependency packages conflict when I upgraded pip, it simply worked ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:2","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Check for broken installations many times we would try to install some something, then for whatever reason, it wouldn’t complete successful. But then we would try to install again, and we would encounter strange errors, that we don’t know the root for them. there error could be because there’s a broken installation, that messed things up. so, when we delete, or remove this broken installation, and install again, it just works. ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:3","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["deeplearning","python"],"content":"A Summary of DL papers","date":"04-22-2022","objectID":"/blog/papers/","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Decoupled Neural Interfaces using Synthetic Gradients In NN, the training process, has 3 bottle-necks forward lock: you need to calculate teh output of the previous layer before you can can go into next layer in forward pass backward pass: the same, but for backward propagation weights lock: you can’t update weights unless you do for weights in next layer the paper trying to unlock these bootle-necks by decoupling each layer, to be sufficient alone it does that by introducing, a Synthetic Gradient Model, that can predict the gradient for the current layer, without waiting for the gradient of the next layer this was we can calculate gradient and update weights as soon as we calculate the activation of the current layer ","date":"04-22-2022","objectID":"/blog/papers/:1:0","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Synthetic Gradient Model can be just a simple NN that is trained to output the gradient of the layer it can be trained using the true gradient, or even the synthetic gradient of the next layer it’s important that the last layer computes the true gradient, as in the end we must have a ground truth to can calculate a true loss, and the NN would actually train we can have also synthetic model for forward pass, that works with the same idea ","date":"04-22-2022","objectID":"/blog/papers/:1:1","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"A Roadmap for Big Models We are in the Era of Big Models Model generalization is hard, models trained on certain data domain, doesn’t scare to other Datasets creation, and high research tasks, made it hard for small companies to train task-specific models Big models solve thees issues. ","date":"04-22-2022","objectID":"/blog/papers/:2:0","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Big Models Big-data driven Multi-task Adaptive can fine-tuned with few-shot learning Data issues data bias data duplication data has to cover all domains low quality data hard to create huge datasets ","date":"04-22-2022","objectID":"/blog/papers/:2:1","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Knowledge a new way to represent data we represent knowledge as knowledge graphs KG consists of: Instances, Relation, Concept, and Values KG can be created using : experts, wiki-based knowledge graphs, or extracted from unstructured texts KG Completion and Integration most of the known KGs has many fields empty, and there’s a going research in how to deal with that and fill the gaps. some methods try to do that using intra-graph knowledge augmentation or with inter-graph. ","date":"04-22-2022","objectID":"/blog/papers/:2:2","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Denoising Diffusion Probabilistic Models Forward diffusion process: gradually keep adding noise to the original image till it’s destroyed the main task is to reverse the noising procedure, so then we can learn the underlying data distribution, then we can generate images from it instead of calculating the steps of the forward diffusion process sequently, we can combine all the steps in one step, by sampling from a distributuion which have mean of the product of all means in each step $\\begin{aligned} q(x_t | x_0) = x_t \\sim \\mathcal{N}( \\sqrt{\\bar \\alpha} x_0 , (1 - \\bar \\alpha ) \\mathcal{I})\\end{aligned}$ ","date":"04-22-2022","objectID":"/blog/papers/:3:0","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["git","version-control"],"content":"Trying to get familiar and understand git more ","date":"03-15-2022","objectID":"/blog/git/","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Beatiful commands git log --oneline --decorate --all --graph git merge --abort ==\u003e abort merge, and get back like it never happened git reset --hard ==\u003e is your way to lose all uncommited work in your working directory git fast forward is basically that git moves the commit pointer upward to the new posotion, without creating a merge commit or anything you can merge with --no-ff flag, to disable the fast forward merge and force git to create the merge commit ","date":"03-15-2022","objectID":"/blog/git/:1:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Git Bisect used when something broke, and you know what did broke, but you can’t figure out when did it broke you just give it a testing criteria to test the commit history against ","date":"03-15-2022","objectID":"/blog/git/:1:1","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Methodology everything inside git is an object all your local branches are located in .git/refs/heads a branch is basically a file that appoints to a commit. a branch is bisacally a pointer to specific commit every commit has a parent, so to assemble branches we follow and compute their parents ","date":"03-15-2022","objectID":"/blog/git/:2:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Commits keep added changes in commits related to the same topic add informative commit message you can add parts of changes in a single file using -p flag in git add -p filename0 ","date":"03-15-2022","objectID":"/blog/git/:3:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Branching ","date":"03-15-2022","objectID":"/blog/git/:4:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Long-running branches Main branch Dev branch ","date":"03-15-2022","objectID":"/blog/git/:4:1","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Short-lived branches features branches bug fixes branches ","date":"03-15-2022","objectID":"/blog/git/:4:2","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Merging When the one of the two branches has the head is the same as the common ancesstor of the two branches, then we can do a fast-forward merge by putting the commits of the another branch on top the common ancesstor commit ","date":"03-15-2022","objectID":"/blog/git/:5:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Rebase rebase puts the commits of the second brach on top of the common ancesstor commit then rebase the commits of the first branch on top of the last commit of the first branch, then it changes the history of commits Only use rebase to clean local commit history, don’t use rebase on commits that is pushed to online ","date":"03-15-2022","objectID":"/blog/git/:6:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["deeplearning","python","NLP"],"content":"Post for Stanford NLP Course","date":"03-08-2022","objectID":"/blog/stanford_nlp_cs224n/","tags":["deeplearning","python","NLP"],"title":"Stanford CS224N NLP with Deep Learning","uri":"/blog/stanford_nlp_cs224n/"},{"categories":["deeplearning","python"],"content":"Long Course that gives a good breadth over many Deep Learning subjects. It covers topics by reviewing the research papers ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Reference Course Repo with the slides, and course info ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:1:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Deep Learning overview we can look at deep learning as an algorithm that writes algorithms, like a compiler in this case the source code would be the data: (examples/experiences) excutable code would be the deployable model Deep: Functions compositions $ fl f{l-1} …. f_1$ Learning: Loss, Back-propagation, and Gradient Descent $ L(\\theta) \\approx J(\\theta)$ –\u003e noisy estimate of the objective function due to mini-batching. That’s why we call it stochastic Gradient Descent why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it’s N*N, so it’s computationally expensive and slow ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:2:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Optimizers to make gradient descent faster, we can add momentum to it. another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum RMSprop: A mini-batch version of rprop method. the original rprop can’t work with mini batches, as it doesn’t consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there Adam: can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta) can also has momentum for all parameter wich can lead to faster convergence Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:2:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Dropout A simple method to prevent the NN from overfitting CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. Computer Vision ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:3:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Image Classification ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Large Networks Network In Network the main idea is to put a network inside another network they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers this idea is bisacally a (one to one convution) they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels from the last conv layer to form the output layer one by one convolution is a normal convolution with fliter size of 1 by 1 in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant) we can achieve local invariant with pooling, and deal with global invariant with data augmentation VGG Net Local Response Normalization: the idea is to normalize a pixel across nearing channels after comparing nets with lrn and nets without, they didn’t find big difference, so they stoped using it Data Augmentation Image translations( random crops), and horizontal reflection altering the intensities of the RGB channels scale jittering GoogleNet You stack multiple inception modules on top of each ohter the idea is that you don’t have to choose which filter size to use, so why don’t use them all to make the network more efficient, they first projected the input with one by one convolution then applied the main filters you concatinate the many filters through the channel dimension Batch Normalization The main goal of batch normalization is to redude the Internal Covariant Shift we can just normalize the inputs and it would work fine the problem is that in each following layer, and statistics of its output would depend on its weights so we also need to nomalize the inputs in hidden layers here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen in inference we can’t have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset conv layers for conv layers we apply normalization across every channel for every pixel in the batch of images the effective bach size would be ==\u003e mpq where m is the number of images in the batch and p,q are the image resolution Benifits of batch norm: you can use higher learning rate, as the training is more stable less sensitive to initialization less sensitive to activation function it has regularization effects, because thre’s random mini batch every time preserve gradient magintude ?? maybe –\u003e because the jacobian doesn’t scale as we scales the weights Parametric Relu: $ f({y_i}) = \\max(0,y_i) + a_i \\min(0, y_i) $ if $a_i = 0$ –\u003e Relu if $a_i = 0.01$ –\u003e Leaky Relu the initialization of weights and biases depends on the type of activation function Kaiming Initialization (I didn’t fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations): professor went into deep mathematical details into how to choose the intial values for weights the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we want the weights to have values centred around 1 Label smoothing regularization the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset ResNet The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection. Identity mapping in resnets the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes Wide Residual Networks a","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Small Networks Knowledge Distillation the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter T that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference. Network Pruning: all connections with weights below a threshold are removed from the network weight are sparse now then we can represent them using fewer bits Quantization we basically cluster our weight to some centroids the number of centroids for conv layers are more than the ones for FC layers why: because conv layer filters are already sparse, we need higher level of accuracy in them FC layers are so dense that we can tolerate fewer quantization levels Huffman Coding store the more common symbols with more bits Squeeze Net the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made the main idea is to use one by one comvultion to reduce the dimensionality XNOR-NET the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==\u003e $W = \\alpha * B $ where alpha is postative 32 bit constant and B is a binary matrix then mean we try to train by using a means square error loss function of the original weights and alpha and B I still can’t fully understand how to binarize the input Mobile Nets the idea is to reduce computation complexity by doing c onv for each channel separately, and not across channels. so we use number of filters as the same as the input channels but then we will end up with output size as the input size, so we still need to do one by one convolution to output the correct size Xception unify the filters sizes for the inception, and then apply them for each channel separately, then do one by one convolution to fix the output size Mobile Net V2 the same as MobileNet, but with Residuals connections. ShuffleNet the idea is to shuffle channels after doing a group convolution ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Auto ML the question is can we automate architicture engineering, as we automated feature engineering in DL? we can use RNN to output a probability, to sample an architicture from, then use train using this arch, and give the eval acc, as a feedback to the RNN Regularized Evolution it’s basically random search + selection at first you randomly choose some architecture train, and eval on it and push it to to the population then you sample some arch. from the population then u select the best acc model from your samples , and then mutate it (ie. change some of its arch.), then add it to your samples then remove the oldest arch. in the population you keep repeating this cycle till you evolve for C cycles (history size reaches the limit) and report the best arch. EfficientNet the idea is that we do grid seach on a small network to come with the best depth scaling coefficient d, width scaling coefficient w, and resolution scalling coefficient r, then we try to find scaling parameter $\\phi$, that gives the best accuracy while maintaning the flops under the limit ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Robustness The main goal is to make your network robust against adverarial attacks Intrigiong peroperties of neural networks there’s nothing special about individual units, and the individual features that the network learn, and they you can interpret any random direction. So, the entire spacd matters neural networks has blind spots, this means you can add small pertirbations to an image, they are not noticable to the human eye, but they make the network wrongly classify the image Adversiral examples tend to stay hard even for models trained with different hyper-parameters, or ever for different training datasets you can train your network to defend against attacks but that’s expensive, as: first, you have to train your network, then train it again to find some adversiral attacks, then add those examples to the training set, and finally train for a third time. small perturbation to the image, leads to huge perturbation to the activation, due to high dimensionality untargeted adversiral examples fast gradient sign: using the trick of the sign of the loss gradient, and add it to the original image to generate an adversiral example then you can just add a weighted loss, one for the orginal example, and another for the adversiral one, so that the network would be more robust to adversiral examples Towards Evaluating the Robustness of Neural Networks another way to generate targetted adversiral examples is: to choose a function that forces the network to make the logits for the targeted example the biggest, so that this class is selected. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:4","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Visualizing \u0026 Understanding now we want to debug our network, to understand how it works so we want do a backward pass, by inverting our forward pass, but then we habe a problem with pooling layers as we subsamples the input. so we store the locations for the max pixels that we choose in our pooling operation, so that we can upsample the input again in the backward pass. we call these max locations, switches the main idea is, visualising the feature maps, gonna help you modify the network you can have two models that have the same output for the same input but which one do you trust more? to answer that, you need to see which features each one of them focuses on, so if one of them focuses on features that are important to classfication, then this model is more trustworthy LIME: Local Interpretable Model-agnsortic Explanations you want to trust the model, meaning that you wanna make sure the model prioritized the important features but you can’t interpret non linear models, so the idea is to make a locally linear model, that have the same output for your local input example, then use this linear model to get the features that the model prioritized Understanding Deep Learning Requires Rethinking Generalization NN are powerful enough to fit random data, but then it will not generalize for test data so when we introduce radom labels, random pixels, etc: we still can go for 0 train loss, but for test data, the error is gonna be equal to random selection. so, this means: The model architecture itself isn’t a sufficient regularizer. Explicit regularization: dropout, weight decay, data augmentation Implicit regularization: early stopping there exist a two-layer NN with Relu activation, that can fit any N random example ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:5","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Transfer Learning labled data is expensive you split a data set in half,we find that transfer learning for the same task, have higher acc' transfer learning with fine-tuned weight is better than locking the learned weights on average you just wanna cut the network in he middle and start learingn after few layers, as the first few layers ar more general leayers and can acctually help you in traninge for another task DeCAF first layers learn low-level features, whereas latter layers learn semantic or high-lebel features ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:6","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Image Transformation ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Semantic Segmentation you want to segments different classes in the image The fully connected layers can also be viewed as convoluting with kernels that cover their entire input regions. Atrous Convolution: you don’t wanna lose much info when you do conv, and then upsample again, so you fill your filter with holes, so that you lose less info reduce the degree of signal downsampling CRF: deals with the reduced localization accuracy due to the Deep Convolution NN invariance Dilated Convolution: basically atrous convolution increases the size of the receptive points layer by layer ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Image Super-Resolution we want to develope a NN that can up-sample images we can do that using convolution and in the middle we use one to one convolution to work as non-leaner mapping ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Perceptual Losses mse isn’t the best for images, for example, if we shift an image by one pixel in any direction, we will end up with huge loss, while the two images are the same the idea it to use a CNN like VGG-16 to calculate the loss, this works because any CNN would have some perceptual understanding of the images so we push the output of our model, and the target (label) through a NN, and compare the feature maps on different layers Single Image Super-Resolution(SISR) the idea is to make the network to only learn the residual not the full image, so it just learns the difference between the two images ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Object Detection ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:6:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Two Stage Detectors R-CNN we enter the input image into extract regions algorithm. this algorithm is cheap algorithm that output millions of boxes per image. we do that using an algorithm called “selective search” we then enter that to a CNN to do features extractions at the end we have a per-class classifier Spatial Pyramid Pooling the idea is that we use spatial pyramid pooling to have a fixed length representation for the image also we push the input image once through the conv layers, then choose multiple windows after to do the classification for. this way we cut so much on computations cause we for the first few conv layers, we pushed just one image Fast R-CNN just like RCNN but, changed the multi-class SVM with multi-task loss, this way we don’t have to calculate many classifiers, one for each class. also we don’t need bounding box proposals, and we can acc train a Region Proposal Network, to propose bounding boxes for every pixel in the feature map. last trick is to use a CNN instead of the FC head at the end of the network, but CNN is translation invariant, so we need to do pooling for each region separately. Feature Pyramids the idea is that we need to use different versions for our input image, each with different resolution, so that we detect objects with different sizes. to do that we can use the different features maps at different layers, so that at each layers the resolution changes, and we can use that to choose our windows the problem is that each layer represent a different semantic meaning of the image, so the first few layers consider the image colors, while the last few consider the more complex shapes of the image to overcome this, from each layer we add a connection to the layer below so we up sample the feature maps first then do one by one convolution to adjust the number of channels, ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:6:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"One Stage Detectors YOLO we want the detector to be realtime, so we can detect objects live divide the input image into S * S grid if the center of an object fell inside a cell, that cell is the one responsible to detect that object each grid cell gonna predict, B bounding boxes, each with confidence score SSD: Single Shot MultiBox Detector we want to take the speed from YOLO, and the high acc from the two-stage detectors unlike YOLO, we can use early layers, not just the last layers of the network, and for each one we can predict more boxes, so we end with much more boxes than YOLO YOLO9000 - YOLO V2 Tries to improve upon YOLOv1 using idead from fast-CNN and SSD we can use higher res images in training can the anchor boxes from the training images not just randomly introduced passthrough layer used hierarchical classification to extend many classes ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:6:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Video ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Large-scale video classification we would have a context stream which learns features on a low res frames. and a Fovea Stream, which operates on a high res middle portion of the frames Early Fusion the idea is to take a few frames from the middle of the clip, and apply conv on them, the only diff is that we add a new dimension to filters which coreespond to the number of frames that’s just for the first layer, but then it’s normal conv Late Fusion we have two separate single-frame networks, each one takes a diff frame from the clip, and we concatenate them in the end ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Two-Stream CNN for action recognition video can be decomposed into spatial and temporal components Optical flow stacking we can just follow pixels from frame to another, and then create a flow vectors, in the x,y axis then we can stack these flow vectors Trajectory stacking follow the point from frame to another ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Non-local Neural Network the idea is that we want to see for output pixel, which areas did it pay attention to in the input so we attention every output with all possible pixels in the input if we are using it with videos, then we add another dimension for the time ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Group Normalization Batch norm, is good as long as we have reasonable batch size but whe we have very small batch size, then batch norm isn’t the best Here’s diff between normalizaiton methods: Natural Language Processing ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:4","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Word Representation ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:8:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Distributed Representation of Words and Phrases and their Compositionality Word2Vec ( Efficient Estimation of Word Representation in Vector Space ) using CBOW model, or skip-gram model uses the cosine-similarity distance function Skip-gram Model Continuous Bag of Words is the opposite of the skip-gram in the sense of what are we predicting (word vs context) the idea is that you pick a word, and try to predict the context around it so you have a word in the middle and try to predict words around in (before, and after), given a defined window size and our objective is to maximize the liklihood of the context given the reference word we can use binary trees, to do an approximation, and speed up the softmax caculation, as for every word in the vocab, we would calculate it’s softmax with all other words,but now we can use binary trees, and do that in just log(n), using an approximation, that we group words together, and in each level coming from the root, we go right or left, till we reach the word in the leaves we can make this even faster, using huffman encoding to assign shorter paths for more frequent words noise sampling: the idea is to give the model negative samples, that doesn’t appear together, and give it low probability Evaluation we can evaluate the model, using syntactic, and semantic analogies for example, Berlin to Germany is like France to Paris GloVe: Global Vectors for Word Representation the idea is to use: global matrix factorization methods local context window methods we compute a co-occurrence method, that holds the counts every two words come after each other, we try to learn two matrieces and two biases, that log(X) = w1 * w2 + b1 + b2 ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:8:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Text Classification ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:9:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Recursive deep models for semantic compositionality over sentiment Treebank the dataset is presented as a tree with leafs as words the idea is that for evert word we get it’s embedding, then we multiply that by a weight matrix, and apply softmax, so then we have probability distribution over our classes (sentiment classes) then we can concatenate every words together, and keep recursing till we finish the whole sentence the problem with this model, is that we are losing the pair wise interaction between the two words, wat we can do it introduce a tensor V, that would capture this interaction ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:9:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"CNN for Text Classification we want to use CNNs with text, so we would have some filters the idea is to treat sentences as one dimensional vector, and then we can apply windows that contain bunch of words to some filters, and aggregate them ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:9:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Doc2Vec as we have representation of words, we can also have the same for sentences, or documents ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:10:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Bag of words for each sentence, count the frequency of each word in your vocab weakness lose ordering or words lose semantics or words “powerful” should be closer to “strong” than “Paris” ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:10:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Paragraph vector for every paragraph, we would have a vector representing it, then we can average those together, and try to get the target paragraph we can do it as CBOW, and instead of words, we would have paragraphs ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:10:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"FastText the idea is that we take a sentence(a bag) of words, or N-grams and then sum their vectors together, then project them to latent space, and then project them again to output space, and apply non-linearity (softmax for example), then apply cross-entropy as a loss function we can also normalize our bag of features (the word representation), so we down weight most frequent words instead of softmax, we can use hierarchial softmax, to decrease the training time this model is super fast, and gives results similar to non-linear complicated models like CNNs ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:11:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Hierarchial Attention Networks for Document Classification the idea is that we want to do document classification so in the end we want to represent the document by a vector, that we can enter to softmax, and then output a class we can use think of document as they are formed of sentences, and sentences are formed of words so we can use GRU based sequence encoder to represent words and then sentences ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:12:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"GRU we have a reset gate, and update gate the idea is that at every step we have a previous hidden output, and a current input then we have an update gate that determines the percentage to take from the current hidden output, vs the previous hidden output. then we also have a reset gate, which determines how much we wanna take from the previous state when calculating the current state we use the tanh function to calculate the hidden state we use the sigmoid to calc the parameter Z, which tell us the percentage between the current state, and the previous state output we can have a forward, and a backward GRU, and concat them then we project these concat words representation, and apply non-linearity then to calculate the sentence vector out of these word vectors, we apply a weighted average on them. this works like a simplified version of attention we can do this weighted average using softmax, but first we need to turn this vector to a scaler, which we can do by applying dot product with “query” or “word context”, like we are doing a query: what is the informative word this will get us with the alphas, which tell us how much we take from each word vector NOTE: cross-entropy with one-hot vector is the same as the log-liklihood ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:12:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Neural Architecture for Named Entity Recognition ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:13:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"LSTM-CRF Model Normal LSTM has 3 gates, input gage, forget gate, output gate. Conditional Random Field in NER, the named tags are dependant on each other, for example: B tag, and I tag. so we want to account for that in our loss to do that we introduce a new compatibility matrix, to count for this dependency Character-based models of words we need it cause, in production, we might encounter new unseen words, so we make up for that using the character encoding we want to add character representation with our words representation so we do a character-based bi-LSTM and we concatenate the output of the LSTM, with our word representation from the lookup table ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:13:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Universal Language Model fine-tuning for Text Classification (ULMFiT) Language Model: a model that trying to give an understanding for language. Like given few words of the sentence, can we guess the next word. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Disctimonative fine-tuning so the idea is to split the model pre-trained parameters for each layer and to also choose a learning rate for each layer the early layers would have smaller learning rate, so their weight wouldn’t update as much ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"slanted triangular learning rate the idea is just to increase the LR gradually, till some point, then decrease in again and we do the increase and decrease linearly, so we end up with the triangular shape ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"gradual unfreezing gradually unfreezing parameters through time ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Natural Machine Translation bu Jointly Learning to Align and Translate we wanna model the conditional probability $ p(y|x)$. where x is the source sentence, and y is the target sentence ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"RNN Encoder-Decoder Encoder the encoder gonna encode our entire sentence into a single vector c we can use LSTM, which will output a sequence of vectors $ h_1, h_2, \\dots ,h_T$. we can choose c to be just the last vector of the LSTM $h_T$ ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Decoder we can model $p(y|x)$, as that product of $y_i$ for i from 0 to input time T. but we can do an approximation, that instead of X, we calc using C which is a representation of X. and instead of using the previous Y outputs in previous time steps, we can use the previous hidden state ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"BLUE Score ( Bilingual Evaluation Understudy ) Good reference provides an automatic score for machine translation the normal precision gives terrible results they introduced a modified precision score, which gives score to words up to the maximum number of occurrences in the reference sentences we need also to account for different grams. for example, for bi-grams, we would count the bi-grams in the output, and count-clip them at the maximum of the bi-gram in the reference sentences Pn = BLUE score on n-grams only Combined Blue score: Bp exp(1/n * sigma(Pn)) Bp: brevity Penalty it basically penalize short translations Bp: is one if output is longer than reference otherwise, it’s exp(1 - (output length / reference length)) ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Sequence to Sequence Learning with Neural Networks One limitation of RNNs is that the output sequence, is the same length as the input sequence this is using two different LSTMs one for input, and one for output it stacks multiple LSTMs together creating deep LSTM reversing the order of the words of the input sentence the intuition is that the first of the output is gonna take most info from the first tokens of the input ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:16:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Phrase representation they combined DL approaches like RNNs, with statistical ML approached to enhance the translation we cen learn word embedding from the translation task ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:17:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Attention-based Neural Machine Translation attention solves the problem of decreasing Blue-score with increasing the sentence length ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:18:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Global Attention with previous approaches, we used a small version of attention, to choose which source vector would have the bigger weight in this version we do the same, but with different, source-target hidden state vectors so we attend source and target hidden state vectors ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:18:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Local Attention instead of attending to all of the input space, we can jus attend to a portion of it it’s faster than global attention how to choose the portion to attend to, is learnable ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:18:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Byte Pair Encoding the main objective is to reduce the amount of unknown words the idea is to iterate over the bytes in the sequence and pick the most frequent one and replace it with an unused byte a byte is a character, or a sequence of characters we keep doing that till we convert our input sequence to bunch of bytes at test time, we would have Out Of Vocab words, but we can convert them to known bytes that we extracted in training. we are trying to find a balance between word-encoding and character-encoding ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:19:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Google’s Neural Machine Translation the first paper to beat statistics MT methods we will use encoder RNN to encode the input then we will use attention, to attend to the input they added 8-layer LSTM with residual connection they used (Byte-Pair) word-pieces technique the loss function, is the log likelihood of the output, conditioned on the input, but we wanna to add the GLeu score to penalize depending on the quality of the translation, so we can add the Gleu score to the loss function as a reward, in RL the did beam-search which penalized small sentences, and added penalty for long sequence contexts lastly, they quantize the model, and its parameters in inference ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:20:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Convolution Sequence to Sequence Learning we want to use the parallelization of the CNNs to learn seq-to-seq we add positional embedding to account for the different sentence positions. we didn’t need to do this for RNNs as they process words sequentially by default The network has and encoder, and a decoder the encoder, process all input the decoder, only considers the previous inputs we have a stack of conv blocks for each convolution block, you take a k words, each is d dimensional, then you flatten them to be (kd) dimensional, and apply convolution, which is multiplying by filter of size (2d,kd), then you have an output of (2d) dimension. you talk the first half and dot product it with the sigmoid of the second half. there we applied the non-linearity. and for each block, we also add residual connection. lastly, we add attention between our encoder blocks output, and the decoder blocks output ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:21:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Attention Is All You Need in RNNs and CNNs, there’s this inductive bias, that the useful information, is right next to us. while, is Attention we don’t assume that Just as all attention based models, we need to add positional encoding they do that by fourier expansion all previous work was cross attention between encoder, and decoder here they introduced, self-attention, where the encoder attend to its inputs Then the added residual connection and layer normalization ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"One Head Attention we have Query, Key, and Value. we multiply each of them with a weight matrix to add learnable parameters first, we do attention between, the query and the key, they we down-scale the dot product by the square root of the embedding dimension. we choose the square root, because it’s not big nor small number, so we keep the attention weights in a reasonable range then we do a weighted sum between the attention weights and the Value matrix ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Multi Head Attention then one the most important ideas here is multi-head attention we make many single head attention, then concatenate them together ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Decoder the same as the encoder the main difference, is that we do masked attention, which only attend to previous outputs only Here, the query is coming from the output sequence, and the key, and value, are coming from the encoder ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Subword Regularization we want to have my many Subword tokenization for the same word the multiple subword tokenization works like kind of data augmentation, and also adds a regularization effect ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:23:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Transformers are RNNs The idea is that we don’t have to use softmax fuction, to caputer the similarity between two tokens, we can use any other suitable function. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:24:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"ELMo We need to have multible word representation for the same words, depending on the context. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:25:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Forward language model you predict the next word given the previous words ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:25:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Backward language model you predict the next word given the next words ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:25:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python","NLP"],"content":"Course 1: Classification and vector Spaces ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:0:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Weak 4 ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:1:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Hashing We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space Locality senstive hashing the idea is to put items that are close in the vector space, in the same hashing buckets we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly but how can we be sure, that the planes that we chose are the perfect set to seperate our space, we can’t be sure of that, so we create a multi sets of random planes and every set would get us a different way to seperate our words Course 2: Probabilstic Models ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:1:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week 1: Autocorrect to make a simple auto-correction system, you need to perform 4 simple steps: you need to identify the miss spelled words get the n edit distance correct words filter these candidates for correct words in the dictionary change miss spelled word with one that has the highest probability ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:2:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week 2: POS using Viterbi algorithm Part of Speech Tagging (POS) is the process of assigning a part of speech to a word You can use part of speech tagging for: Identifying named entities Speech recognition Co-reference Resolution You can use the probabilities of POS tags happening near one another to come up with the most reasonable output. ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:3:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Markov Chains: You can use Markov chains to identify the probability of the next word. calculate transmission probability calculate emission probability ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:3:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Viterbi algorithm calculates a probability for each possible path a probability for a given state, is the (transition probability * emission probability) total probability of the path, is to product of all states probabilities we use a top down dynamic programming algorithm to build the paths matrix we use sum of logs instead of product of probabilities, to avoid converging to zero values ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:3:2","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week3 Autocomplete ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"N-Gram models: it’s a language model that predicts probabilities of sentences depending on the probabilities of their N-grams to capture the context of beginning end ending of the sentences, we add start and end tokens to each sentence ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Preplexity: a measure to calculate how complex a sentence is humans type low preplex sentences ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:2","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"out of vocab words and smoothing we can add UNK token for unseen words in the vocab we can apply smoothing of interpolation for unseen Ngrams ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:3","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week4: Word vectors using Bag of Words method we can use self-supervised learning in predicting the next word, to learn the weight matrix word2vec continuous bag-of-words predicts a word in context continuous skip-gram tries to predict the words surrounding input word Global Vectors (GloVe) factorizes the corpus word co-occurrence matrix fastText based on skip-gram model support out-of-vocab words Advanced word embedding methods Deep Learning, contextual embedding BERT ELMO GPT-2 ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:5:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Continuous Bag-of-Words Model you choose a center word, and a context words around it, and try to predict the centered word we model the words by one-hot encoding then we prepare the input training example feature to be an average of the one-hot vectors of the context words, and the label would be one-hot vector of the center word after training the model, the word embedding is one of the weight matrices, or an average of them Evaluations Intrinsic evaluation test the relationships between words Analogies Clustering Visualizing Extrinsic evaluation test the embedding on the end task you want to perform (ex. Sentiment Analysis) evaluates actual usefulness of embeddings time consuming more difficult to troubleshoot ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:5:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning"],"content":"CS480/680 Intro to Machine Learning - Spring 2019 Course ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 12 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:1:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Gausain process infinite dimentional gaussian distribution ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:1:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 16 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:2:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Convolution NN a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:2:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Residual Networks even after using Relu, NN can still suffer from gradient vanishing the idea in to add skip connections so that we can create shorter paths ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:2:2","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 18 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:3:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"LSTM vs GRU vs Attention LSTM: 3 gates, one for the cell state, one for the input, one for the output GRU: only two states, one for output, and one for taking weighted probablitiy for the contribution of the input and the hidden state takes less parameters Attention: at every step of producing the output, create a new context vector that gives more attention to the importat input tokens for this output token ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:3:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 20 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:4:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Autoencoder takes different input and generates the same output used in: compression denoising sparse representation data generation ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:4:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 21 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:5:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Generative models Variational autoencoders idea: train the encoder to sample a fixed distribution , we want the network to sample a fixed distribution that is close to the distribution of the encoder, so that it generate similar outputs to the input, but not the same GANS: ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:5:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 22 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Ensemble Learning: the idea is to combine the hypothesis of several models to produce a better one Bagging - choose the class the majority votes Weighted majority - decrease the weight of corrlelated hypothesis - increase the weight of good hypothesis Boosting - idea: when an instance is missclassified by hypothesis, increase its weight so that the next hypothesis is more likely to classify it correctly - can boost a weak learner - makes weighted training set, so that it can focus on missclassified examples - at the end generate weighted hypotheses based on the acc of each hypothsis - Advantages: - no need to learn perfect hypothesis - can boost any weak learning algo - boosting is very simple - has good generalization Netflex challenge 2006 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 23 Normalizing flows ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:2","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 24 Gradient boosting boosting for regression ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:3","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deepdearning","VoiceConversion"],"content":"A Research notebook, for Voice conversion models ","date":"11-21-2021","objectID":"/blog/voice_conversion/","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"AutoVC paper Demo Repo ","date":"11-21-2021","objectID":"/blog/voice_conversion/:1:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"Speach Split Paper Demo repo ","date":"11-21-2021","objectID":"/blog/voice_conversion/:2:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"AutoPST Paper Demo repo ","date":"11-21-2021","objectID":"/blog/voice_conversion/:3:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"Sources IMPROVING ZERO-SHOT VOICE STYLE TRANSFER VIA DISENTANGLED REPRESENTATION LEARNING Voice Conversion Challenge 2020 results ","date":"11-21-2021","objectID":"/blog/voice_conversion/:4:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["python"],"content":"Python tricks ","date":"11-10-2021","objectID":"/blog/python/","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Inheritance If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn’t overload it Ex: class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don't override the constructor and use the parent one ","date":"11-10-2021","objectID":"/blog/python/:1:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Multiple Inheritance when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority if the two parent classes have the same function, then Method Resolution Orded (MRO) makes the fist class method to be the one called if we override a function from the parent class in the child class, the cild class has the pariority to call the both finctions, then we can call the parent class from the child class directly ex: in the child class we can do: parent.method() ","date":"11-10-2021","objectID":"/blog/python/:1:1","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Underscore single underscore (Before): used for internal variables double undescore (After): used also for internal scopes only, and also tells python to change the variable name(mangling) underscore (After): helps avoid conflicts with key words ex(class_, int_, etc) underscore (Before and Afer): used for thing like _init_, _main_, etc ","date":"11-10-2021","objectID":"/blog/python/:2:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Decorators def function(func): def inner(): print(1) func() print(3) return inner @function def print_name(): print(2) print_name() 1 2 3 so in this example we decorated a function (print_name), which means we gonna pass the deocared function as a paramemter to the decorator so with decorator, we call our function in the inner function, the decorate it, then return the decorated function used when u wanna decorate a multiple function with the same, suppose you have an add and multiply function, and you want to print something at the beginig of them, so you crate a decorator that does that, and decorate the two functions with that. ","date":"11-10-2021","objectID":"/blog/python/:3:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["deeplearning","python"],"content":"A bunch of very different topics scribbles","date":"10-27-2021","objectID":"/blog/scribbles/","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Transformers To deal with sequential data we have to options: 1-D convolution NN processing can be parallel not practical for long sequences Recurrent NN can’t happen in prallel have gradient vanshing problem of the squence becomes so long we have bottleneck at the end of the encoder RNN with attention mechanism to solve the bottleneck problem, we make Encoder-Decoder attention Decoder utilzes: context vector weighted sum of hidden states (h1,h2, … ) from the encoder ","date":"10-27-2021","objectID":"/blog/scribbles/:1:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Transformers Encoder first we do input embedding, and positional embedding in self attention: we multiply q,w,v by a matrix to do lenear transformation self attentoion: k * q –\u003e scaling down –\u003e softmax –\u003e * v ","date":"10-27-2021","objectID":"/blog/scribbles/:1:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"multi-head attention works as we use many filters in CNN in wide attention: it takes every word and spread it multi-head attention in narrow attention: it take every word and split it up across the multi-head but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN? ","date":"10-27-2021","objectID":"/blog/scribbles/:1:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Positional info positional encoding using the rotation sin/cos matrix positional embedding ","date":"10-27-2021","objectID":"/blog/scribbles/:1:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Residual connections to give the chance to skip some learning parameters if it’s better to minimize the loss ","date":"10-27-2021","objectID":"/blog/scribbles/:1:4","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Layer Normalization in batch normalization ==\u003e we normalize to zero mean and unity varince we calculate for all samples in each batch (for each channel ) in layer normalization ==\u003e $y = \\gamma * x + \\beta $ where gamm and bata are trainable parametes calculates for all channles in the same sample in instance normalization ==\u003e calculate for one channel in one sample ","date":"10-27-2021","objectID":"/blog/scribbles/:1:5","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Debugging ML Models Understand bias-variance diagnoses getting more data ==\u003e fixes high variance smaller set of features ==\u003e fixes high variance Refrence Prof. Andrew NG Vid ","date":"10-27-2021","objectID":"/blog/scribbles/:2:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"SVM and Kernels The main idea of kernels, is that if you can formlate the optimization problem as some of inner products of feater vectors, that can have infinite dimentions, and to come up with a way to calc these inner products efficiently we have $ X(i) \\in R^{100}$, suppose W can be expressed as a linear combintaion of X $ W = \\sum_{i = 1}^{M} \\alpha_{i} y^i x^i$ (This can be proved with the representer theorem) vector W is perpendicular to the decsion boundry specified by algorithm, so W kinds of sets the orientation of the decision boundry and the bias moves it alont right and left. optimization problem is : $\\min {w,b} {1/2} ||W||^2 $ s.t $y^i((W^T * x^i) + b) \u003e= 1$ For SVM you can make a trade off between the margin and how much you can tolerate wrong calssified examples using a constant ","date":"10-27-2021","objectID":"/blog/scribbles/:3:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Distributed Training in Pytorch ","date":"10-27-2021","objectID":"/blog/scribbles/:4:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Pytorch DDP Internal DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP. The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready. Backward pass: Because backward() function is called on the loss directly, which out of DDP’s control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization. DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes. Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration. ","date":"10-27-2021","objectID":"/blog/scribbles/:4:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DataParallel VS DistributedDataParallel DataParallel is single-process, multi-thread, and only works on a single machine, while DistributedDataParallel is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs. DataParallel doesn’t support model parallel Resources - https://pytorch.org/docs/master/notes/ddp.html - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training ","date":"10-27-2021","objectID":"/blog/scribbles/:4:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Complete Statistical Theory of Learning - Vladimir Vapnik There are two ways for generalization: more data, and complete learning theory Turing said, that you should imitate intelligent person ","date":"10-27-2021","objectID":"/blog/scribbles/:5:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Ref: https://www.youtube.com/watch?v=Ow25mjFjSmg ","date":"10-27-2021","objectID":"/blog/scribbles/:5:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Statistical Machine Learning ","date":"10-27-2021","objectID":"/blog/scribbles/:6:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"part 1: deduction vs induction: deduction: is the process of reasoning from one or more general statements to reach a logically certain conclusion. premises must be correct. induction: reasoning that construct or evaluates general proposition that are derived from specific examples. we can never be sure our conclusion can be wrong! machine learning tries to automate the process of inductive inference. why should ML work? ML tries to find patterns in data we will only be able to learn if there’s something to learn ML makes some assumptions, which are are rarely made explicit. we need to have an idea what we are looking for. This is called “inductive bias”. Learning is impossible without such a bias the formal theorem if this is called no free lunch theorem on the other hand, if we have a very strong inductive bias, then with just few training examples, then we can have high certainty in the output the problem of selecting a good hypothesis class is called model selection. any system that learns has an inductive bias. if the algorithm works, THERE HAS TO BE A BIAS the inductive bias, rules over our function space ","date":"10-27-2021","objectID":"/blog/scribbles/:6:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Part 2: it;s not hard for ML algorithm to correctly predict training labels usually ML algorithms make training errors, that is the function,they come up with doesn’t perfectly fit the training data what we care about is the performance on teh test set it’s not always the case that lowering the train data would lower that test data K-Nearest algorithms: for K-nearest algorithm, the best value for K is log(N) if k is too small ==\u003e overfitting if k is too large ==\u003e underfitting k nearest algo achieves good results on MNIST dataset for classifying two classes, with simple euclidean distance function k-nearest can be used for density estimation, clustering, outlier detection the inductive bias in K nearest algo, is that near points are the of teh same category the challenging part about k nearest algo is how to measure the distance between points ","date":"10-27-2021","objectID":"/blog/scribbles/:6:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Part3 for ML, we don’t put any assmuptions for the data probability ditribution often, the input and output are random variables in some applications, it’s important that the loss depends on the input X. also in some cases, the type of error is critical, for exp. spaam detection Bayes risk: is the min error for the expected values over all examples –\u003e basically the it’s the lowest error you can achieve Consistency: we say algorithm A is consistent, if we have an infinite iid datapoints, and the risk of algorithm’s selected function converges to the Baye’s risk. basically means that if we have infinite data samples, then our algorthms reaches the Bayes risk, which is the lowest error possible Universally consistent: no mattter the underlying probability distribution is, when we have enough data points, the algorithm would be consistent consistent independantly of the data distribution KNN classifier, SVM, boositn, random forests are universally consistent ","date":"10-27-2021","objectID":"/blog/scribbles/:6:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DL Book ","date":"10-27-2021","objectID":"/blog/scribbles/:7:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"CH1 one of the key ideas in DL is that data representation matters a lot, and that DL is a technique for learning how to represent the data ","date":"10-27-2021","objectID":"/blog/scribbles/:7:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"AI in AI we need the computer to do some tasks like humans we do that by providing the computer with a lot of rules describing the world and how to act in different scenarios ","date":"10-27-2021","objectID":"/blog/scribbles/:7:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"ML in machine learning we can learn these rules without explicitly told them but we still need to be provided with custom features that are given by domain experts ","date":"10-27-2021","objectID":"/blog/scribbles/:7:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Representation learning a specific type of ML where we don’t tell the computer the specific features instead, we give the computer raw input, and it should learn the more complex features explicitly ex: autoencoders ","date":"10-27-2021","objectID":"/blog/scribbles/:7:4","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DL is a representation learning algorithms that is applied in multi sequential manner ","date":"10-27-2021","objectID":"/blog/scribbles/:7:5","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"CH2 ","date":"10-27-2021","objectID":"/blog/scribbles/:7:6","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Deep Generative Modeling latent variable: it’s a variable that is controlling some behaviors, but we can’t directly observe it we are trying to observe true explanatory factors, for example, latent variables, from only observed data ","date":"10-27-2021","objectID":"/blog/scribbles/:8:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Autoencoders: the encoder learns to map from data, to a low-dimensional latent space the decoder learns to map back from the low-dimensional space back into a reconstructed observation the bottleneck hidden layer forces the network to learn a compressed latent representation the reconstruction loss forces the latent representation to capture as much information from the data ","date":"10-27-2021","objectID":"/blog/scribbles/:8:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Variational Autoencoders (VAE) with classic autoencoders, once we train the network, then the latent representation is deterministic but in VAE, we add some randomness, so we can generate new samples so the encoder should output a mean and a standard deviation, which represents a distribution of the input, then we can sample from this distribution to generate new sample the encoder is trying to infer a probability distribution of the latent space with respect to its input data the decoder is trying to infer a new probability distribution over the input space given the latent space the loss is going to be function of the parameters of the two distributions the loss would consist of a construction loss and a regularization term, which is responsible for inducing some notion of structure of this probabilistic space We need regularization and a prior to: continuity: points that are close in the latent space, should remain close after decoding completeness: samples from the latent space, should produce meaning content after decoding we can’t perform back propagation, as there’s stochasticity in the latent space, to solve this issue, we fix, the mean and variance, and introduce the stochastic term separate from them The key problem with VAEs, is a concern of density estimation ","date":"10-27-2021","objectID":"/blog/scribbles/:8:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Generative Adversarial Networks (GANs) we need to sample from a very complex distribution, that we don’t know, and can’t estimate the solution, is to sample from a simple distribution (eg. noise), then learn a transformation, to the data distribution we have a Generator, that’s tries to transform the data sampled from the random noise, into data that looks real, to trick the discriminator we have a discriminator, which tries to identify real data from fake. ","date":"10-27-2021","objectID":"/blog/scribbles/:8:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"SubWords it’s just like the skip-gram model but we just changed the score function so that we increased the vocab size by adding N-grams of all words we have then we use them to capture more meaning from the words if we encounter new word, then we add it’s N-gram and thats would be the word vector ","date":"10-27-2021","objectID":"/blog/scribbles/:9:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Decision Trees they are greedy algorithm they can stuck in local minimum if the we have some continuous features, we can use it multiple times, every time with different range ","date":"10-27-2021","objectID":"/blog/scribbles/:10:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Purity function we want to define a purity function, that has the following it has it’s maximum value when probability of any class in 1 it has it’s minimum value when all classes has the same probability Purity( pa, pb) == Purity (pb, pa) entropy = impurity = -1 * purity one function that satisfies all these requirements, is $ purity(p_{c1}, p_{c2}, p_{c3}) = p_{c1} \\log(p_{c1}) * p_{c2} \\log(p_{c2}) * p_{c3} \\log(p_{c3})$ so we choose features, that would increase purity the most after splitting the dataset using it to calculate after entropy or purity of a set after seperation, would be the weighted average of the subsets ","date":"10-27-2021","objectID":"/blog/scribbles/:11:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Why going deep in Deep Learning one motivation, is that going deep can reduce the size of our units exponentially in our underlying function, there could be some symmetry, that we can fold the function across its axis for statistical reasons, we would want to infer out initial beliefs, about our function, that is involve the composition of several simpler functions Empirical experiments show that deeper networks generalize better ","date":"10-27-2021","objectID":"/blog/scribbles/:12:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Few-shot Learning we want to classify examples, that we only have few examples for, maybe even 0 the idea is instead of having a classifier as the last layer(softmax layer), we can use a siamese network, just to tell us is the two examples are similar so we just learn a similarity function ","date":"10-27-2021","objectID":"/blog/scribbles/:13:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"GLIDE: Generates realistic images from text GLIDE is the next version of DALLE, with respect to photo realism and caption similarity this is fine-tuned model, not zero-shot like DALLE It can generate or edit images so you can generate an image using zero-shot, then you can edit the image by putting masks on the image and tell the model what to draw in the masked area ","date":"10-27-2021","objectID":"/blog/scribbles/:14:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Diffusion Models we start with the original image, then we keep add noise to it till it become so noisy then we try to reverse the operation and get it back to the input image ","date":"10-27-2021","objectID":"/blog/scribbles/:14:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"P vs NP P = problems solvable in polynomial time NP = decision problems solvable in nondeterministic polynomial time decision problems = yes, no problems NP problems are hard to solve, but each to check the correctness of the answers ","date":"10-27-2021","objectID":"/blog/scribbles/:15:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Expectation Maximization If we have a probability distribution that is function of two variables, one is observable and the other is latent and we want to calculate the MLE for this model. Then we can calculate the marginal likelihood instead The marginal probability basically means that we take the summation over all possible states of the latent variable ","date":"10-27-2021","objectID":"/blog/scribbles/:16:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["TTS","Deeplearning"],"content":"A summary research for TTS","date":"10-14-2021","objectID":"/blog/text_to_speech/","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"TTS TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. ","date":"10-14-2021","objectID":"/blog/text_to_speech/:0:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"End-to-end TTS easier pipeline better peformance ","date":"10-14-2021","objectID":"/blog/text_to_speech/:1:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Wavenet ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Based on: DeepMind’s Wavenet https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html https://deepmind.com/blog/article/wavenet-generative-model-raw-audio ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Wavenet V1 before wavenet, ther was two methods: generative method, which would produce the over all song of the sentece well, but would fail to produce the individual sounds well concatinative: we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence wavenet: tries to do both of the above methods, it also can change the speaker by changing some parameters data output: 16 khz rate we cant use normal RNN as the max seq length around 50 they used dilated CNNs: can have very long look back fast to train This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:2","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Wavenet V2 The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn’t depend on previous samples to produce the current sample, while still maintaining the same quality ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:3","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"End-to-end adversarial TTS ","date":"10-14-2021","objectID":"/blog/text_to_speech/:3:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Based on: https://www.youtube.com/watch?v=WTB2p4bqtXU https://deepmind.com/research/publications/2020/End-to-End-Adversarial-Text-to-Speech https://arxiv.org/abs/2006.03575 Adversarial: means we have a generator and a descriminator which tries to detect which output is generated and which is real End-to-end : they take in text and output the speech steps: we enter the text, then we tokenize it then we use a stack of dilated conv layers to predict the length of each token with this info, we can predict the center of each token then we use a gaussian kernel to give a prob distributaion for the place of the token center then the generator can generate the sound ** Q: here we assumed that every token starts directly after the one before it, but is that the case? don’t we need to add a small duration of no sound between tokens? ","date":"10-14-2021","objectID":"/blog/text_to_speech/:3:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Tacotron2 ","date":"10-14-2021","objectID":"/blog/text_to_speech/:4:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"based on: paper repo you train tacotron-like seq2seq model to output a mel spectrogram, then pass that to wavenet to generate the wave form This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech. ","date":"10-14-2021","objectID":"/blog/text_to_speech/:4:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Common TTS Architictures: - Autoregressive models - DCCN - Flow - Teacher Student - Variational auto encoders (VAE) - GANS Resources https://towardsdatascience.com/state-of-the-art-of-speech-synthesis-at-the-end-of-may-2021-6ace4fd512f2 ","date":"10-14-2021","objectID":"/blog/text_to_speech/:5:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["FastAi"],"content":"A summary for FastAi Practical deep learning for coders course.","date":"10-06-2021","objectID":"/blog/fastai/","tags":["FastAi"],"title":"FastAi 2020","uri":"/blog/fastai/"},{"categories":["FastAi"],"content":"Lecture two P value: determines if some numbers have realationship, or they are random (whether they are independat or dependant) suppose we have the temp and R (transmitity) values of a 100 cities in China and we want to see if there's a relation between them. then we generate many sets of random numbers for each parameter then we calculate the P value which would tell us what's the percentage this slope is a random, and that ther's no relation A P-value is the probability of an observed result assuming that the null hypothesis (there's no relation ) is true PS: P-value also is dependant on the size of the set u used, so they don't measure the importance of the result. so don't use P-values If the P value is \u003e 0.5 then we sure that these daata have no ralation, and if the p-value is so small, then there's a chance that the data have a relation ","date":"10-06-2021","objectID":"/blog/fastai/:0:1","tags":["FastAi"],"title":"FastAi 2020","uri":"/blog/fastai/"},{"categories":["FastAi"],"content":"Lecture three In the course video and book, we built a bear classifier, using data from Microsoft Ping Api. To build a deep learning model, we have first to gather the data, then we should prepare the data to be in the right format for the model, then we train the model and observe if we get satisfiable results, if not then we try to investigate to try to get better results. Finally we save our model and deploy it! while gathering the data, notice that all the time the data would be biased, and in sometimes these biases would be severe that they can’t be ignored Race classifier I have tried to rebuild the notebook and to make a Race classifier. I got the dataset from here Dataset, and then trained a small Resnet18 NN to classify images. To deploy the app, I used Voila and MyBinder to make it available online here: Race Classifier Lastly, all the code can be found in this githun repo here ","date":"10-06-2021","objectID":"/blog/fastai/:0:2","tags":["FastAi"],"title":"FastAi 2020","uri":"/blog/fastai/"},{"categories":[],"content":"This is Post for Computational Linear Algebra ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":[],"content":"Lecture 1 import numpy as np a = np.array( [[6,5,3,1], [3,6,2,2], [3,4,3,1] ]) b = np.array( [ [1.5 ,1], [2,2.5], [5 ,4.5] ,[16 ,17] ]) for c in (a @ b): print(c) [50. 49.] [58.5 61. ] [43.5 43.5] Lecture 2 Matrix decomposition: we decopose matricies into smaller ones that has special properties ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/:0:0","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":[],"content":"Singular Value Decomposition (SVD): it’s an exact decomposition, so you can retrieve the orginal matrix again Some SVD applications: semantic analysis collaborative filtering / recommendation data compression PCA (principal component analysis) ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/:0:1","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":[],"content":"Non-negative Matrix Factorization (NMF) ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/:0:2","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":["MIT","Linear Algebra","Math"],"content":"summarization of some Professor Gilbert Strang's MIT Linear Algebra course 18.06","date":"10-02-2021","objectID":"/blog/1806_mit/","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 1 We learn about the big picture behind multiplication of matrix and vector we learn about the row picture and column picture ","date":"10-02-2021","objectID":"/blog/1806_mit/:1:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 2 we learned about elimination method to solve a system of equations ","date":"10-02-2021","objectID":"/blog/1806_mit/:2:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 3 in this lecture we learned about matrices multiplication: we can do that in five ways: row * col ==\u003e gives an entry (1 cell) col _ row ==\u003e sum ( r1 _ c1 , r2 * c2, etc) by columns ==\u003e A * c1 = combination of A columns by columns ==\u003e r1 * B = combination of A B rows by blocks ==\u003e A (A1,A2,A3,A4) _ B (B1,B2,B3,B4) = C1 = (A1_ B1 + A2 * B3) and so on then we learned about gausian-Jordan elimination to find the matrix inverse [A | I] ==\u003e we apply elimination till we get [ I | A-1 ] ","date":"10-02-2021","objectID":"/blog/1806_mit/:3:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 4 in this lecture we learn about A= L U, where L is E^ -1, and whats special about this is that it has all multipliers in the lower triangular with ones on the diagonal ","date":"10-02-2021","objectID":"/blog/1806_mit/:4:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 5 we continued a little with permutations and moved to vector spaces we learned about sub spaces and columns spaces ==\u003e which is u take the columns of the matrix and all its combinations and that creates a plane through origin making a columns space ","date":"10-02-2021","objectID":"/blog/1806_mit/:5:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 6 In this lecture we continued about columns spaces and that we build those up by taking the combinations of all columns. Then we learned about null spaces while are sub spaces of X that satisfies A X = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:6:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 7 in this lecture we continued about null space then we learned about the special solution, where we assume the variables of the free vector then get the special solution finally we learned about the reduced form where R = [ I F 0 0 ] and the null matrix is [ -F I ] then R N = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:7:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 8 in this lecture we expanded to talk about A x = b and we find the whether there’s a solution to the equation or not depends on the rank of the matrix also we get the Xcomplete = Xparticular + Xnull space and we get particular soln by putting all free variables = zero ","date":"10-02-2021","objectID":"/blog/1806_mit/:8:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 9 in this lecture we learned about independent columns and how they make a space, we also learned about Basis and what are two conditions for it rank(A) = number of pivot columns of A = dimension of C(A). dimension of N(A) = number of free variables = n − r, ","date":"10-02-2021","objectID":"/blog/1806_mit/:9:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 10 In this lecture we learned about the four subspaces we also started in matrix space M ","date":"10-02-2021","objectID":"/blog/1806_mit/:10:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 11 We learned about matrix space we take introduction about graph ","date":"10-02-2021","objectID":"/blog/1806_mit/:11:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 12 in this lecture we learned about graphs and how to represent them with matrices, then we applied that to electrical system and applied kerchofs law ","date":"10-02-2021","objectID":"/blog/1806_mit/:12:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 13 Quiz 1 review ","date":"10-02-2021","objectID":"/blog/1806_mit/:13:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 14 in this lecture learned about othrignilaity of the four vector spaces and what does it means ","date":"10-02-2021","objectID":"/blog/1806_mit/:14:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 15 in this lecture we learned about projection of matrices into subspaces ","date":"10-02-2021","objectID":"/blog/1806_mit/:15:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 16 we got example explaning the projection into subspaces and how to get the best fit using the least square error ","date":"10-02-2021","objectID":"/blog/1806_mit/:16:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 17 in this lecture le learned about orthonormal vectors and their special features and we learn how to produce them from any independent vectors using gram-schmeit ","date":"10-02-2021","objectID":"/blog/1806_mit/:17:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 18 Propertise of determants ","date":"10-02-2021","objectID":"/blog/1806_mit/:18:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 19 det I =1 sign reverses with each row or colums exchange det is linear in each row seperately ","date":"10-02-2021","objectID":"/blog/1806_mit/:19:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Big Det Formula for a N * N matrix, we calc the sum of N! terms $$detA=\\sum_{i=1}^ N a1\\alphaa2\\betaa3\\gamma*an\\omega$$ (where $\\alpha, \\beta, … \\omega $) = perm of (1,2,3, …, N ) ","date":"10-02-2021","objectID":"/blog/1806_mit/:19:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Cofactors cofactor of aij = Cij = +/- det of ( n-1 matrix with column j, and row i erased ) it is plus if i+j is even, minus if i+j is odd cofactor formula (along row 1) det A = a11 C11 + a12 C12 + .... + a1n C1n ","date":"10-02-2021","objectID":"/blog/1806_mit/:19:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 20 $ A^{-1} = 1/detA * C^T$ where C is the cofactors matrix ","date":"10-02-2021","objectID":"/blog/1806_mit/:20:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Cramers rule A x = b x= A^ (-1) b = 1/detA C^T b X_j = detB_j / detA where B_j is A matrix with column j replaced by b ","date":"10-02-2021","objectID":"/blog/1806_mit/:20:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Det A = Volume detA = volume of the shape created by making an edge from each of the rows ","date":"10-02-2021","objectID":"/blog/1806_mit/:20:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 21 Eigenvalues and Eigenvectors Eigenvectors: Ax is prallel to x ==\u003e Ax = $\\lambda$x lambda ia the eigen values if we have a plane: any x in the plane: Px= x ==\u003e x is eigenvector and lambda = 1 any x perpendicular to plane Px = 0 ==\u003e x is eigen vector and lambda = 0 Fact: the sum of the eigenvalues = the sum of the diagonal of A ","date":"10-02-2021","objectID":"/blog/1806_mit/:21:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 22: Diagnolization to get power of matrix $A^k$ first get the eigenvalues and vectors for A then compute $A = S \\lambda S^{-1}$ where S is the eigenvector matrix, and Lambda is diagonal matrix of the eigenvalues then $A^k = S * \\lambda^k*S^{-1}$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:22:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 23 for the diffrential equations: 1- Stability if lambda \u003c 0 ==\u003e u(t) –\u003e 0 2- Steady state if lambda1 = 0, lambda2 \u003c 0 3- Blowup if any lambda \u003e 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:23:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 24 ","date":"10-02-2021","objectID":"/blog/1806_mit/:24:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Markov Matrix 1- All entries \u003e= 0 2- The sum of every column is 1 3- lambda = 1 is eigenvalue 4- all othe lambda \u003c 1 5- eigenvector values \u003e= 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:24:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Fourier series integration of ( f(x) g(x) dx ) from 0 to 2pi = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:24:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 25 ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Symmetric matrices $ A = A^T$ the eigenvalues are real the eigenvectors are perpendicular usual case: $A = S \\lambda S^{-1}$ symmetric case: we have orthonormal eigenvectors $A = Q \\lambda Q^{-1} = Q \\lambda Q^{-T} $ ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Every symmetric matrix is a combination of perp. projection matrices ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Signs of pivots are the same as the sign of the eigenvalues product of pivots = product of eigenvalues = det of matrix ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:3","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Positive definite symmetric matrix all eigenvalues are positives all pivots are positive det is positive as it’s the product of the eigenvalues also all sub detemants are positive ( determants of lower matrices ) if S is pos. definite ==\u003e $ X^TSX$ must be positive ","date":"10-02-2021","objectID":"/blog/1806_mit/:26:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 26 ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Complex Matrices we wanna utilize tha fact that $\\bar{Z^T}*Z = \\left|{Z}\\right|^2$ Hermitian is biscally the conj. and transpose ==\u003e $Z^H = \\bar{Z^T}$ Hermitian Matricies : $A^H = A$ perpendicular: q1, q2, …, qn $\\bar{qi}^T * qj = 0 if i!=j, 1 if i=j $ $Q^H*Q = I$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Fourier Matrix a matrix with entries are powers of some number W. where $W^n = 1$ $ F^H*F = I$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Fast fourier transform reduces complexity from $N^2 to N log(N)$ $W_{2n}^2 = W_n ==\u003e W_4^2 = W_2$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:3","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 27 when det= 0 ==\u003e then the matrix is positive semi-definite f(x1,x2,x3…,xn) ==\u003e min when the matrix of second derivatives is positive definite the eigenvalues tells us the length of the axis of the shape crated by cutting through the shape of the $X^T A X $ the direction of the eigenvectors is the direction of the axis of that shape ","date":"10-02-2021","objectID":"/blog/1806_mit/:28:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 28 A is a m by n matrix ==\u003e $ A^T*A$ is a positive definite symmetric matrix ","date":"10-02-2021","objectID":"/blog/1806_mit/:29:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"similar matrices A and B are similar means: for some M ==\u003e $B = M^{-1}AM$ Similar matrices has the same eigenvalues if the eigenvalues are unique the eigenvector of B is $M^{-1} * (eigenvector ofA)$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:29:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Jordan form every square A is similar to Jordan matrix J every jordan block has one eigenvaector the number of jordan blocks = number of eigenvectors ","date":"10-02-2021","objectID":"/blog/1806_mit/:29:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 29 eigenvalues of (AB) = eigenvalues of (BA) ","date":"10-02-2021","objectID":"/blog/1806_mit/:30:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Singular value composition (SVD) $Av = \\sigma u $ $ A = u \\sigma v^T = u \\sigma v^{-1}$ $A^T A = v \\sigma^T u^Tu /sigma v^T = v \\sigma^2 v^T$ $A A^T= u \\sigma^T v^Tv /sigma u^T = u \\sigma^2 u^T$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:30:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 30 ","date":"10-02-2021","objectID":"/blog/1806_mit/:31:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Linear transformation examples: projection, rotation if u know what transfotmation does to the basis of a plane, then u know what it does to every vector in the plane every $ v = c_1 v_1 + c_2 v_2 + … + c_n v_n$ then $T(v) = c_1 T(v_1) + …. + c_n T(v_n)$ coordinates come from a basis (think of basis like the X-Y-Z axis so we have a basis for the input and a basis for the output Rule to find matrix A, given the input and output basis : input basis: v1 ===\u003e vn output basis: w1 ===\u003e wm 1st column of A : write T(v1) = a11 w1 + a21 w2 + … + am1 wm 2nd column of A : write T(v2) = a12 w1 + a22 w2 + … + am2 wm repeat that for the n columns A * (input coordinates) = (output coordinates) ","date":"10-02-2021","objectID":"/blog/1806_mit/:31:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 31 ","date":"10-02-2021","objectID":"/blog/1806_mit/:32:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"change of basis we have a new basis vectors and we wanna change to the new basis W A = c _ W ==\u003e c = W^-1 _ A when we change the basis, every vector would have new coordinates ==\u003e old coordinates = new basis * new coordinates ==\u003e x = W c ","date":"10-02-2021","objectID":"/blog/1806_mit/:32:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 32 ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"2-sided inverse $A A^{-1} = I = A^{-1} A$ r = m = n ==\u003e full rank ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"left inverse full column rank r = n nullspace = 0 then $A^T A $ is invertable $A^{-1}_{left} = (A^T A)^{-1} A^T $ $A^{-1}_{left} A = I$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"right inverse full row rank r = m n-m free variables left nullspace = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:3","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"pseudo inverse $A^+$ $A^+ = v*\\sigma^{-1}*u^T$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:4","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecute 34 no solution ==\u003e rank \u003c m has one solution ==\u003e there’e no null space ==\u003e rank = n a matrix is invertable when there’s no null space ==\u003e r = n ==\u003e indep. columns positive definite matrix must have full rank ==\u003e has no null space positive def is invertable the matrix has soln of any c when the matrix has full row rank matix with orthogonal eigen vectors : symmetric matrices skew-symmetric orthogonal matrices in markov matrix the eigen values are one, and some sother values less than one $k_m$ and m goes to infinity is the steady state we ge the eigenvector that corresponts to eigenvalue one asn multiply it with c, and notes that the sum of u is alwayes the same, so the sum of u0 is the sum of uk, so look what c achieves that ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:5","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"}]