[{"categories":[],"content":"What is Peer-review Peer-review is a corner-stone in scientific research. It serves a quality check for scientific drafts. It is formally defined as: Peer review is the evaluation of work by one or more people with similar competencies as the producers of the work. Why is it important It’s important as, only after an article passes peer review can it be officially published, and it’s the knowledge shared with the scientific community and the world. Current state of Peer-reivew Peer review requires human experts, therefore, it scales poorly, particularly, with the rapid increase of submitted articles. This leads to an increased reviewing load, with reviewers having limited time to review papers (leading to poor and noisy feedback) and authors having to wait (and revise their work often) before it can be published. This leads to a degrading quality of reviews. Authors are faced with one-liner reviews like Why haven't you evaluated more baselines, and more datasets. Therefore, making reviewers more efficient (writing reviews faster) and effective (writing better reviews) is crucial to scale peer reviewing. What can we do? We aim to support reviews by developing a system that can evaluate and improve their feedback. Specifically, given a human-written (or generated) draft review, our goal is to evaluate its feedback according to defined aspects and automatically re-write the feedback so that it improves on the desired aspects and is eventually more helpful to authors. ","date":"10-05-2024","objectID":"/blog/peer_review/:0:0","tags":[],"title":"Can we save peer-review quality?","uri":"/blog/peer_review/"},{"categories":[],"content":"You and your Research https://www.cs.virginia.edu/~robins/YouAndYourResearch.html ","date":"05-05-2023","objectID":"/blog/reading_summary/:0:0","tags":[],"title":"You and your Research","uri":"/blog/reading_summary/"},{"categories":[],"content":"Main Ideas Commit to your idea Ask yourself: What are the important problems in my field? Communicate with the bright minds Ask the important questions Closed door, or open door when you work with door closed you work harder you work on the wrong thing open door working get many interruptions get important clues My take on this is we should keep an open mind about what other people are doing and where the research is heading ","date":"05-05-2023","objectID":"/blog/reading_summary/:1:0","tags":[],"title":"You and your Research","uri":"/blog/reading_summary/"},{"categories":["pytorch","python","autograd"],"content":"Trying to understand some errors we get, while using pyTorch","date":"01-10-2023","objectID":"/blog/pytorch_internals/","tags":["pytorch","python","autograd"],"title":"A glimpse into PyTorch Autograd internals","uri":"/blog/pytorch_internals/"},{"categories":["pytorch","python","autograd"],"content":"Intro Here, we are going to discuss the internals of PyTorch Autograd module. The most of us don’t have to know about this. I was the same till I came across this error: unsupported operand type(s) for *: 'float' and 'NoneType' This came from executing the following code: import torch a = torch.tensor(5.0, requires_grad=True) * 0.1 b = torch.tensor(2.0, requires_grad=True) c = a + b c.backward() a += + 0.1 * a.grad But why? We defined that the gradient of a should be calculated by putting requires_grad to be True! After some investigation, the error was due to that a.grad is None. But why is that the case? When we multiplied a by 0.1 we internally created a new tensor that is intermediate. And by default, PyTorch only populate gradients for leaf tensors only. ","date":"01-10-2023","objectID":"/blog/pytorch_internals/:1:0","tags":["pytorch","python","autograd"],"title":"A glimpse into PyTorch Autograd internals","uri":"/blog/pytorch_internals/"},{"categories":["pytorch","python","autograd"],"content":"Notes These are some notes to help understand the internals of PyTorch, and its Autograd module, and why did we get this error. To know more about this, in a more structured way,read this blog post here requires_grad tells PyTorch, if it should save the forward results, to use it to calculate the gradients. All tensors with requires_grad is False are leafs tensors. Tensors with requires_grad is True are leafs only if they are created by the user, and not as a result by a mathematical expression for example. When a tensor is created as a result of an operation on a few tensors, then it would be leaf if and only if all the tensor used to generate it has requires_grad set to False, as only then, the requires_grad for this tensor will be set to False, and as we mentioned above, it will be a leaf. For a tensor to have tensor.grad populated, it must have requires_grad to be True, and it must be a leaf. You can’t change the requires_grad for non-leaf tensors. They are set automatically according the tensors used in the operation creating them PyTorch doesn’t allow updates(in-place) on leaf tensors if the requires_grad is set to True, as it causes troubles in the backward pass. What does it mean, that Autograd will not populate the gradient for a tensor? Here’s what the backward graph does while calculating and propagating the gradient: If this tensor has requires_grad set to False, then this tensor is not part of the backward graph, and it will do nothing. If the tensor has requires_grad set to True, and is_leaf set to False, this means that this is an intermediate tensor that came off an operation. In this case, Autograd doesn’t populate the grad attribute for this tensor, and just propagate the gradient to the operation that generated this tensor (to grad_fn.) Lastly, if the tensor is a leaf, and has requires_grad set to True, then Autograd calculates the accumulated grad value, and puts it in grad field for the tensor. There’s an amazing video explaining this here Now for the Solutions: There are two solutions for this: call a.retain_grad(), which tells PyTorch to keep the grad for this tensor anyway. Note: this solution is not fast. remove requires_grad from the initialization of the tensor, and then do it explicitly in a separate step. wrap the weight update under torch.no_grad, and I will explain why we need this step below. import torch a = torch.tensor(5.0) * 0.1 b = torch.tensor(2.0, requires_grad=True) a.requires_grad = True c = a + b c.backward() with torch.no_grad(): a += + 0.1 * a.grad ","date":"01-10-2023","objectID":"/blog/pytorch_internals/:2:0","tags":["pytorch","python","autograd"],"title":"A glimpse into PyTorch Autograd internals","uri":"/blog/pytorch_internals/"},{"categories":["pytorch","python","autograd"],"content":"Normal training Here’s another script for a training a network with just one neuron. Our normal training system, would be something like this: # Input a = torch.tensor(2.0) # weight w = torch.tensor(3.0, requires_grad=True) # bias b = torch.tensor(2.0, requires_grad=True) for _ in range(10): # forward pass c = a * w + b # backward pass c.backward() # weight update w = w + 0.1*w.grad This will not work!! As we said, the weight update line, will produce an intermediate tensor, which the grad will not be populated for. That’s why we all wrap our weight updating steps under torch.no_grad() context, and do in-place calculation,which tells PyTorch not to keep track of these operations in the grad calculations, and thus the weights tensor will remain a leaf tensor. with torch.no_grad(): w += 0.1*w.grad so we use torch.no_grad() for two main reasons: we don’t need to include the weight update operation in the gradient graph. although, we zero our gradient before every backward pass, but this weight update step will add a new branch in the gradient calculations for the weights tensors,and this will mess with the calculations. we need to be able to do in-place operation, and so, our tensor will remain a leaf, and Autograd will keep populating its gradient. ","date":"01-10-2023","objectID":"/blog/pytorch_internals/:3:0","tags":["pytorch","python","autograd"],"title":"A glimpse into PyTorch Autograd internals","uri":"/blog/pytorch_internals/"},{"categories":["pytorch","python","autograd"],"content":"Conclusion At last, we usually, use pre-built optimizers for the training step, or we even use a trainer, which make the things way easier. This was just a try to go back to the basics and trying to understand the roots of the problem. ","date":"01-10-2023","objectID":"/blog/pytorch_internals/:4:0","tags":["pytorch","python","autograd"],"title":"A glimpse into PyTorch Autograd internals","uri":"/blog/pytorch_internals/"},{"categories":["pytorch","python","autograd"],"content":"References https://PyTorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf https://www.youtube.com/watch?v=MswxJw-8PvE https://medium.com/@mrityu.jha/understanding-the-grad-of-Autograd-fc8d266fd6cf ","date":"01-10-2023","objectID":"/blog/pytorch_internals/:5:0","tags":["pytorch","python","autograd"],"title":"A glimpse into PyTorch Autograd internals","uri":"/blog/pytorch_internals/"},{"categories":["deeplearning","python","TTS"],"content":"Research for Vocoders","date":"11-07-2022","objectID":"/blog/univnet/","tags":["deeplearning","python","TTS"],"title":"Univnet","uri":"/blog/univnet/"},{"categories":["deeplearning","python","TTS"],"content":"TTS (Text To Speech) TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. ","date":"11-07-2022","objectID":"/blog/univnet/:0:0","tags":["deeplearning","python","TTS"],"title":"Univnet","uri":"/blog/univnet/"},{"categories":["deeplearning","python","TTS"],"content":"Wavenet Wavenet V1 before wavenet, ther was two methods: generative method: which would produce the over all song of the sentece well, but would fail to produce the individual sounds well concatinative: we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence wavenet: tries to do both of the above methods, it also can change the speaker by changing some parameters data output: 16 khz rate we cant use normal RNN as the max seq length around 50 they used dilated CNNs: can have very long look back fast to train WaveNet: is a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals.WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. Wavenet V2 The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn’t depend on previous samples to produce the current sample, while still maintaining the same quality. ","date":"11-07-2022","objectID":"/blog/univnet/:0:1","tags":["deeplearning","python","TTS"],"title":"Univnet","uri":"/blog/univnet/"},{"categories":["deeplearning","python","TTS"],"content":"WaveGan WaveGAN is a generative adversarial network for unsupervised synthesis of raw-waveform audio (as opposed to image-like spectrograms). The WaveGAN architecture is based off DCGAN. The DCGAN generator uses the transposed convolution operation to iteratively upsample low-resolution feature maps into a high-resolution image. WaveGAN modifies this transposed convolution operation to widen its receptive field, using a longer one-dimensional filters of length 25 instead of two-dimensional filters of size 5x5, and upsampling by a factor of 4 instead of 2 at each layer. The discriminator is modified in a similar way, using length-25 filters in one dimension and increasing stride from 2 to 4. These changes result in WaveGAN having the same number of parameters, numerical operations, and output dimensionality as DCGAN Before WaveGan Autoregressive generation: It’s an approach in which speech samples are generated one by one in sequence. Examples: WaveNet Has high quality Takes around 180 secs to generate a one second of speech can’t be applied to services in production due to low speed Non-autoregressive generation: It’s an approach where all voice samples are generated in prallel Examples: Prallel WaveNet Lower quality than autoregressive method takes 0.03 seconds to generates one second of speed ","date":"11-07-2022","objectID":"/blog/univnet/:0:2","tags":["deeplearning","python","TTS"],"title":"Univnet","uri":"/blog/univnet/"},{"categories":["deeplearning","python","TTS"],"content":"End-to-End TTS End-to-end TTS systems can be splitted into two main components: Speech Synthesizer, which takes in raw text and output mel-spectrogram. Ex: Tacotron Vocoder, which takes in mel-spectrogram and outputs sound waves. Ex: Prallel WaveGan, Univnet Tacotron2 Tacotron 2 is a neural network architecture for speech synthesis directly from text. It consists of two components: a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Tacotron2 Architecture Prallel WaveGan Parallel WaveGAN1, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained even with a small number of parameters. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system. Parallel WaveGan Architecture Univnet UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time. Inspired by works in the field of voice activity detection, we added a multiresolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets. Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input Univnet Architecture ","date":"11-07-2022","objectID":"/blog/univnet/:0:3","tags":["deeplearning","python","TTS"],"title":"Univnet","uri":"/blog/univnet/"},{"categories":["deeplearning","python","TTS"],"content":"Resources Wavenet https://deepmind.com/blog/article/high-fidelity-speech-synthesis-wavenet https://www.youtube.com/watch?v=YyUXG-BfDbE https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html https://deepmind.com/blog/article/wavenet-generative-model-raw-audio WaveGan https://arxiv.org/pdf/1802.04208v3.pdf https://paperswithcode.com/method/wavegan Prallel WaveGan https://www.youtube.com/watch?v=knzT7M6qsl0 https://github.com/kan-bayashi/ParallelWaveGAN https://arxiv.org/pdf/1910.11480.pdf Tacotron https://arxiv.org/pdf/1712.05884v2.pdf Univnet https://arxiv.org/pdf/2106.07889.pdf ","date":"11-07-2022","objectID":"/blog/univnet/:0:4","tags":["deeplearning","python","TTS"],"title":"Univnet","uri":"/blog/univnet/"},{"categories":["pytorch","python","distributed_training"],"content":"Distributed Training ","date":"08-11-2022","objectID":"/blog/distributed_training/:0:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Why? Need more compute power to process large batches in parallel (DDP) Uses collective communication Large model that couldn’t be fit in memory of one GPU (RPC) Uses P2P communication All of the above XD ","date":"08-11-2022","objectID":"/blog/distributed_training/:1:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DDP in Pytorch Every GPU has a model replica, controlled by a process. Every process fetches different batch of data. Forward. Overlapping between computation of and communication(broadcast - allreduced) of gradient. Validation ","date":"08-11-2022","objectID":"/blog/distributed_training/:2:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"4 steps-recipe to Distributed Training ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Initialize Distributed Group init_process_group(backend='nccl') ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Data Local Training # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) Distributed Training # Create distributed sampler pinned to rank sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) # May be True # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, # Must be False! sampler=sampler) ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Model Local Training # Initialize the model model = create_model() Distributed Training # Initialize the model model = create_model() # Create CUDA device device = torch.device(f'cuda:{rank}') # Send model parameters to the device model = model.to(device) # Wrap the model in DDP wrapper model = DistributedDataParallel(model, device_ids=[rank], output_device=rank) ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:3","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Saving and Logging if rank == 0: torch.save(model.module.state_dict(), 'model.pt') ","date":"08-11-2022","objectID":"/blog/distributed_training/:3:4","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DP vs DDP ","date":"08-11-2022","objectID":"/blog/distributed_training/:4:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DP: Can’t scale to multiple machines Single Process, multiple threads Module is replicated on each device, and the gradients are all summed into the original module Doesn’t give the best performance, as a result of the GIL problem with multi-thread applications in python ","date":"08-11-2022","objectID":"/blog/distributed_training/:4:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"DDP: Can be used for Single machine, multiple GPUs training, or for multi-node training It initiates process for every device (eg. 2 nodes, with 4 GPUs each = 8 processes) Gradients are gathered using “all_reduce” operation It’s advised to use DDP for any distributed training ","date":"08-11-2022","objectID":"/blog/distributed_training/:4:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Torch.Distributed.Launch vs Torchrun ","date":"08-11-2022","objectID":"/blog/distributed_training/:5:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Distributes Launch scripts: We need to run the script on every node, with the correct ranks We have to pass it all necessary environment variables Example #!/bin/bash MASTER=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1) #Launch the pytorch processes, first on master (first in $HOSTLIST) then on the slaves RANK=0 for node in $HOSTLIST; do echo \"RAnk is \"$RANK srun --nodes=1 --gpus=$GPUS_PER_NODE \\ python -m torch.distributed.launch \\ --nproc_per_node=$GPUS_PER_NODE \\ --nnodes=$NNODES \\ --use_env \\ --node_rank=$RANK \\ --master_addr=${MASTER} \\ --master_port=$PORT \\ train.py \\ arg1 \\ arg2 \u0026 sleep 1 echo \"node is\" $node echo \"node id is \"$RANK RANK=$((RANK+1)) done wait Watch out for “\u0026” ","date":"08-11-2022","objectID":"/blog/distributed_training/:5:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Torchrun: We run the script only once, and it runs it on all nodes It adds all environment variables, and we can use them directly in the code Example: srun torchrun \\ --nnodes=2 \\ --nproc_per_node=$GPUS \\ --rdzv_id=123456 \\ // worker group ID --rdzv_backend=c10d \\ // Communication library --rdzv_endpoint=${MASTER} \\ test.py Overall, Torchrun removes a lot of mundane steps. ","date":"08-11-2022","objectID":"/blog/distributed_training/:5:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Hugging Face ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Option 1: using HF trainer You can use HF trainer, but in this case, you need to manually run the training script on every node ( torch.distributed launch with for loop, or using torchrun once) ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Option 2: using HF accelerator You just need to run the script using the accelerate library. You need to create the training loop manually, and can’t use HF trainer then ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:2","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"PyTorch Lightning PyTorch Lightning is the easiest in running distributed training We pass in the number of nodes, and number of GPUs per node to the trainer Calls your script internally multiple times with the correct environment variables ","date":"08-11-2022","objectID":"/blog/distributed_training/:6:3","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Common errors: ","date":"08-11-2022","objectID":"/blog/distributed_training/:7:0","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["pytorch","python","distributed_training"],"content":"Dict names issue: Problem: When we wrap our model with DDP, pytorch adds (module.dict_key) for all keys in the state_dict Solution: We need to add a function, that detect if we are running distributed training or not, and add or delete “module” from all keys accordingly ","date":"08-11-2022","objectID":"/blog/distributed_training/:7:1","tags":["pytorch","python","distributed_training"],"title":"Distributed Training in PyTorch","uri":"/blog/distributed_training/"},{"categories":["life","python","programming"],"content":"Some facts that I learned by time","date":"05-23-2022","objectID":"/blog/programming_facts/","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Intro this is intended to be some kind or reference to go through whenever I face some kind of bug, or error, that I don’t know how to solve.usually these kinds of error that doesn’t make sense, or we don’t know the cause of them, and the worst it, that we don’t find many ppl facing the same issue, so internet can’t be so useful then. I learned theses facts the hard way, spending so much time trying to figure out the root of the issue. Debugging Facts here are some tips, to help whenever you are facing some error, trying get a package to work, or you these kinds of error that takes you life few days, you know. ","date":"05-23-2022","objectID":"/blog/programming_facts/:0:0","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Package installation issues ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:0","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Python version this might sound trivial, but every package version, only works with some python versions so you should run pip install package-name== to get the package versions supported by your python version ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:1","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Update Pip whenever there’s a dependency conflict, or versions conflict, and you can’t install a package, then check your pip and update it if possible I spent like 4 days clueless why a simple package that I installed a week before, won’t install now, like out of a sudden, it won’t install anymore, due to dependency packages conflict when I upgraded pip, it simply worked ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:2","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["life","python","programming"],"content":"Check for broken installations many times we would try to install some something, then for whatever reason, it wouldn’t complete successful. But then we would try to install again, and we would encounter strange errors, that we don’t know the root for them. there error could be because there’s a broken installation, that messed things up. so, when we delete, or remove this broken installation, and install again, it just works. ","date":"05-23-2022","objectID":"/blog/programming_facts/:1:3","tags":["life","python","programming"],"title":"Programming Facts","uri":"/blog/programming_facts/"},{"categories":["deeplearning","python"],"content":"A Summary of DL papers","date":"04-22-2022","objectID":"/blog/papers/","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Decoupled Neural Interfaces using Synthetic Gradients In NN, the training process, has 3 bottle-necks forward lock: you need to calculate teh output of the previous layer before you can can go into next layer in forward pass backward pass: the same, but for backward propagation weights lock: you can’t update weights unless you do for weights in next layer the paper trying to unlock these bootle-necks by decoupling each layer, to be sufficient alone it does that by introducing, a Synthetic Gradient Model, that can predict the gradient for the current layer, without waiting for the gradient of the next layer this was we can calculate gradient and update weights as soon as we calculate the activation of the current layer ","date":"04-22-2022","objectID":"/blog/papers/:1:0","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Synthetic Gradient Model can be just a simple NN that is trained to output the gradient of the layer it can be trained using the true gradient, or even the synthetic gradient of the next layer it’s important that the last layer computes the true gradient, as in the end we must have a ground truth to can calculate a true loss, and the NN would actually train we can have also synthetic model for forward pass, that works with the same idea ","date":"04-22-2022","objectID":"/blog/papers/:1:1","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"A Roadmap for Big Models We are in the Era of Big Models Model generalization is hard, models trained on certain data domain, doesn’t scare to other Datasets creation, and high research tasks, made it hard for small companies to train task-specific models Big models solve thees issues. ","date":"04-22-2022","objectID":"/blog/papers/:2:0","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Big Models Big-data driven Multi-task Adaptive can fine-tuned with few-shot learning Data issues data bias data duplication data has to cover all domains low quality data hard to create huge datasets ","date":"04-22-2022","objectID":"/blog/papers/:2:1","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Knowledge a new way to represent data we represent knowledge as knowledge graphs KG consists of: Instances, Relation, Concept, and Values KG can be created using : experts, wiki-based knowledge graphs, or extracted from unstructured texts KG Completion and Integration most of the known KGs has many fields empty, and there’s a going research in how to deal with that and fill the gaps. some methods try to do that using intra-graph knowledge augmentation or with inter-graph. ","date":"04-22-2022","objectID":"/blog/papers/:2:2","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["deeplearning","python"],"content":"Denoising Diffusion Probabilistic Models Forward diffusion process: gradually keep adding noise to the original image till it’s destroyed the main task is to reverse the noising procedure, so then we can learn the underlying data distribution, then we can generate images from it instead of calculating the steps of the forward diffusion process sequently, we can combine all the steps in one step, by sampling from a distributuion which have mean of the product of all means in each step $\\begin{aligned} q(x_t | x_0) = x_t \\sim \\mathcal{N}( \\sqrt{\\bar \\alpha} x_0 , (1 - \\bar \\alpha ) \\mathcal{I})\\end{aligned}$ ","date":"04-22-2022","objectID":"/blog/papers/:3:0","tags":["deeplearning","python"],"title":"Deep Learning Papers Summarization ","uri":"/blog/papers/"},{"categories":["git","version-control"],"content":"Trying to get familiar and understand git more ","date":"03-15-2022","objectID":"/blog/git/","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Beatiful commands git log --oneline --decorate --all --graph git merge --abort ==\u003e abort merge, and get back like it never happened git reset --hard ==\u003e is your way to lose all uncommited work in your working directory git fast forward is basically that git moves the commit pointer upward to the new posotion, without creating a merge commit or anything you can merge with --no-ff flag, to disable the fast forward merge and force git to create the merge commit ","date":"03-15-2022","objectID":"/blog/git/:1:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Git Bisect used when something broke, and you know what did broke, but you can’t figure out when did it broke you just give it a testing criteria to test the commit history against ","date":"03-15-2022","objectID":"/blog/git/:1:1","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Methodology everything inside git is an object all your local branches are located in .git/refs/heads a branch is basically a file that appoints to a commit. a branch is bisacally a pointer to specific commit every commit has a parent, so to assemble branches we follow and compute their parents ","date":"03-15-2022","objectID":"/blog/git/:2:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Commits keep added changes in commits related to the same topic add informative commit message you can add parts of changes in a single file using -p flag in git add -p filename0 ","date":"03-15-2022","objectID":"/blog/git/:3:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Branching ","date":"03-15-2022","objectID":"/blog/git/:4:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Long-running branches Main branch Dev branch ","date":"03-15-2022","objectID":"/blog/git/:4:1","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Short-lived branches features branches bug fixes branches ","date":"03-15-2022","objectID":"/blog/git/:4:2","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Merging When the one of the two branches has the head is the same as the common ancesstor of the two branches, then we can do a fast-forward merge by putting the commits of the another branch on top the common ancesstor commit ","date":"03-15-2022","objectID":"/blog/git/:5:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["git","version-control"],"content":"Rebase rebase puts the commits of the second brach on top of the common ancesstor commit then rebase the commits of the first branch on top of the last commit of the first branch, then it changes the history of commits Only use rebase to clean local commit history, don’t use rebase on commits that is pushed to online ","date":"03-15-2022","objectID":"/blog/git/:6:0","tags":["git","version-control"],"title":"GIT","uri":"/blog/git/"},{"categories":["deeplearning","python","NLP"],"content":"Post for Stanford NLP Course","date":"03-08-2022","objectID":"/blog/stanford_nlp_cs224n/","tags":["deeplearning","python","NLP"],"title":"Stanford CS224N NLP with Deep Learning","uri":"/blog/stanford_nlp_cs224n/"},{"categories":["deeplearning","python"],"content":"Long Course that gives a good breadth over many Deep Learning subjects. It covers topics by reviewing the research papers ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Reference Course Repo with the slides, and course info ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:1:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Deep Learning overview we can look at deep learning as an algorithm that writes algorithms, like a compiler in this case the source code would be the data: (examples/experiences) excutable code would be the deployable model Deep: Functions compositions $ fl f{l-1} …. f_1$ Learning: Loss, Back-propagation, and Gradient Descent $ L(\\theta) \\approx J(\\theta)$ –\u003e noisy estimate of the objective function due to mini-batching. That’s why we call it stochastic Gradient Descent why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it’s N*N, so it’s computationally expensive and slow ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:2:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Optimizers to make gradient descent faster, we can add momentum to it. another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum RMSprop: A mini-batch version of rprop method. the original rprop can’t work with mini batches, as it doesn’t consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there Adam: can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta) can also has momentum for all parameter wich can lead to faster convergence Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:2:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Dropout A simple method to prevent the NN from overfitting CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. Computer Vision ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:3:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Image Classification ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Large Networks Network In Network the main idea is to put a network inside another network they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers this idea is bisacally a (one to one convution) they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels from the last conv layer to form the output layer one by one convolution is a normal convolution with fliter size of 1 by 1 in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant) we can achieve local invariant with pooling, and deal with global invariant with data augmentation VGG Net Local Response Normalization: the idea is to normalize a pixel across nearing channels after comparing nets with lrn and nets without, they didn’t find big difference, so they stoped using it Data Augmentation Image translations( random crops), and horizontal reflection altering the intensities of the RGB channels scale jittering GoogleNet You stack multiple inception modules on top of each ohter the idea is that you don’t have to choose which filter size to use, so why don’t use them all to make the network more efficient, they first projected the input with one by one convolution then applied the main filters you concatinate the many filters through the channel dimension Batch Normalization The main goal of batch normalization is to redude the Internal Covariant Shift we can just normalize the inputs and it would work fine the problem is that in each following layer, and statistics of its output would depend on its weights so we also need to nomalize the inputs in hidden layers here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen in inference we can’t have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset conv layers for conv layers we apply normalization across every channel for every pixel in the batch of images the effective bach size would be ==\u003e mpq where m is the number of images in the batch and p,q are the image resolution Benifits of batch norm: you can use higher learning rate, as the training is more stable less sensitive to initialization less sensitive to activation function it has regularization effects, because thre’s random mini batch every time preserve gradient magintude ?? maybe –\u003e because the jacobian doesn’t scale as we scales the weights Parametric Relu: $ f({y_i}) = \\max(0,y_i) + a_i \\min(0, y_i) $ if $a_i = 0$ –\u003e Relu if $a_i = 0.01$ –\u003e Leaky Relu the initialization of weights and biases depends on the type of activation function Kaiming Initialization (I didn’t fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations): professor went into deep mathematical details into how to choose the intial values for weights the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we want the weights to have values centred around 1 Label smoothing regularization the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset ResNet The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection. Identity mapping in resnets the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes Wide Residual Networks a","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Small Networks Knowledge Distillation the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter T that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference. Network Pruning: all connections with weights below a threshold are removed from the network weight are sparse now then we can represent them using fewer bits Quantization we basically cluster our weight to some centroids the number of centroids for conv layers are more than the ones for FC layers why: because conv layer filters are already sparse, we need higher level of accuracy in them FC layers are so dense that we can tolerate fewer quantization levels Huffman Coding store the more common symbols with more bits Squeeze Net the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made the main idea is to use one by one comvultion to reduce the dimensionality XNOR-NET the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==\u003e $W = \\alpha * B $ where alpha is postative 32 bit constant and B is a binary matrix then mean we try to train by using a means square error loss function of the original weights and alpha and B I still can’t fully understand how to binarize the input Mobile Nets the idea is to reduce computation complexity by doing c onv for each channel separately, and not across channels. so we use number of filters as the same as the input channels but then we will end up with output size as the input size, so we still need to do one by one convolution to output the correct size Xception unify the filters sizes for the inception, and then apply them for each channel separately, then do one by one convolution to fix the output size Mobile Net V2 the same as MobileNet, but with Residuals connections. ShuffleNet the idea is to shuffle channels after doing a group convolution ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Auto ML the question is can we automate architicture engineering, as we automated feature engineering in DL? we can use RNN to output a probability, to sample an architicture from, then use train using this arch, and give the eval acc, as a feedback to the RNN Regularized Evolution it’s basically random search + selection at first you randomly choose some architecture train, and eval on it and push it to to the population then you sample some arch. from the population then u select the best acc model from your samples , and then mutate it (ie. change some of its arch.), then add it to your samples then remove the oldest arch. in the population you keep repeating this cycle till you evolve for C cycles (history size reaches the limit) and report the best arch. EfficientNet the idea is that we do grid seach on a small network to come with the best depth scaling coefficient d, width scaling coefficient w, and resolution scalling coefficient r, then we try to find scaling parameter $\\phi$, that gives the best accuracy while maintaning the flops under the limit ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Robustness The main goal is to make your network robust against adverarial attacks Intrigiong peroperties of neural networks there’s nothing special about individual units, and the individual features that the network learn, and they you can interpret any random direction. So, the entire spacd matters neural networks has blind spots, this means you can add small pertirbations to an image, they are not noticable to the human eye, but they make the network wrongly classify the image Adversiral examples tend to stay hard even for models trained with different hyper-parameters, or ever for different training datasets you can train your network to defend against attacks but that’s expensive, as: first, you have to train your network, then train it again to find some adversiral attacks, then add those examples to the training set, and finally train for a third time. small perturbation to the image, leads to huge perturbation to the activation, due to high dimensionality untargeted adversiral examples fast gradient sign: using the trick of the sign of the loss gradient, and add it to the original image to generate an adversiral example then you can just add a weighted loss, one for the orginal example, and another for the adversiral one, so that the network would be more robust to adversiral examples Towards Evaluating the Robustness of Neural Networks another way to generate targetted adversiral examples is: to choose a function that forces the network to make the logits for the targeted example the biggest, so that this class is selected. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:4","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Visualizing \u0026 Understanding now we want to debug our network, to understand how it works so we want do a backward pass, by inverting our forward pass, but then we habe a problem with pooling layers as we subsamples the input. so we store the locations for the max pixels that we choose in our pooling operation, so that we can upsample the input again in the backward pass. we call these max locations, switches the main idea is, visualising the feature maps, gonna help you modify the network you can have two models that have the same output for the same input but which one do you trust more? to answer that, you need to see which features each one of them focuses on, so if one of them focuses on features that are important to classfication, then this model is more trustworthy LIME: Local Interpretable Model-agnsortic Explanations you want to trust the model, meaning that you wanna make sure the model prioritized the important features but you can’t interpret non linear models, so the idea is to make a locally linear model, that have the same output for your local input example, then use this linear model to get the features that the model prioritized Understanding Deep Learning Requires Rethinking Generalization NN are powerful enough to fit random data, but then it will not generalize for test data so when we introduce radom labels, random pixels, etc: we still can go for 0 train loss, but for test data, the error is gonna be equal to random selection. so, this means: The model architecture itself isn’t a sufficient regularizer. Explicit regularization: dropout, weight decay, data augmentation Implicit regularization: early stopping there exist a two-layer NN with Relu activation, that can fit any N random example ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:5","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Transfer Learning labled data is expensive you split a data set in half,we find that transfer learning for the same task, have higher acc' transfer learning with fine-tuned weight is better than locking the learned weights on average you just wanna cut the network in he middle and start learingn after few layers, as the first few layers ar more general leayers and can acctually help you in traninge for another task DeCAF first layers learn low-level features, whereas latter layers learn semantic or high-lebel features ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:4:6","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Image Transformation ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Semantic Segmentation you want to segments different classes in the image The fully connected layers can also be viewed as convoluting with kernels that cover their entire input regions. Atrous Convolution: you don’t wanna lose much info when you do conv, and then upsample again, so you fill your filter with holes, so that you lose less info reduce the degree of signal downsampling CRF: deals with the reduced localization accuracy due to the Deep Convolution NN invariance Dilated Convolution: basically atrous convolution increases the size of the receptive points layer by layer ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Image Super-Resolution we want to develope a NN that can up-sample images we can do that using convolution and in the middle we use one to one convolution to work as non-leaner mapping ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Perceptual Losses mse isn’t the best for images, for example, if we shift an image by one pixel in any direction, we will end up with huge loss, while the two images are the same the idea it to use a CNN like VGG-16 to calculate the loss, this works because any CNN would have some perceptual understanding of the images so we push the output of our model, and the target (label) through a NN, and compare the feature maps on different layers Single Image Super-Resolution(SISR) the idea is to make the network to only learn the residual not the full image, so it just learns the difference between the two images ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:5:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Object Detection ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:6:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Two Stage Detectors R-CNN we enter the input image into extract regions algorithm. this algorithm is cheap algorithm that output millions of boxes per image. we do that using an algorithm called “selective search” we then enter that to a CNN to do features extractions at the end we have a per-class classifier Spatial Pyramid Pooling the idea is that we use spatial pyramid pooling to have a fixed length representation for the image also we push the input image once through the conv layers, then choose multiple windows after to do the classification for. this way we cut so much on computations cause we for the first few conv layers, we pushed just one image Fast R-CNN just like RCNN but, changed the multi-class SVM with multi-task loss, this way we don’t have to calculate many classifiers, one for each class. also we don’t need bounding box proposals, and we can acc train a Region Proposal Network, to propose bounding boxes for every pixel in the feature map. last trick is to use a CNN instead of the FC head at the end of the network, but CNN is translation invariant, so we need to do pooling for each region separately. Feature Pyramids the idea is that we need to use different versions for our input image, each with different resolution, so that we detect objects with different sizes. to do that we can use the different features maps at different layers, so that at each layers the resolution changes, and we can use that to choose our windows the problem is that each layer represent a different semantic meaning of the image, so the first few layers consider the image colors, while the last few consider the more complex shapes of the image to overcome this, from each layer we add a connection to the layer below so we up sample the feature maps first then do one by one convolution to adjust the number of channels, ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:6:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"One Stage Detectors YOLO we want the detector to be realtime, so we can detect objects live divide the input image into S * S grid if the center of an object fell inside a cell, that cell is the one responsible to detect that object each grid cell gonna predict, B bounding boxes, each with confidence score SSD: Single Shot MultiBox Detector we want to take the speed from YOLO, and the high acc from the two-stage detectors unlike YOLO, we can use early layers, not just the last layers of the network, and for each one we can predict more boxes, so we end with much more boxes than YOLO YOLO9000 - YOLO V2 Tries to improve upon YOLOv1 using idead from fast-CNN and SSD we can use higher res images in training can the anchor boxes from the training images not just randomly introduced passthrough layer used hierarchical classification to extend many classes ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:6:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Video ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Large-scale video classification we would have a context stream which learns features on a low res frames. and a Fovea Stream, which operates on a high res middle portion of the frames Early Fusion the idea is to take a few frames from the middle of the clip, and apply conv on them, the only diff is that we add a new dimension to filters which coreespond to the number of frames that’s just for the first layer, but then it’s normal conv Late Fusion we have two separate single-frame networks, each one takes a diff frame from the clip, and we concatenate them in the end ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Two-Stream CNN for action recognition video can be decomposed into spatial and temporal components Optical flow stacking we can just follow pixels from frame to another, and then create a flow vectors, in the x,y axis then we can stack these flow vectors Trajectory stacking follow the point from frame to another ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Non-local Neural Network the idea is that we want to see for output pixel, which areas did it pay attention to in the input so we attention every output with all possible pixels in the input if we are using it with videos, then we add another dimension for the time ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Group Normalization Batch norm, is good as long as we have reasonable batch size but whe we have very small batch size, then batch norm isn’t the best Here’s diff between normalizaiton methods: Natural Language Processing ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:7:4","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Word Representation ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:8:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Distributed Representation of Words and Phrases and their Compositionality Word2Vec ( Efficient Estimation of Word Representation in Vector Space ) using CBOW model, or skip-gram model uses the cosine-similarity distance function Skip-gram Model Continuous Bag of Words is the opposite of the skip-gram in the sense of what are we predicting (word vs context) the idea is that you pick a word, and try to predict the context around it so you have a word in the middle and try to predict words around in (before, and after), given a defined window size and our objective is to maximize the liklihood of the context given the reference word we can use binary trees, to do an approximation, and speed up the softmax caculation, as for every word in the vocab, we would calculate it’s softmax with all other words,but now we can use binary trees, and do that in just log(n), using an approximation, that we group words together, and in each level coming from the root, we go right or left, till we reach the word in the leaves we can make this even faster, using huffman encoding to assign shorter paths for more frequent words noise sampling: the idea is to give the model negative samples, that doesn’t appear together, and give it low probability Evaluation we can evaluate the model, using syntactic, and semantic analogies for example, Berlin to Germany is like France to Paris GloVe: Global Vectors for Word Representation the idea is to use: global matrix factorization methods local context window methods we compute a co-occurrence method, that holds the counts every two words come after each other, we try to learn two matrieces and two biases, that log(X) = w1 * w2 + b1 + b2 ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:8:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Text Classification ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:9:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Recursive deep models for semantic compositionality over sentiment Treebank the dataset is presented as a tree with leafs as words the idea is that for evert word we get it’s embedding, then we multiply that by a weight matrix, and apply softmax, so then we have probability distribution over our classes (sentiment classes) then we can concatenate every words together, and keep recursing till we finish the whole sentence the problem with this model, is that we are losing the pair wise interaction between the two words, wat we can do it introduce a tensor V, that would capture this interaction ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:9:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"CNN for Text Classification we want to use CNNs with text, so we would have some filters the idea is to treat sentences as one dimensional vector, and then we can apply windows that contain bunch of words to some filters, and aggregate them ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:9:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Doc2Vec as we have representation of words, we can also have the same for sentences, or documents ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:10:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Bag of words for each sentence, count the frequency of each word in your vocab weakness lose ordering or words lose semantics or words “powerful” should be closer to “strong” than “Paris” ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:10:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Paragraph vector for every paragraph, we would have a vector representing it, then we can average those together, and try to get the target paragraph we can do it as CBOW, and instead of words, we would have paragraphs ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:10:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"FastText the idea is that we take a sentence(a bag) of words, or N-grams and then sum their vectors together, then project them to latent space, and then project them again to output space, and apply non-linearity (softmax for example), then apply cross-entropy as a loss function we can also normalize our bag of features (the word representation), so we down weight most frequent words instead of softmax, we can use hierarchial softmax, to decrease the training time this model is super fast, and gives results similar to non-linear complicated models like CNNs ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:11:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Hierarchial Attention Networks for Document Classification the idea is that we want to do document classification so in the end we want to represent the document by a vector, that we can enter to softmax, and then output a class we can use think of document as they are formed of sentences, and sentences are formed of words so we can use GRU based sequence encoder to represent words and then sentences ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:12:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"GRU we have a reset gate, and update gate the idea is that at every step we have a previous hidden output, and a current input then we have an update gate that determines the percentage to take from the current hidden output, vs the previous hidden output. then we also have a reset gate, which determines how much we wanna take from the previous state when calculating the current state we use the tanh function to calculate the hidden state we use the sigmoid to calc the parameter Z, which tell us the percentage between the current state, and the previous state output we can have a forward, and a backward GRU, and concat them then we project these concat words representation, and apply non-linearity then to calculate the sentence vector out of these word vectors, we apply a weighted average on them. this works like a simplified version of attention we can do this weighted average using softmax, but first we need to turn this vector to a scaler, which we can do by applying dot product with “query” or “word context”, like we are doing a query: what is the informative word this will get us with the alphas, which tell us how much we take from each word vector NOTE: cross-entropy with one-hot vector is the same as the log-liklihood ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:12:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Neural Architecture for Named Entity Recognition ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:13:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"LSTM-CRF Model Normal LSTM has 3 gates, input gage, forget gate, output gate. Conditional Random Field in NER, the named tags are dependant on each other, for example: B tag, and I tag. so we want to account for that in our loss to do that we introduce a new compatibility matrix, to count for this dependency Character-based models of words we need it cause, in production, we might encounter new unseen words, so we make up for that using the character encoding we want to add character representation with our words representation so we do a character-based bi-LSTM and we concatenate the output of the LSTM, with our word representation from the lookup table ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:13:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Universal Language Model fine-tuning for Text Classification (ULMFiT) Language Model: a model that trying to give an understanding for language. Like given few words of the sentence, can we guess the next word. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Disctimonative fine-tuning so the idea is to split the model pre-trained parameters for each layer and to also choose a learning rate for each layer the early layers would have smaller learning rate, so their weight wouldn’t update as much ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"slanted triangular learning rate the idea is just to increase the LR gradually, till some point, then decrease in again and we do the increase and decrease linearly, so we end up with the triangular shape ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"gradual unfreezing gradually unfreezing parameters through time ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:14:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Natural Machine Translation bu Jointly Learning to Align and Translate we wanna model the conditional probability $ p(y|x)$. where x is the source sentence, and y is the target sentence ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"RNN Encoder-Decoder Encoder the encoder gonna encode our entire sentence into a single vector c we can use LSTM, which will output a sequence of vectors $ h_1, h_2, \\dots ,h_T$. we can choose c to be just the last vector of the LSTM $h_T$ ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Decoder we can model $p(y|x)$, as that product of $y_i$ for i from 0 to input time T. but we can do an approximation, that instead of X, we calc using C which is a representation of X. and instead of using the previous Y outputs in previous time steps, we can use the previous hidden state ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"BLUE Score ( Bilingual Evaluation Understudy ) Good reference provides an automatic score for machine translation the normal precision gives terrible results they introduced a modified precision score, which gives score to words up to the maximum number of occurrences in the reference sentences we need also to account for different grams. for example, for bi-grams, we would count the bi-grams in the output, and count-clip them at the maximum of the bi-gram in the reference sentences Pn = BLUE score on n-grams only Combined Blue score: Bp exp(1/n * sigma(Pn)) Bp: brevity Penalty it basically penalize short translations Bp: is one if output is longer than reference otherwise, it’s exp(1 - (output length / reference length)) ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:15:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Sequence to Sequence Learning with Neural Networks One limitation of RNNs is that the output sequence, is the same length as the input sequence this is using two different LSTMs one for input, and one for output it stacks multiple LSTMs together creating deep LSTM reversing the order of the words of the input sentence the intuition is that the first of the output is gonna take most info from the first tokens of the input ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:16:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Phrase representation they combined DL approaches like RNNs, with statistical ML approached to enhance the translation we cen learn word embedding from the translation task ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:17:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Attention-based Neural Machine Translation attention solves the problem of decreasing Blue-score with increasing the sentence length ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:18:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Global Attention with previous approaches, we used a small version of attention, to choose which source vector would have the bigger weight in this version we do the same, but with different, source-target hidden state vectors so we attend source and target hidden state vectors ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:18:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Local Attention instead of attending to all of the input space, we can jus attend to a portion of it it’s faster than global attention how to choose the portion to attend to, is learnable ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:18:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Byte Pair Encoding the main objective is to reduce the amount of unknown words the idea is to iterate over the bytes in the sequence and pick the most frequent one and replace it with an unused byte a byte is a character, or a sequence of characters we keep doing that till we convert our input sequence to bunch of bytes at test time, we would have Out Of Vocab words, but we can convert them to known bytes that we extracted in training. we are trying to find a balance between word-encoding and character-encoding ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:19:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Google’s Neural Machine Translation the first paper to beat statistics MT methods we will use encoder RNN to encode the input then we will use attention, to attend to the input they added 8-layer LSTM with residual connection they used (Byte-Pair) word-pieces technique the loss function, is the log likelihood of the output, conditioned on the input, but we wanna to add the GLeu score to penalize depending on the quality of the translation, so we can add the Gleu score to the loss function as a reward, in RL the did beam-search which penalized small sentences, and added penalty for long sequence contexts lastly, they quantize the model, and its parameters in inference ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:20:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Convolution Sequence to Sequence Learning we want to use the parallelization of the CNNs to learn seq-to-seq we add positional embedding to account for the different sentence positions. we didn’t need to do this for RNNs as they process words sequentially by default The network has and encoder, and a decoder the encoder, process all input the decoder, only considers the previous inputs we have a stack of conv blocks for each convolution block, you take a k words, each is d dimensional, then you flatten them to be (kd) dimensional, and apply convolution, which is multiplying by filter of size (2d,kd), then you have an output of (2d) dimension. you talk the first half and dot product it with the sigmoid of the second half. there we applied the non-linearity. and for each block, we also add residual connection. lastly, we add attention between our encoder blocks output, and the decoder blocks output ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:21:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Attention Is All You Need in RNNs and CNNs, there’s this inductive bias, that the useful information, is right next to us. while, is Attention we don’t assume that Just as all attention based models, we need to add positional encoding they do that by fourier expansion all previous work was cross attention between encoder, and decoder here they introduced, self-attention, where the encoder attend to its inputs Then the added residual connection and layer normalization ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"One Head Attention we have Query, Key, and Value. we multiply each of them with a weight matrix to add learnable parameters first, we do attention between, the query and the key, they we down-scale the dot product by the square root of the embedding dimension. we choose the square root, because it’s not big nor small number, so we keep the attention weights in a reasonable range then we do a weighted sum between the attention weights and the Value matrix ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Multi Head Attention then one the most important ideas here is multi-head attention we make many single head attention, then concatenate them together ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Decoder the same as the encoder the main difference, is that we do masked attention, which only attend to previous outputs only Here, the query is coming from the output sequence, and the key, and value, are coming from the encoder ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:22:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Subword Regularization we want to have my many Subword tokenization for the same word the multiple subword tokenization works like kind of data augmentation, and also adds a regularization effect ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:23:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Transformers are RNNs The idea is that we don’t have to use softmax fuction, to caputer the similarity between two tokens, we can use any other suitable function. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:24:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"ELMo We need to have multible word representation for the same words, depending on the context. we take all hidden repreasentations from all LSTM layers, in both forward, and backward we then take the Elmo representation, and add it to the different tasks, we can add it in the input or the output, for our task-specific network we can aslo finetune the elmo parameters in the downstream tasksdd ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:25:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Forward language model you predict the next word given the previous words ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:25:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Backward language model you predict the next word given the next words ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:25:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"GPT1 Just like Elmo, but use transformer decoder, instead of Lstm your hidden repersentation contains, token embedding and position embedding you have two outputs (heads)one for the downstream task, and another for the text prediction Loss function is weighted average of the two losses ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:26:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Bert same concept of pre-training and fine-tuning uses transformer encoder you start by masking some of the input tokens task two, is doing “next sentence prediction” we traing the model on those two tasks Position Embeddings could be learnable or fixed ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:27:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"GPT2 we used to predict the next word or sympol conditioned on previous words or tokens it would be way better to condition on also the given task we want to do ‘model-agnostic meta-learning’ they changed the input format to be probpts like translate to Arabic, English text, Arabic text intersting thing about language, is that NLP tasks are similar so they was able to answer some task-specific questions using zero-shot learning just based on the input prompt ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:28:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"ALBERT they did factorized embedding parameterization this does huge parameter reduction it has cross-layer parameter they changed the second task ‘NSP’ to sentence-order prediction, and this task is harder than the previous one, as the negative examples would be harder to lear, as the positive examples and negative ones are the same two sentences swapped they found that removing dropout helps the training ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:29:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Roberta bert was biased towards the masked tokens, so to fix that, they didn’t mask all selected mask-tokens, but masked them with 80%, and left them unchanged with 10%, and replaced them with random vocab with 10% Modifications on top of Bert: bigger dataset train for longer time change the masking pattern a bit replace NSP, with POS (forces the model to learn harder task, by flipping the order of two consequtive sentences) bigger bathces and bigger number of tokens ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:30:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"DistilBERT we want to have a model that is compact for production we use the knowledge distilattion technique we train a small student model using a larger teacher model we use the same sof-temperature form the student and teacher ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:31:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Triple Loss to get 97% of Bert acc, they needed to apply more loss they trained the student model on small dataset, and introduced masked language modeling loss they added cosine embedding loss, to align the directions of the student and teacher hidden states in addition to the distilation loss ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:31:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Transformer-XL rop with it, so we stop the gradient for the extra context vectors this means if our context size in the idea is that we want to increase the context in the decoder to capture more data for normal transformer, we only have fixed size context vector to caputre the previous data but here, we take all the previous context with us in the forward pass, but this can be huge to do back p5 words, we will consider all previous context in the forward, but only calculate the gradient for jsut these last 5 words in order for this to work, they made relative position encoding ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:32:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"XLNet the idea is that they wanted to gather the two ideas of Autoenconding and autoregressive language modeling they did that by applying different permutations for the input sequence, instead of jsut the forward and backward products ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:33:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"T5 it’s a text to text model (seq-to-seq) Bert model used to be biased towards mask tokens, so they masked with many different masks they masked consequtive tokens, not just one token per mask Uses Prefix LM masking strategy uses sentence-piece tokenization (uni-gram encoding) ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:34:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Don’t stop pretraining the idea is why to pretrain on large dataset, then finetune for specific task instead, they propose that we keep pretraining on large dataset, then on domain-specific unlabeled data, then pretrain on task-specific data, then finally finetune with labeled data ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:35:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Steps: first pretrain on the whole web then pretrain on the target domain then use k-means lgorithm to pick smaller refined dataset to your task then pretrain on the smaller dataset finally finetune using labeled data on the specific task ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:35:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Cross-lingual Language Model Pretraining we need to work on multiple languages ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:36:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Shared sub-word vocabulary Learn BPE splits on the concatenation of sentences sampled randomly from the monolingual corpora Sentences are sampled according to a multinomial distribution we don’t need our sampling method to be biased towards any language ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:36:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Causal Language Modeling an autoregressive LM, that predicts the next token in the senctence Does this make sense given that our sentces is just a concatenation of sentences from different languages sampled randomly?? ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:36:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Masked Language Modeling just normal MLM, but they add vector to represent the language ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:36:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Translation Language Modeling we have parallel data, of sentces in two languages we can use it to train a MLM to predict masked tokens from the two languages ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:36:4","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"XLM-RoBerta they want to do multilingual Models but they run into the Curse of Multilingulity, which states that for a fixed model capacity, more languages leads to better cross-lingual performance on low-resources languages up until a point, after which thd overall performance on monolingual and cross-lingual benchmarks degrades. This is weird, becuase we wouldn’t expect the low-resources languages performance to degrade with inceasing the number of languages ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:37:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Positive Transfer vs. Capacity Dilution Tradeoff The transfer-interference trade-off: Low-resource languages benefit from scaling to more languages, until dilution (interference) kick in and degrades the overall performance. this happens as a result of limited model capacity to take in all the languages ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:37:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"SpanBERT: we want to mask more than one word, because some entities consist of many tokens, and it’s better to mask them together we can do that by adding another term to the MLM loss, which takes into consideration, when we startd masking, and when we end masking and the position of the current token in this masked span they sample the length of the mask from a geometric distribution, because they want to give high probability for shorter masks to remvoe the bias towards these mask tokens, we do like Albert, and leave chance to keep masked tokens unchanged, or change with random tokens this masking gonna happen on the span level, which means if we decide to change masked tokens with random ones, we gonna do that for all tokens in the span. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:38:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"BART it combines that encoder and decoder parts of the Transformer The key difference here, is that it applies arbitrary transformations to the orignal tokens Examples of transformations: token masking sentence permutations document rotation token deletion text infilling (replacing arbitrary spans to text tokens with single mask token) After that we try to generate the original document again using the autoregressive decoder ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:39:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Longformer (Long Document Transformer) combine a local windowed attention with a task motivated global attention ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:40:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Sliding Window attention used in pretraining Sliding Window attention: only attend to tokens inside a window around you Dilated Sliding Widnow: just like diluted conv, you attend to bigger window around you, but it is filled with holes, like you pay attention to every next two holes Ex: you at pos 0, and attend to : (-4,-2,0,2,4) you can use different dilation configuration for each attention head ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:40:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Global attention used in downstream tasks Task specific Ex: for classification tasks, for CLS, attend to all tokens for QA, use global attention, for every token in the question ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:40:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Sentence-BERT suppose you want to get a similarity score between two sentences, the normal way is to append a FF layer on top of the CLS token on bert, that would map from the 768 dimension vector, to one number which is the similarity if we have 10,000 sentences, then we need to do (N * (n-1) / 2 ) inference computations to get the best similarity. This would take around 65 hours with BERT Another approach is to input all sentences to BERT, and obtain the corresponding vectors for them, then apply cosine similarity on them. This would take roughly 5 seconds. This second approach gives bad results, as BERT is optimized to get masked tokens,and it’s not good at comparing sentences It’s ever worse than averaging Glove embeddings ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:41:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Approach you finetune bert on SNLI dataset (Natural Language Inference tasks) then you take this model and finetune, or use it directly to calculate the cosine similarity ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:41:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"GPT3 Just like gpt2, but with much more data it gives good results for zero and one shot learning, and even better ones for few-shot learning the problems with it, is it doesn’t see the data, it’s talking about, like you are talking the dog, but it doesn’t know the visuals of dogs, that’s why we go to multi-modal models ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:42:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"ELECTRA The depend on Discriminator instead of generator They still use a small MLM generator ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:43:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"SimCSE mainly depends on Contrastive learning here, they depend on the dropout in Bert to give the two related examples so they just enter the same example twice for bert, and every time,it applies dropout and cancels some hidden units ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:44:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Contrastive learning the main idea is to make two represnetations for two close examples (sample, and its augmented version) as similar as possible, and in the same time make the representations as far for different examples ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:44:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Pay Attention to MLPs They main idea is that we don’t need attention to get the high results instead, they just used MLP blocks they achieved comparable results to Transfers, and even surpassed it in some classes lastly, they used attention head on top of the MLP unit, and that boosted the performance to be better than Transfers Multi Modal ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:45:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Show and Tell we want to generate image captions they did that by using Googlenet as a feateure extractor then followed that with LSTMs to generate captions ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:46:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Deep Visual-Semantic Alignments for generating Image Descriptions we have a limitation of labeled images-captions pairs we can utilize that the image would have long captions which describes multiple objects in the image We try to detect those objects them align then with their corresponding captions we start by using R-CNN to get the boudning boxes for objects in the image then we represent every objcet with a vector we then ue RNN to get a vector representing every word we then caluclate teh similirity between each pair of all sentences in the dataset, and all the iamges then they used Markov Random Field to allign words to bounding boxes this depends that we got good representation for the iamges and captions from the training in the previous step some of the captions would be incorrect, but overall we would have increased the size of our dataset ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:47:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Layer Normalization it was motivated because batach nomalization doesn’t work with RNN it doesn’t depend on batch size it takes the mean and variance over the hiddedn units dimension Now, every training examples, gonna have different normalizaiton terms ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:48:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"DALL-E it’s a 12-Billion parameter autoregressive transformer trained on 250 Million image-text pair the used discrete VAE, to tokenize the images ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:49:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Perceiver they are trying to unify the architecture for different data formats (text, image, voice) the input and output depend on the task but we can choose the size of the encoding Generative Networks ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:50:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"VAE The main goals is to learn the latent data distribution It’s like PCA but with nonlinearity It’s doens’t have the same bases as PCA, but it spans the same spaces can be used as noise reduction, if we add some noise to the input the problem ends up as two parts a kl-divergance which tries to limit our learned distribution to be similar to our chosen tractable distribution and expectation of the log of the learned probability, which will basically act like a reconstruction error. and it will be a simple MSE loss in case of gaussian distribution The main point is, VAE, is like a normal autoencoder, but we added a regularization effect to it, which enforce the learned distribution, to stay similar to the chosen distribution (gaussian for example) ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:51:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Gumbel-softmax it tries to make sampling from categorical distribution diffrentiable it all sarted with the reparameterization trick, that allowes us to combine a deterministic part with a stochastic part, which will allow us to diffrentiate and use the back propagation there was gumbel-max before, which used the argmax function. but this function is not diffrentiable so they used softmax instead which is diffrentiable ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:52:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"GANs the main idea is that we want to avoide the likelihood in VAE so instead, we will utilize the ability of NN to do calssification we will train a discriminator, and generator ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:53:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"CV Self-Supervised Learning ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:54:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Deep Clustering the idea is to do unsupervised learning using any CNN architecture, by first doing a forward pass and do clustering on the output, then take these psuedo-labeles and do traditional classification with them. ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:54:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Constrastive Predictive Coding the idea is to mask parts from the images and train the model to predict them but we can’t just use MSE loss, as the model won’t learn any thing instead, they discreteize the output, and used contrastive loss to make the model chose the correct label the more negative examples we use, the higher the chance that there’s gonna be a similar example, which is make it harder for the NN to find the answer we can think of contrastive learining as clustering in the fact that every training point is a centroid of it’s own cluster, and that perterbation to it should be near the cluster ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:54:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Constrastive multiview coding the idea here is to use different view of the same image, and treat them as the positive examples they used the contrastive loss in the hidden dimension space (z space) so, they enode the two images into the z space, then apply contrastive loss minimizing the contrasitve loss is equal to maximizing the mutual information between the different views ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:54:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Pretext-Invariant representataions we want to learn a representation of images that is invariant to the type of the transformation done on the original image we can do that be encouraging the representation of the different transformation to be similar and the represntation of different images to be different the logic behind it, is that the positive pairs are data, and any ohter pair is noise, and we try to increase the probability of data ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:54:4","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Mel Spectrogram we use it cause it reduce the dimention that we use to represent the data with we use the filter banks to change the scale we represent the data with we first chop the wav into chunks or windows, and then for each window, we apply STFT (short time fast fourier transform) then we create the filter banks and then multiply the filter banks with the stft signal to create the mel spectrogram ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:55:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Connectionist temporal classification temporal classification: the task of labelling unsegmented data sequences we have a problem of aligning the output to the labels, cause there might be some repeated outputs and some pauses that we want to get rid of ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:56:0","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"CTC Loss we use it when we have a many to many projection between the input and the output and when there’s no one to one correspondence between the outputs and the true labels we use dynamic programming to get all possible outputs the would collapse to our label, and sum their probabilities to get the ctc loss ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:56:1","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Transducer ctc loss doesn’t take into account the last outputs to genrate the next output, so it predicts each output indivedually to solve this issue, we add something similar to a language model which takes the previous outputs into account Reinforcement Learning ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:56:2","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python"],"content":"Playing Atari with RL We have and environment, and we can take actions that changes this environment, and we can take inputs from the environment in the form of observations, and our actions on the environment has rewards. for atari, we can’t get all the data frrom the raw pizels(for example, we can’t get the direction of the ball from it), so we will take some of our previouse states and actions into account also our actions affects all our future actions, so we will apply future discoundted return ","date":"02-01-2022","objectID":"/blog/applied_deep_learning/:56:3","tags":["deeplearning","python"],"title":"Applied Deep Learning","uri":"/blog/applied_deep_learning/"},{"categories":["deeplearning","python","NLP"],"content":"Course 1: Classification and vector Spaces ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:0:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Weak 4 ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:1:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Hashing We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space Locality senstive hashing the idea is to put items that are close in the vector space, in the same hashing buckets we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly but how can we be sure, that the planes that we chose are the perfect set to seperate our space, we can’t be sure of that, so we create a multi sets of random planes and every set would get us a different way to seperate our words Course 2: Probabilstic Models ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:1:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week 1: Autocorrect to make a simple auto-correction system, you need to perform 4 simple steps: you need to identify the miss spelled words get the n edit distance correct words filter these candidates for correct words in the dictionary change miss spelled word with one that has the highest probability ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:2:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week 2: POS using Viterbi algorithm Part of Speech Tagging (POS) is the process of assigning a part of speech to a word You can use part of speech tagging for: Identifying named entities Speech recognition Co-reference Resolution You can use the probabilities of POS tags happening near one another to come up with the most reasonable output. ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:3:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Markov Chains: You can use Markov chains to identify the probability of the next word. calculate transmission probability calculate emission probability ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:3:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Viterbi algorithm calculates a probability for each possible path a probability for a given state, is the (transition probability * emission probability) total probability of the path, is to product of all states probabilities we use a top down dynamic programming algorithm to build the paths matrix we use sum of logs instead of product of probabilities, to avoid converging to zero values ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:3:2","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week3 Autocomplete ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"N-Gram models: it’s a language model that predicts probabilities of sentences depending on the probabilities of their N-grams to capture the context of beginning end ending of the sentences, we add start and end tokens to each sentence ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Preplexity: a measure to calculate how complex a sentence is humans type low preplex sentences ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:2","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"out of vocab words and smoothing we can add UNK token for unseen words in the vocab we can apply smoothing of interpolation for unseen Ngrams ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:4:3","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Week4: Word vectors using Bag of Words method we can use self-supervised learning in predicting the next word, to learn the weight matrix word2vec continuous bag-of-words predicts a word in context continuous skip-gram tries to predict the words surrounding input word Global Vectors (GloVe) factorizes the corpus word co-occurrence matrix fastText based on skip-gram model support out-of-vocab words Advanced word embedding methods Deep Learning, contextual embedding BERT ELMO GPT-2 ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:5:0","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning","python","NLP"],"content":"Continuous Bag-of-Words Model you choose a center word, and a context words around it, and try to predict the centered word we model the words by one-hot encoding then we prepare the input training example feature to be an average of the one-hot vectors of the context words, and the label would be one-hot vector of the center word after training the model, the word embedding is one of the weight matrices, or an average of them Evaluations Intrinsic evaluation test the relationships between words Analogies Clustering Visualizing Extrinsic evaluation test the embedding on the end task you want to perform (ex. Sentiment Analysis) evaluates actual usefulness of embeddings time consuming more difficult to troubleshoot ","date":"01-30-2022","objectID":"/blog/coursera_nlp_specialization/:5:1","tags":["deeplearning","python","NLP"],"title":"NLP Specialization","uri":"/blog/coursera_nlp_specialization/"},{"categories":["deeplearning"],"content":"CS480/680 Intro to Machine Learning - Spring 2019 Course ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 12 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:1:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Gausain process infinite dimentional gaussian distribution ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:1:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 16 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:2:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Convolution NN a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:2:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Residual Networks even after using Relu, NN can still suffer from gradient vanishing the idea in to add skip connections so that we can create shorter paths ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:2:2","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 18 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:3:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"LSTM vs GRU vs Attention LSTM: 3 gates, one for the cell state, one for the input, one for the output GRU: only two states, one for output, and one for taking weighted probablitiy for the contribution of the input and the hidden state takes less parameters Attention: at every step of producing the output, create a new context vector that gives more attention to the importat input tokens for this output token ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:3:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 20 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:4:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Autoencoder takes different input and generates the same output used in: compression denoising sparse representation data generation ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:4:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 21 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:5:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Generative models Variational autoencoders idea: train the encoder to sample a fixed distribution , we want the network to sample a fixed distribution that is close to the distribution of the encoder, so that it generate similar outputs to the input, but not the same GANS: ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:5:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 22 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:0","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Ensemble Learning: the idea is to combine the hypothesis of several models to produce a better one Bagging - choose the class the majority votes Weighted majority - decrease the weight of corrlelated hypothesis - increase the weight of good hypothesis Boosting - idea: when an instance is missclassified by hypothesis, increase its weight so that the next hypothesis is more likely to classify it correctly - can boost a weak learner - makes weighted training set, so that it can focus on missclassified examples - at the end generate weighted hypotheses based on the acc of each hypothsis - Advantages: - no need to learn perfect hypothesis - can boost any weak learning algo - boosting is very simple - has good generalization Netflex challenge 2006 ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:1","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 23 Normalizing flows ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:2","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deeplearning"],"content":"Lecture 24 Gradient boosting boosting for regression ","date":"12-14-2021","objectID":"/blog/introduction_to_machine_learning/:6:3","tags":["deeplearning"],"title":"CS480/680 Intro to Machine Learning","uri":"/blog/introduction_to_machine_learning/"},{"categories":["deepdearning","VoiceConversion"],"content":"A Research notebook, for Voice conversion models ","date":"11-21-2021","objectID":"/blog/voice_conversion/","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"AutoVC paper Demo Repo ","date":"11-21-2021","objectID":"/blog/voice_conversion/:1:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"Speach Split Paper Demo repo ","date":"11-21-2021","objectID":"/blog/voice_conversion/:2:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"AutoPST Paper Demo repo ","date":"11-21-2021","objectID":"/blog/voice_conversion/:3:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["deepdearning","VoiceConversion"],"content":"Sources IMPROVING ZERO-SHOT VOICE STYLE TRANSFER VIA DISENTANGLED REPRESENTATION LEARNING Voice Conversion Challenge 2020 results ","date":"11-21-2021","objectID":"/blog/voice_conversion/:4:0","tags":["deepdearning","VoiceConversion"],"title":"Voice Conversion","uri":"/blog/voice_conversion/"},{"categories":["python"],"content":"Python tricks ","date":"11-10-2021","objectID":"/blog/python/","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Inheritance If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn’t overload it Ex: class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don't override the constructor and use the parent one ","date":"11-10-2021","objectID":"/blog/python/:1:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Multiple Inheritance when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority if the two parent classes have the same function, then Method Resolution Orded (MRO) makes the fist class method to be the one called if we override a function from the parent class in the child class, the cild class has the pariority to call the both finctions, then we can call the parent class from the child class directly ex: in the child class we can do: parent.method() ","date":"11-10-2021","objectID":"/blog/python/:1:1","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Underscore single underscore (Before): used for internal variables double undescore (After): used also for internal scopes only, and also tells python to change the variable name(mangling) underscore (After): helps avoid conflicts with key words ex(class*, int*, etc) underscore (Before and Afer): used for thing like __init**, __main**, etc ","date":"11-10-2021","objectID":"/blog/python/:2:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Decorators def function(func): def inner(): print(1) func() print(3) return inner @function def print_name(): print(2) print_name() 1 2 3 so in this example we decorated a function (print_name), which means we gonna pass the deocared function as a paramemter to the decorator so with decorator, we call our function in the inner function, the decorate it, then return the decorated function used when u wanna decorate a multiple function with the same, suppose you have an add and multiply function, and you want to print something at the beginig of them, so you crate a decorator that does that, and decorate the two functions with that. ","date":"11-10-2021","objectID":"/blog/python/:3:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["python"],"content":"Operators for custom classes, or structures, you can do operators overloads but there’s also reversed operators when python can’t handle operations ex: class.__rmul() ","date":"11-10-2021","objectID":"/blog/python/:4:0","tags":["python"],"title":"Python","uri":"/blog/python/"},{"categories":["deeplearning","python"],"content":"A bunch of very different topics scribbles","date":"10-27-2021","objectID":"/blog/scribbles/","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Transformers To deal with sequential data we have to options: 1-D convolution NN processing can be parallel not practical for long sequences Recurrent NN can’t happen in prallel have gradient vanshing problem of the squence becomes so long we have bottleneck at the end of the encoder RNN with attention mechanism to solve the bottleneck problem, we make Encoder-Decoder attention Decoder utilzes: context vector weighted sum of hidden states (h1,h2, … ) from the encoder ","date":"10-27-2021","objectID":"/blog/scribbles/:1:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Transformers Encoder first we do input embedding, and positional embedding in self attention: we multiply q,w,v by a matrix to do lenear transformation self attentoion: k _ q –\u003e scaling down –\u003e softmax –\u003e _ v ","date":"10-27-2021","objectID":"/blog/scribbles/:1:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"multi-head attention works as we use many filters in CNN in wide attention: it takes every word and spread it multi-head attention in narrow attention: it take every word and split it up across the multi-head but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN? ","date":"10-27-2021","objectID":"/blog/scribbles/:1:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Positional info positional encoding using the rotation sin/cos matrix positional embedding ","date":"10-27-2021","objectID":"/blog/scribbles/:1:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Residual connections to give the chance to skip some learning parameters if it’s better to minimize the loss ","date":"10-27-2021","objectID":"/blog/scribbles/:1:4","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Layer Normalization in batch normalization ==\u003e we normalize to zero mean and unity varince we calculate for all samples in each batch (for each channel ) in layer normalization ==\u003e $y = \\gamma * x + \\beta $ where gamm and bata are trainable parametes calculates for all channles in the same sample in instance normalization ==\u003e calculate for one channel in one sample ","date":"10-27-2021","objectID":"/blog/scribbles/:1:5","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Debugging ML Models Understand bias-variance diagnoses getting more data ==\u003e fixes high variance smaller set of features ==\u003e fixes high variance Refrence Prof. Andrew NG Vid ","date":"10-27-2021","objectID":"/blog/scribbles/:2:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"SVM and Kernels The main idea of kernels, is that if you can formlate the optimization problem as some of inner products of feater vectors, that can have infinite dimentions, and to come up with a way to calc these inner products efficiently we have $ X(i) \\in R^{100}$, suppose W can be expressed as a linear combintaion of X $ W = \\sum*{i = 1}^{M} \\alpha*{i} y^i x^i$ (This can be proved with the representer theorem) vector W is perpendicular to the decsion boundry specified by algorithm, so W kinds of sets the orientation of the decision boundry and the bias moves it alont right and left. optimization problem is : $\\min {w,b} {1/2} ||W||^2 $ s.t $y^i((W^T * x^i) + b) \u003e= 1$ For SVM you can make a trade off between the margin and how much you can tolerate wrong calssified examples using a constant ","date":"10-27-2021","objectID":"/blog/scribbles/:3:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Distributed Training in Pytorch ","date":"10-27-2021","objectID":"/blog/scribbles/:4:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Pytorch DDP Internal DDP relies on c10d ProcessGroup for communications. Hence, applications must create ProcessGroup instances before constructing DDP. The DDP constructor takes a reference to the local module, and broadcasts state_dict() from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. DDP registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready. Backward pass: Because backward() function is called on the loss directly, which out of DDP’s control. So, it waits till one of the autograd hooks are invoked, to trigger the gradients synchronization. DDP waits for all gradients in one bucket are ready, Reducer kicks off an asynchronous allreduce on that bucket to calculate mean of gradients across all processes. Optimizer Step: From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration. ","date":"10-27-2021","objectID":"/blog/scribbles/:4:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DataParallel VS DistributedDataParallel DataParallel is single-process, multi-thread, and only works on a single machine, while DistributedDataParallel is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs. DataParallel doesn’t support model parallel Resources - https://pytorch.org/docs/master/notes/ddp.html - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed%20training ","date":"10-27-2021","objectID":"/blog/scribbles/:4:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Complete Statistical Theory of Learning - Vladimir Vapnik There are two ways for generalization: more data, and complete learning theory Turing said, that you should imitate intelligent person ","date":"10-27-2021","objectID":"/blog/scribbles/:5:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Ref: https://www.youtube.com/watch?v=Ow25mjFjSmg ","date":"10-27-2021","objectID":"/blog/scribbles/:5:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Statistical Machine Learning ","date":"10-27-2021","objectID":"/blog/scribbles/:6:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"part 1: deduction vs induction: deduction: is the process of reasoning from one or more general statements to reach a logically certain conclusion. premises must be correct. induction: reasoning that construct or evaluates general proposition that are derived from specific examples. we can never be sure our conclusion can be wrong! machine learning tries to automate the process of inductive inference. why should ML work? ML tries to find patterns in data we will only be able to learn if there’s something to learn ML makes some assumptions, which are are rarely made explicit. we need to have an idea what we are looking for. This is called “inductive bias”. Learning is impossible without such a bias the formal theorem if this is called no free lunch theorem on the other hand, if we have a very strong inductive bias, then with just few training examples, then we can have high certainty in the output the problem of selecting a good hypothesis class is called model selection. any system that learns has an inductive bias. if the algorithm works, THERE HAS TO BE A BIAS the inductive bias, rules over our function space ","date":"10-27-2021","objectID":"/blog/scribbles/:6:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Part 2: it;s not hard for ML algorithm to correctly predict training labels usually ML algorithms make training errors, that is the function,they come up with doesn’t perfectly fit the training data what we care about is the performance on teh test set it’s not always the case that lowering the train data would lower that test data K-Nearest algorithms: for K-nearest algorithm, the best value for K is log(N) if k is too small ==\u003e overfitting if k is too large ==\u003e underfitting k nearest algo achieves good results on MNIST dataset for classifying two classes, with simple euclidean distance function k-nearest can be used for density estimation, clustering, outlier detection the inductive bias in K nearest algo, is that near points are the of teh same category the challenging part about k nearest algo is how to measure the distance between points ","date":"10-27-2021","objectID":"/blog/scribbles/:6:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Part3 for ML, we don’t put any assmuptions for the data probability ditribution often, the input and output are random variables in some applications, it’s important that the loss depends on the input X. also in some cases, the type of error is critical, for exp. spaam detection Bayes risk: is the min error for the expected values over all examples –\u003e basically the it’s the lowest error you can achieve Consistency: we say algorithm A is consistent, if we have an infinite iid datapoints, and the risk of algorithm’s selected function converges to the Baye’s risk. basically means that if we have infinite data samples, then our algorthms reaches the Bayes risk, which is the lowest error possible Universally consistent: no mattter the underlying probability distribution is, when we have enough data points, the algorithm would be consistent consistent independantly of the data distribution KNN classifier, SVM, boositn, random forests are universally consistent ","date":"10-27-2021","objectID":"/blog/scribbles/:6:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DL Book ","date":"10-27-2021","objectID":"/blog/scribbles/:7:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"CH1 one of the key ideas in DL is that data representation matters a lot, and that DL is a technique for learning how to represent the data ","date":"10-27-2021","objectID":"/blog/scribbles/:7:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"AI in AI we need the computer to do some tasks like humans we do that by providing the computer with a lot of rules describing the world and how to act in different scenarios ","date":"10-27-2021","objectID":"/blog/scribbles/:7:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"ML in machine learning we can learn these rules without explicitly told them but we still need to be provided with custom features that are given by domain experts ","date":"10-27-2021","objectID":"/blog/scribbles/:7:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Representation learning a specific type of ML where we don’t tell the computer the specific features instead, we give the computer raw input, and it should learn the more complex features explicitly ex: autoencoders ","date":"10-27-2021","objectID":"/blog/scribbles/:7:4","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DL is a representation learning algorithms that is applied in multi sequential manner ","date":"10-27-2021","objectID":"/blog/scribbles/:7:5","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"CH2 ","date":"10-27-2021","objectID":"/blog/scribbles/:7:6","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Deep Generative Modeling latent variable: it’s a variable that is controlling some behaviors, but we can’t directly observe it we are trying to observe true explanatory factors, for example, latent variables, from only observed data ","date":"10-27-2021","objectID":"/blog/scribbles/:8:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Autoencoders: the encoder learns to map from data, to a low-dimensional latent space the decoder learns to map back from the low-dimensional space back into a reconstructed observation the bottleneck hidden layer forces the network to learn a compressed latent representation the reconstruction loss forces the latent representation to capture as much information from the data ","date":"10-27-2021","objectID":"/blog/scribbles/:8:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Variational Autoencoders (VAE) with classic autoencoders, once we train the network, then the latent representation is deterministic but in VAE, we add some randomness, so we can generate new samples so the encoder should output a mean and a standard deviation, which represents a distribution of the input, then we can sample from this distribution to generate new sample the encoder is trying to infer a probability distribution of the latent space with respect to its input data the decoder is trying to infer a new probability distribution over the input space given the latent space the loss is going to be function of the parameters of the two distributions the loss would consist of a construction loss and a regularization term, which is responsible for inducing some notion of structure of this probabilistic space We need regularization and a prior to: continuity: points that are close in the latent space, should remain close after decoding completeness: samples from the latent space, should produce meaning content after decoding we can’t perform back propagation, as there’s stochasticity in the latent space, to solve this issue, we fix, the mean and variance, and introduce the stochastic term separate from them The key problem with VAEs, is a concern of density estimation ","date":"10-27-2021","objectID":"/blog/scribbles/:8:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Generative Adversarial Networks (GANs) we need to sample from a very complex distribution, that we don’t know, and can’t estimate the solution, is to sample from a simple distribution (eg. noise), then learn a transformation, to the data distribution we have a Generator, that’s tries to transform the data sampled from the random noise, into data that looks real, to trick the discriminator we have a discriminator, which tries to identify real data from fake. ","date":"10-27-2021","objectID":"/blog/scribbles/:8:3","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"SubWords it’s just like the skip-gram model but we just changed the score function so that we increased the vocab size by adding N-grams of all words we have then we use them to capture more meaning from the words if we encounter new word, then we add it’s N-gram and thats would be the word vector ","date":"10-27-2021","objectID":"/blog/scribbles/:9:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Decision Trees they are greedy algorithm they can stuck in local minimum if the we have some continuous features, we can use it multiple times, every time with different range ","date":"10-27-2021","objectID":"/blog/scribbles/:10:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Purity function we want to define a purity function, that has the following it has it’s maximum value when probability of any class in 1 it has it’s minimum value when all classes has the same probability Purity( pa, pb) == Purity (pb, pa) entropy = impurity = -1 * purity one function that satisfies all these requirements, is $ purity(p*{c1}, p*{c2}, p*{c3}) = p*{c1} \\log(p*{c1}) * p*{c2} \\log(p*{c2}) * p*{c3} \\log(p_{c3})$ so we choose features, that would increase purity the most after splitting the dataset using it to calculate after entropy or purity of a set after seperation, would be the weighted average of the subsets ","date":"10-27-2021","objectID":"/blog/scribbles/:11:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Why going deep in Deep Learning one motivation, is that going deep can reduce the size of our units exponentially in our underlying function, there could be some symmetry, that we can fold the function across its axis for statistical reasons, we would want to infer out initial beliefs, about our function, that is involve the composition of several simpler functions Empirical experiments show that deeper networks generalize better ","date":"10-27-2021","objectID":"/blog/scribbles/:12:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Few-shot Learning we want to classify examples, that we only have few examples for, maybe even 0 the idea is instead of having a classifier as the last layer(softmax layer), we can use a siamese network, just to tell us is the two examples are similar so we just learn a similarity function ","date":"10-27-2021","objectID":"/blog/scribbles/:13:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"GLIDE: Generates realistic images from text GLIDE is the next version of DALLE, with respect to photo realism and caption similarity this is fine-tuned model, not zero-shot like DALLE It can generate or edit images so you can generate an image using zero-shot, then you can edit the image by putting masks on the image and tell the model what to draw in the masked area ","date":"10-27-2021","objectID":"/blog/scribbles/:14:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Diffusion Models we start with the original image, then we keep add noise to it till it become so noisy then we try to reverse the operation and get it back to the input image ","date":"10-27-2021","objectID":"/blog/scribbles/:14:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"P vs NP P = problems solvable in polynomial time NP = decision problems solvable in nondeterministic polynomial time decision problems = yes, no problems NP problems are hard to solve, but each to check the correctness of the answers ","date":"10-27-2021","objectID":"/blog/scribbles/:15:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Expectation Maximization If we have a probability distribution that is function of two variables, one is observable and the other is latent and we want to calculate the MLE for this model. Then we can calculate the marginal likelihood instead The marginal probability basically means that we take the summation over all possible states of the latent variable ","date":"10-27-2021","objectID":"/blog/scribbles/:16:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"How it works you have some samples, and you want to cluster them according to two distributions you init the two ditros, randomly, and calculate the postrior that every sample belong to this distro then you take the weighted average that every one of thees samples belong to the distro it’s like K-means except that we have probability that every sample belong to a distro, instead of 0-1 we keep iterating till we reach the most accurate distros ","date":"10-27-2021","objectID":"/blog/scribbles/:16:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"CTC Loss when we have many-to-many sequence prediction Labeling order matters , but there’s no one-to-one correspondence between outputs and labels we need to impose structural constraints on the output sequence Mostly used with speech recognition, where we have labels much less that input they have a special blank token, and they reduce all similar tokens in the same span between blanks with one token ","date":"10-27-2021","objectID":"/blog/scribbles/:17:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Contrastive loss In cross-entropy loss, all we care about is make similar images in the same side of the decision boundry But in contrastive loss, we first try to move all similar examples near each other, so then we can train the calssification layer easily in case of self supervised learning, there can be a probelm of taking a postive example as a negative one, because we don’t have labeled data, which can make it hard on downstream tasks ","date":"10-27-2021","objectID":"/blog/scribbles/:18:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"How it works we have the original image, and they call it the anchor they augment this anchor image to obtain the positive examples for each anchor, they generate one positive pair, and 2N-2 negative pairs they calculate the dot product similarity between the anchor and these pairs they take the log for the similarity of the anchor and the positive example in the numerator, and the summation of the similarity between the anchor and negative examples in the denomerator so our objective is increasing the enumerator and decreacing the denomerator They also apply temperature smoothing This is can be also extended to supervised contrastive we can have more than one positive label we can have multiple anchor classes we can see the anchor-positive similarity appears in the gradients, and this makes the gradient the biggest for hard-positives (positive examples that the model didn’t learn the similarity between yet) ","date":"10-27-2021","objectID":"/blog/scribbles/:18:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"MAP - Maximum a posteriori Tries to estimate the best parameters given the likilhood on the data, and the prior of the parameters ","date":"10-27-2021","objectID":"/blog/scribbles/:19:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Pros: easy interpretable avoid overfitting (has regularization ) tends to look like MLE asymptotically ","date":"10-27-2021","objectID":"/blog/scribbles/:19:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Cons point estimate, no representation of uncertainty in Theta not invariant under reparameterization (meaning that if we mapped Theta using a function, then the new MAP value will not simply be the paramterized value on the old MAP value) must assume prior on Theta ","date":"10-27-2021","objectID":"/blog/scribbles/:19:2","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Activation Checkpointing normally we store the activations for all layers, because we need them in the backward path for large models, this can cause huge memory footprint The idea is to only store the activation for the last layer in each block and then recalculate the activations for the other layers when we need them Loss Scalling in half-precision training, some small values are rounded to zero, bacause of the representation limitation of FP16 but we can just scale the loss, by multiplying it with certain value,and so the gradients would be scaled as well this trick enables us to cover larger range of values for ex, if we scaled the loss by factor of 8, then extend the covered range with 3 exponents (2^3) ","date":"10-27-2021","objectID":"/blog/scribbles/:20:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"ZeRO The main idea is in Data parallel, why copy state of optimizer and weights and gradients to all devices, while you can partition them the idea is to partition all those over the devices, so that teh total memory would reduce dramatically This works because these states (optimizer, weights, gradietns) are temporal independent, meaning that we don’t the whole matrix to calculate teh local step ","date":"10-27-2021","objectID":"/blog/scribbles/:21:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"PaLM Outperformed finetuned SOA models breakthroughs multi-step reasoning efficient utilization ","date":"10-27-2021","objectID":"/blog/scribbles/:22:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Chain of Though Prompting instead of doing standard prompting, they did chain of thought prompting they just provide the model with the chain of thought on how to generate the answer ","date":"10-27-2021","objectID":"/blog/scribbles/:23:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"HuBert the main idea is that they wanted to do MLM like bert, but that’s challenging with audio. so they first applied MFCCs to convert the audio signal into chunks then they applied offline K-means algorithm to cluster each frame so basically the labels comes from an unsupervised teacher which is the k-means they applied the k-means on the MFCC the main insight of why this works, is that the K-means algorithm is consistent, so the model learns more of the sequential structure of the data ","date":"10-27-2021","objectID":"/blog/scribbles/:24:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Graphical models to solve the marginal probability over the latent variable, we have two options: monte carlo inference this method is unbiased, but have high variance Variational inference this is very lowe variance, but is biased ","date":"10-27-2021","objectID":"/blog/scribbles/:25:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Variation Inference we can’t calculate the original postrior so we estimate it using another approximation, that is tractable, and we tweak it till it becomes like the original intractable distribution if you maximize the lower bound of a function, you maximize that function ","date":"10-27-2021","objectID":"/blog/scribbles/:25:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"VQ VAE they applied sigmoid to the random variable to make it only takes confined range, istead of having values outside of pixels values range ","date":"10-27-2021","objectID":"/blog/scribbles/:26:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Teacher forcing in RNN it’s a technique used to train randomness it’s applicable only in case we have output-to-hidden connections, and it can’t be used in hidden-to-hidden connections the main idea is that they use the true label from last step instead of the output. This way, we can parallelize the training biggest disadvantage to this, is that it can’t be used with open loop, meaning in case we enter the models’s output as input. because this way, there will be some inputs as test time that the model didn’t see at training. ","date":"10-27-2021","objectID":"/blog/scribbles/:27:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Parti it’s autoregressive Text-to-Image model it uses transformer based image tokenizer they can have detailed image description from dataset designed for vision-impaired people ","date":"10-27-2021","objectID":"/blog/scribbles/:28:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"steps first they train image tokenzier, which converts images to tokens then they train encoder-decoder model to do teacher-inforcing style training from the sentence tokens ","date":"10-27-2021","objectID":"/blog/scribbles/:28:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Faster RCNN it’s fast because they merged the the feature map for Region Proposal Network and the classifier network they pass on the extracted feature map with 3*3 sliding window for every anchor point he tries to generate 9 anchor boxes ","date":"10-27-2021","objectID":"/blog/scribbles/:29:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"questions what if this anchor point doesn’t have a ground truth? we only consider the first term of the loss, which is the objectivity. and we exclude the second term as we don’t have ground truth for this pixel why do we make intermediate anchor boxes? it’s like adding a prior knowledge, we normalize our predictions and ground truth according to these anchors ","date":"10-27-2021","objectID":"/blog/scribbles/:29:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"How to choose validation sets This is taken from Dr. Rachel’s blog post here the most common way is to choose a random subset from the training set. but this doesn’t always give the best results in real world ","date":"10-27-2021","objectID":"/blog/scribbles/:30:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Cases when is a random subset not good enough? Time series models it’s better here to choose continuous intervals as val or test when you dataset has many instances of the same object (ex: same person, same car ). and your task is not detecting the object, but rather the action or the environment. so if you didn’t take all the pictures from the same object in one split of the data, there’s a high chance your model would learn features related to the object and not the action so we want to gather all dataset examples of the same object in the same split (train, or val) ","date":"10-27-2021","objectID":"/blog/scribbles/:30:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Masked RCNN ","date":"10-27-2021","objectID":"/blog/scribbles/:31:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"why pyramid network we want to have good semantic understanding but in high resolution it’s so computationally expensive we use it as a backbone for FasterRCNN question? how do we use it as a backbone? The main idea is that they seprated the mask predictor from, classification and bounding box detection ","date":"10-27-2021","objectID":"/blog/scribbles/:31:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Test Time Augmentation (TTA) During inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image. It’s a nice way to increase accuracy Jermy Howard Said: “This is one of the most under-appreciated deep learning tricks, in my opinion!” ","date":"10-27-2021","objectID":"/blog/scribbles/:32:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Linear Factor Models they are one of the simplest classes of probabilistic models they may not be used in SOA models, but they are a building block for many of them. defined by the use of a stochastic linear decoder that generates x by adding noise to a linear transformation of h we take the hidden representation h and do linear transformation, then add noise to that, to get x ","date":"10-27-2021","objectID":"/blog/scribbles/:33:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Autoencoders We force the model to learn rich features by making the hidden features dimension to be less than the input’s ","date":"10-27-2021","objectID":"/blog/scribbles/:34:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Caveats and Dangers Reconstruction loss is indifferent to latent space, meaning it just compares the input with the output. Higher representational power gives flexibility for suboptimal encodings. this means that good construction loss doesn’t necessary mean that we learned good hidden features we need more constraints to learn manifolds manifolds are areas that have high probability for the data to be in ","date":"10-27-2021","objectID":"/blog/scribbles/:34:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"DOM (Document Object Model) it’s how a browser renders the website it’s like a skeleton for the website it’s has documents with elements and attributes it converts your HTML code to objects, and that’s how you can interact with it with JS So the DOM and the HTML are the same thing, with just two different representations ","date":"10-27-2021","objectID":"/blog/scribbles/:35:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Interpreter Vs Compiler compiler just takes your code, and statically compiles it, and hand you the binary for it and goes away interpreter, stays with you and interprets your code line by line, and can make use of some speedups like JIT compilation ","date":"10-27-2021","objectID":"/blog/scribbles/:36:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Category Theory Category theory takes a bird’s eye view of mathematics.From that high you can’t see the details, but you however can see the patterns.. ","date":"10-27-2021","objectID":"/blog/scribbles/:37:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Turing Lecture: Yoshua Bengio ","date":"10-27-2021","objectID":"/blog/scribbles/:38:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Ref: YT Vid As we try to estimate functions that grow exponentially in complexity, we need to have training samples that grows with the same rate as well but we can add the concept on composintialilty to mitigate this we can do that by adding more layers, or adding more parameters per layer local minimas are bad in low dimensions only, but in higher dimensions, they are more rare, and only happen near the global minima, cause they require the grad to be low in all dimensions. on the other hand, high dimensions functions suffer from saddle points we found that NN are good as classifiers, and that our data is concentrated in small regions called the manifolds. so we can use this to separate real data from fake one, and learn these manifolds as a side effect, and this is what GANs do ","date":"10-27-2021","objectID":"/blog/scribbles/:38:1","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"Some backward propagation insights: sum in forward, propagates ones in backward broadcasting in forward, propagates sum in backward ( for example: bias) max,min,median operations in forward , becomes inserting ones in the positions of max,min,etc, and zeros in the other positions view operation is just a reshape, and we can undo it in backward pass (just rearrange the gradient like the original tensor shape) for embedding matrices, we need to see which entries did we use, and propagate the gradient for them, for each time we used them,(example: if we used the embedding for a twice and c once, then we need to propagate the gradient for c twice, and for a once) one way to do this, is to loop over the input and for each entry that we used the embedding for, accumulate the forwarded gradient vector ","date":"10-27-2021","objectID":"/blog/scribbles/:39:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["deeplearning","python"],"content":"RUST and RAII (Resource Acquisition is Initialization) the main idea is that when you create an object, you allocate some memory, and when you destroy that object, you destroy the memory they do that, by ensuring that every chunk of memory the program allocates has only one owner, and this owner has the control over this memory chunk in the code, if we assign this memory to another variable, then it becomes the new owner and the old one has no longer access to the data there’s also the option to borrow the data, instead of taking ownership over it ","date":"10-27-2021","objectID":"/blog/scribbles/:40:0","tags":["deeplearning","python"],"title":"Scribbles","uri":"/blog/scribbles/"},{"categories":["TTS","Deeplearning"],"content":"A summary research for TTS","date":"10-14-2021","objectID":"/blog/text_to_speech/","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"TTS TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. ","date":"10-14-2021","objectID":"/blog/text_to_speech/:0:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"End-to-end TTS easier pipeline better peformance ","date":"10-14-2021","objectID":"/blog/text_to_speech/:1:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Wavenet ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Based on: DeepMind’s Wavenet https://www.kdnuggets.com/2020/07/deep-learning-signal-processing.html https://deepmind.com/blog/article/wavenet-generative-model-raw-audio ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Wavenet V1 before wavenet, ther was two methods: generative method, which would produce the over all song of the sentece well, but would fail to produce the individual sounds well concatinative: we use a huge corpus of phonatics and concatinate them together to procude a whole sentence, this way we would procuce the individual sounds correctly, but we would lose the song of the sentence wavenet: tries to do both of the above methods, it also can change the speaker by changing some parameters data output: 16 khz rate we cant use normal RNN as the max seq length around 50 they used dilated CNNs: can have very long look back fast to train This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition. ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:2","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Wavenet V2 The original Wavenet implementation suffered from low speed inference, because it predicts samples squentially. They needed to predict time samples in prallel so that wavenet can be used in production, so the used a fully trained wavenet teacher, to train a smaller wavnet student, which doesn’t depend on previous samples to produce the current sample, while still maintaining the same quality ","date":"10-14-2021","objectID":"/blog/text_to_speech/:2:3","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"End-to-end adversarial TTS ","date":"10-14-2021","objectID":"/blog/text_to_speech/:3:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Based on: https://www.youtube.com/watch?v=WTB2p4bqtXU https://deepmind.com/research/publications/2020/End-to-End-Adversarial-Text-to-Speech https://arxiv.org/abs/2006.03575 Adversarial: means we have a generator and a descriminator which tries to detect which output is generated and which is real End-to-end : they take in text and output the speech steps: we enter the text, then we tokenize it then we use a stack of dilated conv layers to predict the length of each token with this info, we can predict the center of each token then we use a gaussian kernel to give a prob distributaion for the place of the token center then the generator can generate the sound ** Q: here we assumed that every token starts directly after the one before it, but is that the case? don’t we need to add a small duration of no sound between tokens? ","date":"10-14-2021","objectID":"/blog/text_to_speech/:3:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Tacotron2 ","date":"10-14-2021","objectID":"/blog/text_to_speech/:4:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"based on: paper repo you train tacotron-like seq2seq model to output a mel spectrogram, then pass that to wavenet to generate the wave form This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech. ","date":"10-14-2021","objectID":"/blog/text_to_speech/:4:1","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["TTS","Deeplearning"],"content":"Common TTS Architictures: - Autoregressive models - DCCN - Flow - Teacher Student - Variational auto encoders (VAE) - GANS Resources https://towardsdatascience.com/state-of-the-art-of-speech-synthesis-at-the-end-of-may-2021-6ace4fd512f2 ","date":"10-14-2021","objectID":"/blog/text_to_speech/:5:0","tags":["TTS","Deeplearning"],"title":"TTS Research","uri":"/blog/text_to_speech/"},{"categories":["FastAi"],"content":"A summary for FastAi Practical deep learning for coders course.","date":"10-06-2021","objectID":"/blog/fastai/","tags":["FastAi"],"title":"FastAi 2020","uri":"/blog/fastai/"},{"categories":["FastAi"],"content":"Lecture two P value: determines if some numbers have realationship, or they are random (whether they are independat or dependant) suppose we have the temp and R (transmitity) values of a 100 cities in China and we want to see if there's a relation between them. then we generate many sets of random numbers for each parameter then we calculate the P value which would tell us what's the percentage this slope is a random, and that ther's no relation A P-value is the probability of an observed result assuming that the null hypothesis (there's no relation ) is true PS: P-value also is dependant on the size of the set u used, so they don't measure the importance of the result. so don't use P-values If the P value is \u003e 0.5 then we sure that these daata have no ralation, and if the p-value is so small, then there's a chance that the data have a relation ","date":"10-06-2021","objectID":"/blog/fastai/:0:1","tags":["FastAi"],"title":"FastAi 2020","uri":"/blog/fastai/"},{"categories":["FastAi"],"content":"Lecture three In the course video and book, we built a bear classifier, using data from Microsoft Ping Api. To build a deep learning model, we have first to gather the data, then we should prepare the data to be in the right format for the model, then we train the model and observe if we get satisfiable results, if not then we try to investigate to try to get better results. Finally we save our model and deploy it! while gathering the data, notice that all the time the data would be biased, and in sometimes these biases would be severe that they can’t be ignored Race classifier I have tried to rebuild the notebook and to make a Race classifier. I got the dataset from here Dataset, and then trained a small Resnet18 NN to classify images. To deploy the app, I used Voila and MyBinder to make it available online here: Race Classifier Lastly, all the code can be found in this githun repo here ","date":"10-06-2021","objectID":"/blog/fastai/:0:2","tags":["FastAi"],"title":"FastAi 2020","uri":"/blog/fastai/"},{"categories":[],"content":"This is Post for Computational Linear Algebra ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":[],"content":"Lecture 1 import numpy as np a = np.array( [[6,5,3,1], [3,6,2,2], [3,4,3,1] ]) b = np.array( [ [1.5 ,1], [2,2.5], [5 ,4.5] ,[16 ,17] ]) for c in (a @ b): print(c) [50. 49.] [58.5 61. ] [43.5 43.5] Lecture 2 Matrix decomposition: we decopose matricies into smaller ones that has special properties ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/:0:0","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":[],"content":"Singular Value Decomposition (SVD): it’s an exact decomposition, so you can retrieve the orginal matrix again Some SVD applications: semantic analysis collaborative filtering / recommendation data compression PCA (principal component analysis) ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/:0:1","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":[],"content":"Non-negative Matrix Factorization (NMF) ","date":"10-02-2021","objectID":"/blog/computational_linear_algebra/:0:2","tags":[],"title":"Computational Linear Algebra","uri":"/blog/computational_linear_algebra/"},{"categories":["MIT","Linear Algebra","Math"],"content":"summarization of some Professor Gilbert Strang's MIT Linear Algebra course 18.06","date":"10-02-2021","objectID":"/blog/1806_mit/","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 1 We learn about the big picture behind multiplication of matrix and vector we learn about the row picture and column picture ","date":"10-02-2021","objectID":"/blog/1806_mit/:1:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 2 we learned about elimination method to solve a system of equations ","date":"10-02-2021","objectID":"/blog/1806_mit/:2:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 3 in this lecture we learned about matrices multiplication: we can do that in five ways: row * col ==\u003e gives an entry (1 cell) col _ row ==\u003e sum ( r1 _ c1 , r2 * c2, etc) by columns ==\u003e A * c1 = combination of A columns by columns ==\u003e r1 * B = combination of A B rows by blocks ==\u003e A (A1,A2,A3,A4) _ B (B1,B2,B3,B4) = C1 = (A1_ B1 + A2 * B3) and so on then we learned about gausian-Jordan elimination to find the matrix inverse [A | I] ==\u003e we apply elimination till we get [ I | A-1 ] ","date":"10-02-2021","objectID":"/blog/1806_mit/:3:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 4 in this lecture we learn about A= L U, where L is E^ -1, and whats special about this is that it has all multipliers in the lower triangular with ones on the diagonal ","date":"10-02-2021","objectID":"/blog/1806_mit/:4:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 5 we continued a little with permutations and moved to vector spaces we learned about sub spaces and columns spaces ==\u003e which is u take the columns of the matrix and all its combinations and that creates a plane through origin making a columns space ","date":"10-02-2021","objectID":"/blog/1806_mit/:5:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 6 In this lecture we continued about columns spaces and that we build those up by taking the combinations of all columns. Then we learned about null spaces while are sub spaces of X that satisfies A X = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:6:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 7 in this lecture we continued about null space then we learned about the special solution, where we assume the variables of the free vector then get the special solution finally we learned about the reduced form where R = [ I F 0 0 ] and the null matrix is [ -F I ] then R N = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:7:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 8 in this lecture we expanded to talk about A x = b and we find the whether there’s a solution to the equation or not depends on the rank of the matrix also we get the Xcomplete = Xparticular + Xnull space and we get particular soln by putting all free variables = zero ","date":"10-02-2021","objectID":"/blog/1806_mit/:8:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 9 in this lecture we learned about independent columns and how they make a space, we also learned about Basis and what are two conditions for it rank(A) = number of pivot columns of A = dimension of C(A). dimension of N(A) = number of free variables = n − r, ","date":"10-02-2021","objectID":"/blog/1806_mit/:9:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 10 In this lecture we learned about the four subspaces we also started in matrix space M ","date":"10-02-2021","objectID":"/blog/1806_mit/:10:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 11 We learned about matrix space we take introduction about graph ","date":"10-02-2021","objectID":"/blog/1806_mit/:11:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 12 in this lecture we learned about graphs and how to represent them with matrices, then we applied that to electrical system and applied kerchofs law ","date":"10-02-2021","objectID":"/blog/1806_mit/:12:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 13 Quiz 1 review ","date":"10-02-2021","objectID":"/blog/1806_mit/:13:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 14 in this lecture learned about othrignilaity of the four vector spaces and what does it means ","date":"10-02-2021","objectID":"/blog/1806_mit/:14:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 15 in this lecture we learned about projection of matrices into subspaces ","date":"10-02-2021","objectID":"/blog/1806_mit/:15:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 16 we got example explaning the projection into subspaces and how to get the best fit using the least square error ","date":"10-02-2021","objectID":"/blog/1806_mit/:16:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 17 in this lecture le learned about orthonormal vectors and their special features and we learn how to produce them from any independent vectors using gram-schmeit ","date":"10-02-2021","objectID":"/blog/1806_mit/:17:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 18 Propertise of determants ","date":"10-02-2021","objectID":"/blog/1806_mit/:18:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 19 det I =1 sign reverses with each row or colums exchange det is linear in each row seperately ","date":"10-02-2021","objectID":"/blog/1806_mit/:19:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Big Det Formula for a N * N matrix, we calc the sum of N! terms $$detA=\\sum_{i=1}^ N a1\\alphaa2\\betaa3\\gamma*an\\omega$$ (where $\\alpha, \\beta, … \\omega $) = perm of (1,2,3, …, N ) ","date":"10-02-2021","objectID":"/blog/1806_mit/:19:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Cofactors cofactor of aij = Cij = +/- det of ( n-1 matrix with column j, and row i erased ) it is plus if i+j is even, minus if i+j is odd cofactor formula (along row 1) det A = a11 C11 + a12 C12 + .... + a1n C1n ","date":"10-02-2021","objectID":"/blog/1806_mit/:19:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 20 $ A^{-1} = 1/detA * C^T$ where C is the cofactors matrix ","date":"10-02-2021","objectID":"/blog/1806_mit/:20:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Cramers rule A x = b x= A^ (-1) b = 1/detA C^T b X_j = detB_j / detA where B_j is A matrix with column j replaced by b ","date":"10-02-2021","objectID":"/blog/1806_mit/:20:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Det A = Volume detA = volume of the shape created by making an edge from each of the rows ","date":"10-02-2021","objectID":"/blog/1806_mit/:20:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 21 Eigenvalues and Eigenvectors Eigenvectors: Ax is prallel to x ==\u003e Ax = $\\lambda$x lambda ia the eigen values if we have a plane: any x in the plane: Px= x ==\u003e x is eigenvector and lambda = 1 any x perpendicular to plane Px = 0 ==\u003e x is eigen vector and lambda = 0 Fact: the sum of the eigenvalues = the sum of the diagonal of A ","date":"10-02-2021","objectID":"/blog/1806_mit/:21:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 22: Diagnolization to get power of matrix $A^k$ first get the eigenvalues and vectors for A then compute $A = S \\lambda S^{-1}$ where S is the eigenvector matrix, and Lambda is diagonal matrix of the eigenvalues then $A^k = S * \\lambda^k*S^{-1}$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:22:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 23 for the diffrential equations: 1- Stability if lambda \u003c 0 ==\u003e u(t) –\u003e 0 2- Steady state if lambda1 = 0, lambda2 \u003c 0 3- Blowup if any lambda \u003e 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:23:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 24 ","date":"10-02-2021","objectID":"/blog/1806_mit/:24:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Markov Matrix 1- All entries \u003e= 0 2- The sum of every column is 1 3- lambda = 1 is eigenvalue 4- all othe lambda \u003c 1 5- eigenvector values \u003e= 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:24:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Fourier series integration of ( f(x) g(x) dx ) from 0 to 2pi = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:24:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 25 ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Symmetric matrices $ A = A^T$ the eigenvalues are real the eigenvectors are perpendicular usual case: $A = S \\lambda S^{-1}$ symmetric case: we have orthonormal eigenvectors $A = Q \\lambda Q^{-1} = Q \\lambda Q^{-T} $ ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Every symmetric matrix is a combination of perp. projection matrices ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Signs of pivots are the same as the sign of the eigenvalues product of pivots = product of eigenvalues = det of matrix ","date":"10-02-2021","objectID":"/blog/1806_mit/:25:3","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Positive definite symmetric matrix all eigenvalues are positives all pivots are positive det is positive as it’s the product of the eigenvalues also all sub detemants are positive ( determants of lower matrices ) if S is pos. definite ==\u003e $ X^TSX$ must be positive ","date":"10-02-2021","objectID":"/blog/1806_mit/:26:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 26 ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Complex Matrices we wanna utilize tha fact that $\\bar{Z^T}*Z = \\left|{Z}\\right|^2$ Hermitian is biscally the conj. and transpose ==\u003e $Z^H = \\bar{Z^T}$ Hermitian Matricies : $A^H = A$ perpendicular: q1, q2, …, qn $\\bar{qi}^T * qj = 0 if i!=j, 1 if i=j $ $Q^H*Q = I$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Fourier Matrix a matrix with entries are powers of some number W. where $W^n = 1$ $ F^H*F = I$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Fast fourier transform reduces complexity from $N^2 to N log(N)$ $W_{2n}^2 = W_n ==\u003e W_4^2 = W_2$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:27:3","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 27 when det= 0 ==\u003e then the matrix is positive semi-definite f(x1,x2,x3…,xn) ==\u003e min when the matrix of second derivatives is positive definite the eigenvalues tells us the length of the axis of the shape crated by cutting through the shape of the $X^T A X $ the direction of the eigenvectors is the direction of the axis of that shape ","date":"10-02-2021","objectID":"/blog/1806_mit/:28:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 28 A is a m by n matrix ==\u003e $ A^T*A$ is a positive definite symmetric matrix ","date":"10-02-2021","objectID":"/blog/1806_mit/:29:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"similar matrices A and B are similar means: for some M ==\u003e $B = M^{-1}AM$ Similar matrices has the same eigenvalues if the eigenvalues are unique the eigenvector of B is $M^{-1} * (eigenvector ofA)$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:29:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Jordan form every square A is similar to Jordan matrix J every jordan block has one eigenvaector the number of jordan blocks = number of eigenvectors ","date":"10-02-2021","objectID":"/blog/1806_mit/:29:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 29 eigenvalues of (AB) = eigenvalues of (BA) ","date":"10-02-2021","objectID":"/blog/1806_mit/:30:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Singular value composition (SVD) $Av = \\sigma u $ $ A = u \\sigma v^T = u \\sigma v^{-1}$ $A^T A = v \\sigma^T u^Tu /sigma v^T = v \\sigma^2 v^T$ $A A^T= u \\sigma^T v^Tv /sigma u^T = u \\sigma^2 u^T$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:30:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 30 ","date":"10-02-2021","objectID":"/blog/1806_mit/:31:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Linear transformation examples: projection, rotation if u know what transfotmation does to the basis of a plane, then u know what it does to every vector in the plane every $ v = c_1 v_1 + c_2 v_2 + … + c_n v_n$ then $T(v) = c_1 T(v_1) + …. + c_n T(v_n)$ coordinates come from a basis (think of basis like the X-Y-Z axis so we have a basis for the input and a basis for the output Rule to find matrix A, given the input and output basis : input basis: v1 ===\u003e vn output basis: w1 ===\u003e wm 1st column of A : write T(v1) = a11 w1 + a21 w2 + … + am1 wm 2nd column of A : write T(v2) = a12 w1 + a22 w2 + … + am2 wm repeat that for the n columns A * (input coordinates) = (output coordinates) ","date":"10-02-2021","objectID":"/blog/1806_mit/:31:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 31 ","date":"10-02-2021","objectID":"/blog/1806_mit/:32:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"change of basis we have a new basis vectors and we wanna change to the new basis W A = c _ W ==\u003e c = W^-1 _ A when we change the basis, every vector would have new coordinates ==\u003e old coordinates = new basis * new coordinates ==\u003e x = W c ","date":"10-02-2021","objectID":"/blog/1806_mit/:32:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecture 32 ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:0","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"2-sided inverse $A A^{-1} = I = A^{-1} A$ r = m = n ==\u003e full rank ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:1","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"left inverse full column rank r = n nullspace = 0 then $A^T A $ is invertable $A^{-1}_{left} = (A^T A)^{-1} A^T $ $A^{-1}_{left} A = I$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:2","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"right inverse full row rank r = m n-m free variables left nullspace = 0 ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:3","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"pseudo inverse $A^+$ $A^+ = v*\\sigma^{-1}*u^T$ ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:4","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"},{"categories":["MIT","Linear Algebra","Math"],"content":"Lecute 34 no solution ==\u003e rank \u003c m has one solution ==\u003e there’e no null space ==\u003e rank = n a matrix is invertable when there’s no null space ==\u003e r = n ==\u003e indep. columns positive definite matrix must have full rank ==\u003e has no null space positive def is invertable the matrix has soln of any c when the matrix has full row rank matix with orthogonal eigen vectors : symmetric matrices skew-symmetric orthogonal matrices in markov matrix the eigen values are one, and some sother values less than one $k_m$ and m goes to infinity is the steady state we ge the eigenvector that corresponts to eigenvalue one asn multiply it with c, and notes that the sum of u is alwayes the same, so the sum of u0 is the sum of uk, so look what c achieves that ","date":"10-02-2021","objectID":"/blog/1806_mit/:33:5","tags":["MIT","Linear Algebra","Math"],"title":"MIT 18.06 Linear Algebra course","uri":"/blog/1806_mit/"}]