<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Distributed Training in PyTorch - Boda Blog</title><meta name=Description content="This is my personal Blog, I write about random things that comes to my mind!"><meta property="og:title" content="Distributed Training in PyTorch"><meta property="og:description" content="Distributed Training Why? Need more compute power to process large batches in parallel (DDP)
Uses collective communication Large model that couldn’t be fit in memory of one GPU (RPC)
Uses P2P communication All of the above XD
DDP in Pytorch Every GPU has a model replica, controlled by a process. Every process fetches different batch of data. Forward. Overlapping between computation of and communication(broadcast - allreduced) of gradient. Validation 4 steps-recipe to Distributed Training Initialize Distributed Group 1 init_process_group(backend='nccl') Data Local Training 1 2 3 4 # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) Distributed Training 1 2 3 4 5 6 7 8 9 10 # Create distributed sampler pinned to rank sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) # May be True # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, # Must be False!"><meta property="og:type" content="article"><meta property="og:url" content="https://bodasadalla98.github.io/blog/distributed_training/"><meta property="og:image" content="https://bodasadalla98.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-11T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-11T00:00:00+00:00"><meta property="og:site_name" content="Boda Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bodasadalla98.github.io/logo.png"><meta name=twitter:title content="Distributed Training in PyTorch"><meta name=twitter:description content="Distributed Training Why? Need more compute power to process large batches in parallel (DDP)
Uses collective communication Large model that couldn’t be fit in memory of one GPU (RPC)
Uses P2P communication All of the above XD
DDP in Pytorch Every GPU has a model replica, controlled by a process. Every process fetches different batch of data. Forward. Overlapping between computation of and communication(broadcast - allreduced) of gradient. Validation 4 steps-recipe to Distributed Training Initialize Distributed Group 1 init_process_group(backend='nccl') Data Local Training 1 2 3 4 # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) Distributed Training 1 2 3 4 5 6 7 8 9 10 # Create distributed sampler pinned to rank sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) # May be True # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, # Must be False!"><meta name=application-name content="Boda Blog"><meta name=apple-mobile-web-app-title content="Boda Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://bodasadalla98.github.io/blog/distributed_training/><link rel=prev href=https://bodasadalla98.github.io/blog/programming_facts/><link rel=stylesheet href=/blog/css/style.min.css><link rel=preload href=/blog/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/blog/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Distributed Training in PyTorch","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/bodasadalla98.github.io\/blog\/distributed_training\/"},"genre":"posts","keywords":"pytorch, python, distributed_training","wordcount":654,"url":"https:\/\/bodasadalla98.github.io\/blog\/distributed_training\/","datePublished":"2022-08-11T00:00:00+00:00","dateModified":"2022-08-11T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Baraa, Boda"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/blog/posts/>Posts </a><a class=menu-item href=/blog/tags/>Tags </a><a class=menu-item href=/blog/categories/>Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/blog/posts/ title>Posts</a><a class=menu-item href=/blog/tags/ title>Tags</a><a class=menu-item href=/blog/categories/ title>Categories</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Distributed Training in PyTorch</h1><h2 class=single-subtitle>Overview of some options available for multi[-node training in pytorch</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/BodaSadalla98 title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Baraa, Boda</a></span>&nbsp;<span class=post-category>included in <a href=/blog/categories/pytorch/><i class="far fa-folder fa-fw" aria-hidden=true></i>pytorch</a>&nbsp;<a href=/blog/categories/python/><i class="far fa-folder fa-fw" aria-hidden=true></i>python</a>&nbsp;<a href=/blog/categories/distributed_training/><i class="far fa-folder fa-fw" aria-hidden=true></i>distributed_training</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=08-11-2022>08-11-2022</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;654 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#why>Why?</a></li><li><a href=#ddp-in-pytorch>DDP in Pytorch</a></li><li><a href=#4-steps-recipe-to-distributed-training>4 steps-recipe to Distributed Training</a><ul><li><a href=#initialize-distributed-group>Initialize Distributed Group</a></li><li><a href=#data>Data</a></li><li><a href=#model>Model</a></li><li><a href=#saving-and-logging>Saving and Logging</a></li></ul></li><li><a href=#dp-vs-ddp>DP vs DDP</a><ul><li><a href=#dp>DP:</a></li><li><a href=#ddp>DDP:</a></li></ul></li><li><a href=#torchdistributedlaunch-vs-torchrun>Torch.Distributed.Launch vs Torchrun</a><ul><li><a href=#distributes-launch-scripts>Distributes Launch scripts:</a><ul><li><a href=#example>Example</a></li></ul></li><li><a href=#torchrun>Torchrun:</a><ul><li><a href=#example-1>Example:</a></li></ul></li></ul></li><li><a href=#hugging-face>Hugging Face</a><ul><li><a href=#option-1-using-hf-trainer>Option 1: using HF trainer</a></li><li><a href=#option-2-using-hf-accelerator>Option 2: using HF accelerator</a></li><li><a href=#pytorch-lightning>PyTorch Lightning</a></li></ul></li><li><a href=#common-errors>Common errors:</a><ul><li><a href=#dict-names-issue>Dict names issue:</a></li></ul></li></ul></nav></div></div><div class=content id=content><h1 id=distributed-training>Distributed Training</h1><h2 id=why>Why?</h2><ul><li><p>Need more compute power to process large batches in parallel (DDP)</p><ul><li>Uses collective communication</li></ul></li><li><p>Large model that couldn’t be fit in memory of one GPU (RPC)</p><ul><li>Uses P2P communication</li></ul></li><li><p>All of the above XD</p></li></ul><h2 id=ddp-in-pytorch>DDP in Pytorch</h2><ul><li>Every GPU has a model replica, controlled by a process.</li><li>Every process fetches different batch of data.</li><li>Forward.</li><li>Overlapping between computation of and communication(broadcast - allreduced) of gradient.</li><li>Validation</li></ul><p align=center><img src=ddp1.png width=900 height=400/></p><h2 id=4-steps-recipe-to-distributed-training>4 steps-recipe to Distributed Training</h2><h3 id=initialize-distributed-group>Initialize Distributed Group</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>init_process_group</span><span class=p>(</span><span class=n>backend</span><span class=o>=</span><span class=s1>&#39;nccl&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=data>Data</h3><ul><li>Local Training</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Wrap train dataset into DataLoader</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Distributed Training</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create distributed sampler pinned to rank</span>
</span></span><span class=line><span class=cl><span class=n>sampler</span> <span class=o>=</span> <span class=n>DistributedSampler</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>num_replicas</span><span class=o>=</span><span class=n>world_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>  <span class=c1># May be True</span>
</span></span><span class=line><span class=cl><span class=c1># Wrap train dataset into DataLoader</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>  <span class=c1># Must be False!</span>
</span></span><span class=line><span class=cl>                         <span class=n>sampler</span><span class=o>=</span><span class=n>sampler</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=model>Model</h3><ul><li>Local Training</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Initialize the model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>create_model</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Distributed Training</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Initialize the model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>create_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># Create CUDA device</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;cuda:</span><span class=si>{</span><span class=n>rank</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Send model parameters to the device</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Wrap the model in DDP wrapper</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>DistributedDataParallel</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=p>[</span><span class=n>rank</span><span class=p>],</span> <span class=n>output_device</span><span class=o>=</span><span class=n>rank</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=saving-and-logging>Saving and Logging</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>module</span><span class=o>.</span><span class=n>state_dict</span><span class=p>(),</span> <span class=s1>&#39;model.pt&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=dp-vs-ddp>DP vs DDP</h2><h3 id=dp>DP:</h3><ul><li>Can’t scale to multiple machines</li><li>Single Process, multiple threads</li><li>Module is replicated on each device, and the gradients are all summed into the original module</li><li>Doesn’t give the best performance, as a result of the GIL problem with multi-thread applications in python</li></ul><h3 id=ddp>DDP:</h3><ul><li>Can be used for Single machine, multiple GPUs training, or for multi-node training</li><li>It initiates process for every device (eg. 2 nodes, with 4 GPUs each = 8 processes)</li><li>Gradients are gathered using “all_reduce” operation</li></ul><p><em><strong>It’s advised to use DDP for any distributed training</strong></em></p><h2 id=torchdistributedlaunch-vs-torchrun>Torch.Distributed.Launch vs Torchrun</h2><h3 id=distributes-launch-scripts>Distributes Launch scripts:</h3><ul><li>We need to run the script on every node, with the correct ranks</li><li>We have to pass it all necessary environment variables</li></ul><h4 id=example>Example</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=nv>MASTER</span><span class=o>=</span><span class=k>$(</span>scontrol show hostnames <span class=s2>&#34;</span><span class=nv>$SLURM_JOB_NODELIST</span><span class=s2>&#34;</span> <span class=p>|</span> head -n 1<span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#Launch the pytorch processes, first on master (first in $HOSTLIST) then on the slaves</span>
</span></span><span class=line><span class=cl><span class=nv>RANK</span><span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> node in <span class=nv>$HOSTLIST</span><span class=p>;</span> <span class=k>do</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;RAnk is &#34;</span><span class=nv>$RANK</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>srun --nodes<span class=o>=</span><span class=m>1</span> --gpus<span class=o>=</span><span class=nv>$GPUS_PER_NODE</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>python -m torch.distributed.launch   <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--nproc_per_node<span class=o>=</span><span class=nv>$GPUS_PER_NODE</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--nnodes<span class=o>=</span><span class=nv>$NNODES</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--use_env <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--node_rank<span class=o>=</span><span class=nv>$RANK</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--master_addr<span class=o>=</span><span class=si>${</span><span class=nv>MASTER</span><span class=si>}</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--master_port<span class=o>=</span><span class=nv>$PORT</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>train.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> arg1 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> arg2  <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>sleep <span class=m>1</span>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;node is&#34;</span> <span class=nv>$node</span>
</span></span><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;node id is &#34;</span><span class=nv>$RANK</span>
</span></span><span class=line><span class=cl><span class=nv>RANK</span><span class=o>=</span><span class=k>$((</span>RANK+1<span class=k>))</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span><span class=line><span class=cl><span class=nb>wait</span>
</span></span></code></pre></td></tr></table></div></div><p><em><strong>Watch out for “&”</strong></em></p><h3 id=torchrun>Torchrun:</h3><ul><li>We run the script only once, and it runs it on all nodes</li><li>It adds all environment variables, and we can use them directly in the code</li></ul><h4 id=example-1>Example:</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>srun torchrun <span class=se>\ </span>
</span></span><span class=line><span class=cl>--nnodes<span class=o>=</span><span class=m>2</span> <span class=se>\ </span>
</span></span><span class=line><span class=cl>--nproc_per_node<span class=o>=</span><span class=nv>$GPUS</span> <span class=se>\ </span>
</span></span><span class=line><span class=cl>--rdzv_id<span class=o>=</span><span class=m>123456</span> <span class=se>\ </span>    // worker group ID 
</span></span><span class=line><span class=cl>--rdzv_backend<span class=o>=</span>c10d <span class=se>\ </span> // Communication library 
</span></span><span class=line><span class=cl>--rdzv_endpoint<span class=o>=</span><span class=si>${</span><span class=nv>MASTER</span><span class=si>}</span> <span class=se>\ </span>
</span></span><span class=line><span class=cl>test.py  
</span></span></code></pre></td></tr></table></div></div><p><strong>Overall, Torchrun removes a lot of mundane steps.</strong></p><h2 id=hugging-face>Hugging Face</h2><h3 id=option-1-using-hf-trainer>Option 1: using HF trainer</h3><ul><li>You can use HF trainer, but in this case, you need to manually run the training script on every node ( torch.distributed launch with for loop, or using torchrun once)</li></ul><h3 id=option-2-using-hf-accelerator>Option 2: using HF accelerator</h3><ul><li>You just need to run the script using the accelerate library.</li><li>You need to create the training loop manually, and can’t use HF trainer then</li></ul><p align=center><img src=hf.png width=700 height=300/></p><h3 id=pytorch-lightning>PyTorch Lightning</h3><ul><li>PyTorch Lightning is the easiest in running distributed training</li><li>We pass in the number of nodes, and number of GPUs per node to the trainer</li></ul><p align=center><img src=pl1.png width=700 height=300/></p><ul><li>Calls your script internally multiple times with the correct environment variables</li></ul><p align=center><img src=pl2.png width=700 height=300/></p><h2 id=common-errors>Common errors:</h2><h3 id=dict-names-issue>Dict names issue:</h3><ul><li><p>Problem: When we wrap our model with DDP, pytorch adds (module.dict_key) for all keys in the state_dict</p></li><li><p>Solution: We need to add a function, that detect if we are running distributed training or not, and add or delete “module” from all keys accordingly</p></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 08-11-2022</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/blog/distributed_training/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://bodasadalla98.github.io/blog/distributed_training/ data-title="Distributed Training in PyTorch" data-via=bodasdala data-hashtags=pytorch,python,distributed_training><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://bodasadalla98.github.io/blog/distributed_training/ data-hashtag=pytorch><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://bodasadalla98.github.io/blog/distributed_training/ data-title="Distributed Training in PyTorch"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://bodasadalla98.github.io/blog/distributed_training/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://bodasadalla98.github.io/blog/distributed_training/ data-title="Distributed Training in PyTorch"><i data-svg-src=/blog/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://bodasadalla98.github.io/blog/distributed_training/ data-title="Distributed Training in PyTorch"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/blog/tags/pytorch/>pytorch</a>,&nbsp;<a href=/blog/tags/python/>python</a>,&nbsp;<a href=/blog/tags/distributed_training/>distributed_training</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/blog/>Home</a></span></section></div><div class=post-nav><a href=/blog/programming_facts/ class=prev rel=prev title="Programming Facts"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Programming Facts</a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.102.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/BodaSadalla98 target=_blank>Boda Sadallah</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/blog/lib/katex/katex.min.css><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/blog/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/blog/lib/lunr/lunr.min.js></script><script type=text/javascript src=/blog/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/blog/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/blog/lib/sharer/sharer.min.js></script><script type=text/javascript src=/blog/lib/katex/katex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript src=/blog/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},cookieconsent:{content:{dismiss:"Got it!",link:"Learn more",message:"This website uses Cookies to improve your experience."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/blog/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/blog/js/theme.min.js></script></body></html>