<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>MIT 18.06 Linear Algebra course - Boda Blog</title><meta name=Description content="summarization of some Professor Gilbert Strang's MIT Linear Algebra course 18.06"><meta property="og:url" content="https://bodasadalla98.github.io/blog/1806_mit/">
<meta property="og:site_name" content="Boda Blog"><meta property="og:title" content="MIT 18.06 Linear Algebra course"><meta property="og:description" content="summarization of some Professor Gilbert Strang's MIT Linear Algebra course 18.06"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-02T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-02T00:00:00+00:00"><meta property="article:tag" content="MIT"><meta property="article:tag" content="Linear Algebra"><meta property="article:tag" content="Math"><meta property="og:image" content="https://bodasadalla98.github.io/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bodasadalla98.github.io/logo.png"><meta name=twitter:title content="MIT 18.06 Linear Algebra course"><meta name=twitter:description content="summarization of some Professor Gilbert Strang's MIT Linear Algebra course 18.06"><meta name=twitter:site content="@bodasadallah"><meta name=application-name content="Boda Blog"><meta name=apple-mobile-web-app-title content="Boda Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://bodasadalla98.github.io/blog/1806_mit/><link rel=next href=https://bodasadalla98.github.io/blog/computational_linear_algebra/><link rel=stylesheet href=/blog/css/style.min.css><link rel=preload href=/blog/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/blog/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"MIT 18.06 Linear Algebra course","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/bodasadalla98.github.io\/blog\/1806_mit\/"},"genre":"posts","keywords":"MIT, Linear Algebra, Math","wordcount":1761,"url":"https:\/\/bodasadalla98.github.io\/blog\/1806_mit\/","datePublished":"2021-10-02T00:00:00+00:00","dateModified":"2021-10-02T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Boda Sadallah"},"description":"summarization of some Professor Gilbert Strang's MIT Linear Algebra course 18.06"}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/blog/posts/>Posts </a><a class=menu-item href=/blog/tags/>Tags </a><a class=menu-item href=/blog/categories/>Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/blog/posts/ title>Posts</a><a class=menu-item href=/blog/tags/ title>Tags</a><a class=menu-item href=/blog/categories/ title>Categories</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">MIT 18.06 Linear Algebra course</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=@bodasadallah title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Boda Sadallah</a></span>&nbsp;<span class=post-category>included in <a href=/blog/categories/mit/><i class="far fa-folder fa-fw" aria-hidden=true></i>MIT</a>&nbsp;<a href=/blog/categories/linear-algebra/><i class="far fa-folder fa-fw" aria-hidden=true></i>Linear Algebra</a>&nbsp;<a href=/blog/categories/math/><i class="far fa-folder fa-fw" aria-hidden=true></i>Math</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=10-02-2021>10-02-2021</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1761 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;9 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#lecture-1>Lecture 1</a></li><li><a href=#lecture-2>Lecture 2</a></li><li><a href=#lecture-3>Lecture 3</a></li><li><a href=#lecture-4>Lecture 4</a></li><li><a href=#lecture-5>Lecture 5</a></li><li><a href=#lecture-6>Lecture 6</a></li><li><a href=#lecture-7>Lecture 7</a></li><li><a href=#lecture-8>Lecture 8</a></li><li><a href=#lecture-9>Lecture 9</a></li><li><a href=#lecture-10>Lecture 10</a></li><li><a href=#lecture-11>Lecture 11</a></li><li><a href=#lecture-12>Lecture 12</a></li><li><a href=#lecture-13>Lecture 13</a></li><li><a href=#lecture-14>Lecture 14</a></li><li><a href=#lecture-15>Lecture 15</a></li><li><a href=#lecture-16>Lecture 16</a></li><li><a href=#lecture-17>Lecture 17</a></li><li><a href=#lecture-18>Lecture 18</a></li><li><a href=#lecture-19>Lecture 19</a><ul><li><a href=#big-det-formula>Big Det Formula</a></li><li><a href=#cofactors>Cofactors</a><ul><li><a href=#cofactor-formula-along-row-1>cofactor formula (along row 1)</a></li></ul></li></ul></li><li><a href=#lecture-20>Lecture 20</a><ul><li><a href=#cramers-rule>Cramers rule</a></li><li><a href=#det-a--volume>Det A = Volume</a></li></ul></li><li><a href=#lecture-21-eigenvalues-and-eigenvectors>Lecture 21 Eigenvalues and Eigenvectors</a><ul><li><ul><li><a href=#fact-the-sum-of-the-eigenvalues--the-sum-of-the-diagonal-of-a>Fact: the sum of the eigenvalues = the sum of the diagonal of A</a></li></ul></li></ul></li><li><a href=#lecture-22-diagnolization>Lecture 22: Diagnolization</a></li><li><a href=#lecture-23>Lecture 23</a></li><li><a href=#lecture-24>Lecture 24</a><ul><li><a href=#markov-matrix>Markov Matrix</a></li><li><a href=#fourier-series>Fourier series</a></li></ul></li><li><a href=#lecture-25>Lecture 25</a><ul><li><a href=#symmetric-matrices>Symmetric matrices</a></li><li><a href=#every-symmetric-matrix-is-a-combination-of-perp-projection-matrices><strong>Every symmetric matrix is a combination of perp. projection matrices</strong></a></li><li><a href=#signs-of-pivots-are-the-same-as-the-sign-of-the-eigenvalues><strong>Signs of pivots are the same as the sign of the eigenvalues</strong></a></li></ul></li><li><a href=#positive-definite-symmetric-matrix>Positive definite symmetric matrix</a></li><li><a href=#lecture-26>Lecture 26</a><ul><li><a href=#complex-matrices>Complex Matrices</a></li><li><a href=#fourier-matrix>Fourier Matrix</a></li><li><a href=#fast-fourier-transform>Fast fourier transform</a></li></ul></li><li><a href=#lecture-27>Lecture 27</a></li><li><a href=#lecture-28>Lecture 28</a><ul><li><a href=#similar-matrices>similar matrices</a></li><li><a href=#jordan-form>Jordan form</a></li></ul></li><li><a href=#lecture-29>Lecture 29</a><ul><li><a href=#singular-value-composition-svd>Singular value composition (SVD)</a></li></ul></li><li><a href=#lecture-30>Lecture 30</a><ul><li><a href=#linear-transformation>Linear transformation</a></li></ul></li><li><a href=#lecture-31>Lecture 31</a><ul><li><a href=#change-of-basis>change of basis</a></li></ul></li><li><a href=#lecture-32>Lecture 32</a><ul><li><a href=#2-sided-inverse>2-sided inverse</a></li><li><a href=#left-inverse>left inverse</a></li><li><a href=#right-inverse>right inverse</a></li><li><a href=#pseudo-inverse-a>pseudo inverse $A^+$</a></li><li><a href=#lecute-34>Lecute 34</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 id=lecture-1>Lecture 1</h2><p>We learn about the big picture behind multiplication of matrix and vector</p><p>we learn about the row picture and column picture</p><h2 id=lecture-2>Lecture 2</h2><p>we learned about elimination method to solve a system of equations</p><h2 id=lecture-3>Lecture 3</h2><p>in this lecture we learned about matrices multiplication:</p><p>we can do that in five ways:</p><ol><li>row * col ==> gives an entry (1 cell)</li><li>col _ row ==> sum ( r1 _ c1 , r2 * c2, etc)</li><li>by columns ==> A * c1 = combination of A columns</li><li>by columns ==> r1 * B = combination of A B rows</li><li>by blocks ==> A (A1,A2,A3,A4) _ B (B1,B2,B3,B4) = C1 = (A1_ B1 + A2 * B3) and so on</li></ol><p>then we learned about gausian-Jordan elimination to find the matrix inverse</p><p>[A | I] ==> we apply elimination till we get [ I | A-1 ]</p><h2 id=lecture-4>Lecture 4</h2><p>in this lecture we learn about A= L U, where L is E^ -1, and whats special about this is that it has all multipliers in the lower triangular with ones on the diagonal</p><h2 id=lecture-5>Lecture 5</h2><p>we continued a little with permutations and moved to vector spaces
we learned about sub spaces and columns spaces ==> which is u take the columns of the matrix and all its combinations and that creates a plane through origin making a columns space</p><h2 id=lecture-6>Lecture 6</h2><p>In this lecture we continued about columns spaces and that we build those up by taking the combinations of all columns.
Then we learned about null spaces while are sub spaces of X that satisfies A X = 0</p><h2 id=lecture-7>Lecture 7</h2><p>in this lecture we continued about null space</p><p>then we learned about the special solution, where we assume the variables of the free vector then get the special solution</p><p>finally we learned about the reduced form</p><p>where R = [ I F
0 0 ]</p><p>and the null matrix is [ -F
I ]</p><p>then R N = 0</p><h2 id=lecture-8>Lecture 8</h2><p>in this lecture we expanded to talk about A x = b</p><p>and we find the whether there&rsquo;s a solution to the equation or not depends on the rank of the matrix</p><p>also we get the Xcomplete = Xparticular + Xnull space
and we get particular soln by putting all free variables = zero</p><p><img class=lazyload src=/blog/svg/loading.min.svg data-src=attachment:image.png data-srcset="attachment:image.png, attachment:image.png 1.5x, attachment:image.png 2x" data-sizes=auto alt=attachment:image.png title=image.png></p><h2 id=lecture-9>Lecture 9</h2><p>in this lecture we learned about independent columns and how they make a space, we also learned about Basis and what are two conditions for it</p><p>rank(A) = number of pivot columns of A = dimension of C(A).</p><p>dimension of N(A) = number of free variables = n − r,</p><h2 id=lecture-10>Lecture 10</h2><p>In this lecture we learned about the four subspaces</p><p>we also started in matrix space M</p><h2 id=lecture-11>Lecture 11</h2><p>We learned about matrix space
we take introduction about graph</p><h2 id=lecture-12>Lecture 12</h2><p>in this lecture we learned about graphs and how to represent them with matrices, then we applied that to electrical system and applied kerchofs law</p><h2 id=lecture-13>Lecture 13</h2><p>Quiz 1 review</p><h2 id=lecture-14>Lecture 14</h2><p>in this lecture learned about othrignilaity of the four vector spaces and what does it means</p><h2 id=lecture-15>Lecture 15</h2><p>in this lecture we learned about projection of matrices into subspaces</p><h2 id=lecture-16>Lecture 16</h2><p>we got example explaning the projection into subspaces and how to get the best fit using the least square error</p><h2 id=lecture-17>Lecture 17</h2><p>in this lecture le learned about orthonormal vectors and their special features and we learn how to produce them from any independent vectors using gram-schmeit</p><h2 id=lecture-18>Lecture 18</h2><p>Propertise of determants</p><h2 id=lecture-19>Lecture 19</h2><ul><li>det I =1</li><li>sign reverses with each row or colums exchange</li><li>det is linear in each row seperately</li></ul><h3 id=big-det-formula>Big Det Formula</h3><p>for a N * N matrix, we calc the sum of N! terms</p><p>$$detA=\sum_{i=1}^ N a1\alpha<em>a2\beta</em>a3\gamma*an\omega$$</p><p>(where $\alpha, \beta, &mldr; \omega $) = perm of (1,2,3, &mldr;, N )</p><h3 id=cofactors>Cofactors</h3><pre><code>cofactor of aij  =  Cij  =   +/-  det of ( n-1 matrix with column j, and  row i erased )
    it is plus if i+j is even, minus if i+j is odd
</code></pre><h4 id=cofactor-formula-along-row-1>cofactor formula (along row 1)</h4><pre><code>det A = a11 C11 + a12 C12 + .... + a1n C1n
</code></pre><h2 id=lecture-20>Lecture 20</h2><p>$ A^{-1} = 1/detA * C^T$ where C is the cofactors matrix</p><h3 id=cramers-rule>Cramers rule</h3><p>A x = b</p><p>x= A^ (-1) b = 1/detA C^T b</p><p>X_j = detB_j / detA where B_j is A matrix with column j replaced by b</p><h3 id=det-a--volume>Det A = Volume</h3><p>detA = volume of the shape created by making an edge from each of the rows</p><h2 id=lecture-21-eigenvalues-and-eigenvectors>Lecture 21 Eigenvalues and Eigenvectors</h2><p>Eigenvectors:
Ax is prallel to x ==> Ax = $\lambda$x</p><p>lambda ia the eigen values</p><p>if we have a plane:</p><ul><li>any x in the plane: Px= x ==> x is eigenvector and lambda = 1</li><li>any x perpendicular to plane Px = 0 ==> x is eigen vector and lambda = 0</li></ul><h4 id=fact-the-sum-of-the-eigenvalues--the-sum-of-the-diagonal-of-a>Fact: the sum of the eigenvalues = the sum of the diagonal of A</h4><h2 id=lecture-22-diagnolization>Lecture 22: Diagnolization</h2><p>to get power of matrix $A^k$</p><ul><li><p>first get the eigenvalues and vectors for A</p></li><li><p>then compute $A = S \lambda S^{-1}$ where S is the eigenvector matrix, and Lambda is diagonal matrix of the eigenvalues</p></li><li><p>then $A^k = S * \lambda^k*S^{-1}$</p></li></ul><h2 id=lecture-23>Lecture 23</h2><p>for the diffrential equations:</p><p>1- Stability
if lambda &lt; 0 ==> u(t) &ndash;> 0</p><p>2- Steady state
if lambda1 = 0, lambda2 &lt; 0</p><p>3- Blowup if any lambda > 0</p><h2 id=lecture-24>Lecture 24</h2><h3 id=markov-matrix>Markov Matrix</h3><p>1- All entries >= 0
2- The sum of every column is 1</p><p>3- lambda = 1 is eigenvalue</p><p>4- all othe lambda &lt; 1
5- eigenvector values >= 0</p><h3 id=fourier-series>Fourier series</h3><p>integration of ( f(x) g(x) dx ) from 0 to 2pi = 0</p><h2 id=lecture-25>Lecture 25</h2><h3 id=symmetric-matrices>Symmetric matrices</h3><p>$ A = A^T$</p><ul><li>the eigenvalues are real</li><li>the eigenvectors are perpendicular</li></ul><p>usual case:</p><p>$A = S \lambda S^{-1}$</p><p>symmetric case:</p><ul><li>we have orthonormal eigenvectors</li></ul><p>$A = Q \lambda Q^{-1} = Q \lambda Q^{-T} $</p><h3 id=every-symmetric-matrix-is-a-combination-of-perp-projection-matrices><strong>Every symmetric matrix is a combination of perp. projection matrices</strong></h3><h3 id=signs-of-pivots-are-the-same-as-the-sign-of-the-eigenvalues><strong>Signs of pivots are the same as the sign of the eigenvalues</strong></h3><p><strong>product of pivots = product of eigenvalues = det of matrix</strong></p><h2 id=positive-definite-symmetric-matrix>Positive definite symmetric matrix</h2><ul><li><p>all eigenvalues are positives</p></li><li><p>all pivots are positive</p></li><li><p>det is positive as it&rsquo;s the product of the eigenvalues</p></li><li><p>also all sub detemants are positive ( determants of lower matrices )</p></li><li><p>if S is pos. definite ==> $ X^T<em>S</em>X$ must be positive</p></li></ul><h2 id=lecture-26>Lecture 26</h2><h3 id=complex-matrices>Complex Matrices</h3><ul><li><p>we wanna utilize tha fact that $\bar{Z^T}*Z = \left|{Z}\right|^2$</p></li><li><p>Hermitian is biscally the conj. and transpose ==> $Z^H = \bar{Z^T}$</p></li><li><p>Hermitian Matricies : $A^H = A$</p></li><li><p>perpendicular: q1, q2, &mldr;, qn</p><ul><li>$\bar{qi}^T * qj = 0 if i!=j, 1 if i=j $</li><li>$Q^H*Q = I$</li></ul></li></ul><h3 id=fourier-matrix>Fourier Matrix</h3><ul><li><p>a matrix with entries are powers of some number W. where $W^n = 1$</p></li><li><p>$ F^H*F = I$</p></li></ul><h3 id=fast-fourier-transform>Fast fourier transform</h3><ul><li>reduces complexity from $N^2 to N log(N)$</li><li>$W_{2n}^2 = W_n ==> W_4^2 = W_2$</li></ul><h2 id=lecture-27>Lecture 27</h2><p>when det= 0 ==> then the matrix is positive semi-definite</p><p>f(x1,x2,x3&mldr;,xn) ==> min when the matrix of second derivatives is positive definite</p><ul><li>the eigenvalues tells us the length of the axis of the shape crated by cutting through the shape of the $X^T A X $</li><li>the direction of the eigenvectors is the direction of the axis of that shape</li></ul><h2 id=lecture-28>Lecture 28</h2><p>A is a m by n matrix ==> $ A^T*A$ is a positive definite symmetric matrix</p><h3 id=similar-matrices>similar matrices</h3><ul><li>A and B are similar means: for some M ==> $B = M^{-1}<em>A</em>M$</li><li><strong>Similar matrices has the same eigenvalues if the eigenvalues are unique</strong></li><li>the eigenvector of B is $M^{-1} * (eigenvector ofA)$</li></ul><h3 id=jordan-form>Jordan form</h3><ul><li>every square A is similar to Jordan matrix J</li><li>every jordan block has one eigenvaector</li><li>the number of jordan blocks = number of eigenvectors</li></ul><h2 id=lecture-29>Lecture 29</h2><ul><li>eigenvalues of (AB) = eigenvalues of (BA)</li></ul><h3 id=singular-value-composition-svd>Singular value composition (SVD)</h3><ul><li>$Av = \sigma u $</li><li>$ A = u \sigma v^T = u \sigma v^{-1}$</li><li>$A^T A = v \sigma^T u^Tu /sigma v^T = v \sigma^2 v^T$</li><li>$A A^T= u \sigma^T v^Tv /sigma u^T = u \sigma^2 u^T$</li></ul><h2 id=lecture-30>Lecture 30</h2><h3 id=linear-transformation>Linear transformation</h3><ul><li><p>examples: projection, rotation</p></li><li><p>if u know what transfotmation does to the basis of a plane, then u know what it does to every vector in the plane</p><ul><li>every $ v = c_1 v_1 + c_2 v_2 + &mldr; + c_n v_n$</li><li>then $T(v) = c_1 T(v_1) + &mldr;. + c_n T(v_n)$</li></ul></li><li><p>coordinates come from a basis (think of basis like the X-Y-Z axis</p></li><li><p>so we have a basis for the input and a basis for the output</p></li><li><p>Rule to find matrix A, given the input and output basis :</p><ul><li>input basis: v1 ===> vn</li><li>output basis: w1 ===> wm</li><li>1st column of A : write T(v1) = a11 w1 + a21 w2 + &mldr; + am1 wm</li><li>2nd column of A : write T(v2) = a12 w1 + a22 w2 + &mldr; + am2 wm</li><li>repeat that for the n columns</li></ul></li><li><p>A * (input coordinates) = (output coordinates)</p></li></ul><h2 id=lecture-31>Lecture 31</h2><h3 id=change-of-basis>change of basis</h3><ul><li>we have a new basis vectors and we wanna change to the new basis W</li><li>A = c _ W ==> c = W^-1 _ A</li><li>when we change the basis, every vector would have new coordinates ==> old coordinates = new basis * new coordinates ==> x = W c</li></ul><h2 id=lecture-32>Lecture 32</h2><h3 id=2-sided-inverse>2-sided inverse</h3><ul><li>$A A^{-1} = I = A^{-1} A$</li><li>r = m = n ==> full rank</li></ul><h3 id=left-inverse>left inverse</h3><ul><li>full column rank</li><li>r = n</li><li>nullspace = 0</li><li>then $A^T A $ is invertable</li><li>$A^{-1}_{left} = (A^T A)^{-1} A^T $</li><li>$A^{-1}_{left} A = I$</li></ul><h3 id=right-inverse>right inverse</h3><ul><li>full row rank</li><li>r = m</li><li>n-m free variables</li><li>left nullspace = 0</li></ul><h3 id=pseudo-inverse-a>pseudo inverse $A^+$</h3><p>$A^+ = v*\sigma^{-1}*u^T$</p><h3 id=lecute-34>Lecute 34</h3><ul><li>no solution ==> rank &lt; m</li><li>has one solution ==> there&rsquo;e no null space ==> rank = n</li><li>a matrix is invertable when there&rsquo;s no null space ==> r = n ==> indep. columns</li><li>positive definite matrix must have full rank ==> has no null space</li><li>positive def is invertable</li><li>the matrix has soln of any c when the matrix has full row rank</li><li>matix with orthogonal eigen vectors :<ul><li>symmetric matrices</li><li>skew-symmetric</li><li>orthogonal matrices</li></ul></li><li>in markov matrix<ul><li>the eigen values are one, and some sother values less than one</li><li>$k_m$ and m goes to infinity is the steady state we ge the eigenvector that corresponts to eigenvalue one asn multiply it with c, and notes that the sum of u is alwayes the same, so the sum of u0 is the sum of uk, so look what c achieves that</li></ul></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python></code></pre></td></tr></table></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 10-02-2021</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/blog/1806_mit/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://bodasadalla98.github.io/blog/1806_mit/ data-title="MIT 18.06 Linear Algebra course" data-via=bodasadallah data-hashtags="MIT,Linear Algebra,Math"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://bodasadalla98.github.io/blog/1806_mit/ data-hashtag=MIT><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://bodasadalla98.github.io/blog/1806_mit/ data-title="MIT 18.06 Linear Algebra course"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://bodasadalla98.github.io/blog/1806_mit/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://bodasadalla98.github.io/blog/1806_mit/ data-title="MIT 18.06 Linear Algebra course"><i data-svg-src=/blog/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://bodasadalla98.github.io/blog/1806_mit/ data-title="MIT 18.06 Linear Algebra course"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/blog/tags/mit/>MIT</a>,&nbsp;<a href=/blog/tags/linear-algebra/>Linear Algebra</a>,&nbsp;<a href=/blog/tags/math/>Math</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/blog/>Home</a></span></section></div><div class=post-nav><a href=/blog/computational_linear_algebra/ class=next rel=next title="Computational Linear Algebra">Computational Linear Algebra<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.135.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/BodaSadalla98 target=_blank>Boda Sadallah</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/blog/lib/katex/katex.min.css><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/blog/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/blog/lib/lunr/lunr.min.js></script><script type=text/javascript src=/blog/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/blog/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/blog/lib/sharer/sharer.min.js></script><script type=text/javascript src=/blog/lib/katex/katex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript src=/blog/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},cookieconsent:{content:{dismiss:"Got it!",link:"Learn more",message:"This website uses Cookies to improve your experience."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/blog/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/blog/js/theme.min.js></script></body></html>