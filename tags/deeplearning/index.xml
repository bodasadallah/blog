<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Deeplearning - Tag - Boda Blog</title><link>https://bodasadalla98.github.io/blog/tags/deeplearning/</link><description>Deeplearning - Tag - Boda Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Mon, 07 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/tags/deeplearning/" rel="self" type="application/rss+xml"/><item><title>Univnet</title><link>https://bodasadalla98.github.io/blog/univnet/</link><pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/univnet/</guid><description>&lt;h1 id="tts-text-to-speech">TTS (Text To Speech)&lt;/h1>
&lt;p>TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols
(text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1)
text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural
language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization,
part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word
sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech
synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized
speech waveform.&lt;/p></description></item><item><title>Deep Learning Papers Summarization</title><link>https://bodasadalla98.github.io/blog/papers/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/papers/</guid><description><![CDATA[<h2 id="decoupled-neural-interfaces-using-synthetic-gradients">Decoupled Neural Interfaces using Synthetic Gradients</h2>
<ul>
<li>In NN, the training process, has 3 bottle-necks
<ul>
<li>forward lock: you need to calculate teh output of the previous layer before you can can go into next layer in forward pass</li>
<li>backward pass: the same, but for backward propagation</li>
<li>weights lock: you can&rsquo;t update weights unless you do for weights in next layer</li>
</ul>
</li>
<li>the paper trying to unlock these bootle-necks by decoupling each layer, to be sufficient alone</li>
<li>it does that by introducing, a Synthetic Gradient Model, that can predict the gradient for the current layer, without waiting for the gradient of the next layer</li>
<li>this was we can calculate gradient and update weights as soon as we calculate the activation of the current layer</li>
</ul>
<h3 id="synthetic-gradient-model">Synthetic Gradient Model</h3>
<ul>
<li>
<p>can be just a simple NN that is trained to output the gradient of the layer</p>]]></description></item><item><title>Stanford CS224N NLP with Deep Learning</title><link>https://bodasadalla98.github.io/blog/stanford_nlp_cs224n/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/stanford_nlp_cs224n/</guid><description>Post for Stanford NLP Course</description></item><item><title>Applied Deep Learning</title><link>https://bodasadalla98.github.io/blog/applied_deep_learning/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/applied_deep_learning/</guid><description><![CDATA[<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://github.com/maziarraissi/Applied-Deep-Learning" target="_blank" rel="noopener noreffer ">Course</a></li>
<li><a href="https://github.com/maziarraissi/Applied-Deep-Learning" target="_blank" rel="noopener noreffer ">Repo with the slides, and course info</a></li>
</ul>
<h2 id="deep-learning-overview">Deep Learning overview</h2>
<ul>
<li>we can look at deep learning as an algorithm that writes algorithms, like a compiler</li>
</ul>
<ul>
<li>in this case the source code would be the data: (examples/experiences)</li>
<li>excutable code would be the deployable model</li>
</ul>
<ul>
<li>
<p>Deep: Functions compositions $ f<em>l f</em>{l-1} &hellip;. f_1$</p>
</li>
<li>
<p>Learning: Loss, Back-propagation, and Gradient Descent</p>
</li>
<li>
<p>$ L(\theta) \approx J(\theta)$ &ndash;&gt; noisy estimate of the objective function due to mini-batching. That&rsquo;s why we call it stochastic Gradient Descent</p>]]></description></item><item><title>NLP Specialization</title><link>https://bodasadalla98.github.io/blog/coursera_nlp_specialization/</link><pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/coursera_nlp_specialization/</guid><description><![CDATA[<h1 id="course-1-classification-and-vector-spaces">Course 1: Classification and vector Spaces</h1>
<h2 id="weak-4">Weak 4</h2>
<h3 id="hashing">Hashing</h3>
<ul>
<li>We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space</li>
</ul>
<h4 id="locality-senstive-hashing">Locality senstive hashing</h4>
<ul>
<li>
<p>the idea is to put items that are close in the vector space, in the same hashing buckets</p>
</li>
<li>
<p>we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly</p>]]></description></item><item><title>CS480/680 Intro to Machine Learning</title><link>https://bodasadalla98.github.io/blog/introduction_to_machine_learning/</link><pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/introduction_to_machine_learning/</guid><description><![CDATA[<h2 id="lecture-12">Lecture 12</h2>
<h3 id="gausain-process">Gausain process</h3>
<ul>
<li>infinite dimentional gaussian distribution</li>
</ul>
<h2 id="lecture-16">Lecture 16</h2>
<h3 id="convolution-nn">Convolution NN</h3>
<ul>
<li>a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters</li>
</ul>
<h3 id="residual-networks">Residual Networks</h3>
<ul>
<li>even after using Relu, NN can still suffer from gradient vanishing</li>
<li>the idea in to add skip connections so that we can create shorter paths</li>
</ul>
<h2 id="lecture-18">Lecture 18</h2>
<h3 id="lstm-vs-gru-vs-attention">LSTM vs GRU vs Attention</h3>
<ul>
<li>LSTM: 3 gates, one for the cell state, one for the input, one for the output</li>
<li>GRU: only two states, one for output, and one for taking weighted probablitiy for the contribution of the input and the hidden state
<ul>
<li>takes less parameters</li>
</ul>
</li>
<li>Attention: at every step of producing the output, create a new context vector that gives more attention to the importat input tokens for this output token</li>
</ul>
<h2 id="lecture-20">Lecture 20</h2>
<h3 id="autoencoder">Autoencoder</h3>
<p>takes different input and generates the same output</p>]]></description></item><item><title>Scribbles</title><link>https://bodasadalla98.github.io/blog/scribbles/</link><pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/scribbles/</guid><description><![CDATA[<h2 id="transformers">Transformers</h2>
<p>To deal with sequential data we have to options:</p>
<ul>
<li>1-D convolution NN
<ul>
<li>processing can be parallel</li>
<li>not practical for long sequences</li>
</ul>
</li>
<li>Recurrent NN
<ul>
<li>can&rsquo;t happen in prallel</li>
<li>have gradient vanshing problem of the squence becomes so long</li>
<li>we have bottleneck at the end of the encoder</li>
</ul>
</li>
<li>RNN with attention mechanism
<ul>
<li>to solve the bottleneck problem, we make Encoder-Decoder attention</li>
<li>Decoder utilzes:
<ul>
<li>context vector</li>
<li>weighted sum of hidden states (h1,h2, &hellip; ) from the encoder</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="transformers-1">Transformers</h3>
<h4 id="encoder">Encoder</h4>
<ul>
<li>first we do input embedding, and positional embedding</li>
<li>in self attention: we multiply q,w,v by a matrix to do lenear transformation</li>
<li>self attentoion: k _ q &ndash;&gt; scaling down &ndash;&gt; softmax &ndash;&gt; _ v</li>
</ul>
<h3 id="multi-head-attention">multi-head attention</h3>
<ul>
<li>works as we use many filters in CNN</li>
<li>in wide attention: it takes every word and spread it multi-head attention</li>
<li>in narrow attention: it take every word and split it up across the multi-head
<ul>
<li>but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?</li>
</ul>
</li>
</ul>
<h3 id="positional-info">Positional info</h3>
<ul>
<li>positional encoding using the rotation sin/cos matrix</li>
<li>positional embedding</li>
</ul>
<h3 id="residual-connections">Residual connections</h3>
<ul>
<li>to give the chance to skip some learning parameters if it&rsquo;s better to minimize the loss</li>
</ul>
<h3 id="layer-normalization">Layer Normalization</h3>
<ul>
<li>in batch normalization
<ul>
<li>==&gt; we normalize to zero mean and unity varince</li>
<li>we calculate for all samples in each batch (for each channel )</li>
</ul>
</li>
<li>in layer normalization
<ul>
<li>==&gt; $y = \gamma * x + \beta $ where gamm and bata are trainable parametes</li>
<li>calculates for all channles in the same sample</li>
</ul>
</li>
<li>in instance normalization ==&gt; calculate for one channel in one sample</li>
</ul>
<h2 id="debugging-ml-models">Debugging ML Models</h2>
<ul>
<li>
<p>Understand bias-variance diagnoses</p>]]></description></item><item><title>TTS Research</title><link>https://bodasadalla98.github.io/blog/text_to_speech/</link><pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/text_to_speech/</guid><description>&lt;h3 id="tts">TTS&lt;/h3>
&lt;p>TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols
(text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1)
text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural
language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization,
part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word
sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech
synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized
speech waveform.&lt;/p></description></item></channel></rss>