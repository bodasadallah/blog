<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>A glimpse into PyTorch Autograd internals - Boda Blog</title><meta name=Description content="Trying to understand some errors we get, while using pyTorch"><meta property="og:title" content="A glimpse into PyTorch Autograd internals"><meta property="og:description" content="Trying to understand some errors we get, while using pyTorch"><meta property="og:type" content="article"><meta property="og:url" content="https://bodasadalla98.github.io/blog/pytorch_internals/"><meta property="og:image" content="https://bodasadalla98.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-10T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-10T00:00:00+00:00"><meta property="og:site_name" content="Boda Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bodasadalla98.github.io/logo.png"><meta name=twitter:title content="A glimpse into PyTorch Autograd internals"><meta name=twitter:description content="Trying to understand some errors we get, while using pyTorch"><meta name=application-name content="Boda Blog"><meta name=apple-mobile-web-app-title content="Boda Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://bodasadalla98.github.io/blog/pytorch_internals/><link rel=prev href=https://bodasadalla98.github.io/blog/univnet/><link rel=stylesheet href=/blog/css/style.min.css><link rel=preload href=/blog/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/blog/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"A glimpse into PyTorch Autograd internals","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/bodasadalla98.github.io\/blog\/pytorch_internals\/"},"genre":"posts","keywords":"pytorch, python, autograd","wordcount":863,"url":"https:\/\/bodasadalla98.github.io\/blog\/pytorch_internals\/","datePublished":"2023-01-10T00:00:00+00:00","dateModified":"2023-01-10T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Boda Sadallah"},"description":"Trying to understand some errors we get, while using pyTorch"}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/blog/posts/>Posts </a><a class=menu-item href=/blog/tags/>Tags </a><a class=menu-item href=/blog/categories/>Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/blog/posts/ title>Posts</a><a class=menu-item href=/blog/tags/ title>Tags</a><a class=menu-item href=/blog/categories/ title>Categories</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">A glimpse into PyTorch Autograd internals</h1><h2 class=single-subtitle>Trying to understand some errors we get, while using pyTorch</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://twitter.com/bodasadallah title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Boda Sadallah</a></span>&nbsp;<span class=post-category>included in <a href=/blog/categories/pytorch/><i class="far fa-folder fa-fw" aria-hidden=true></i>pytorch</a>&nbsp;<a href=/blog/categories/python/><i class="far fa-folder fa-fw" aria-hidden=true></i>python</a>&nbsp;<a href=/blog/categories/autograd/><i class="far fa-folder fa-fw" aria-hidden=true></i>autograd</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=01-10-2023>01-10-2023</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;863 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;5 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#notes>Notes</a></li><li><a href=#normal-training>Normal training</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=content id=content><h2 id=intro>Intro</h2><p>Here, we are going to discuss the internals of PyTorch <code>Autograd</code> module. The most of us don&rsquo;t have to know about this. I was the same till I came across this error:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>unsupported operand type(s) for *: &#39;float&#39; and &#39;NoneType&#39;
</span></span></code></pre></td></tr></table></div></div><p>This came from executing the following code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>5.0</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl><span class=n>c</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>+=</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>a</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><p>But why? We defined that the gradient of <code>a</code> should be calculated by putting <code>requires_grad</code> to be <code>True</code>!</p><p>After some investigation, the error was due to that <code>a.grad</code> is <code>None</code>. But why is that the case? When we multiplied <code>a</code> by <code>0.1</code> we internally created a new tensor that is <code>intermediate</code>. And by default, PyTorch only populate gradients for <code>leaf</code> tensors only.</p><h2 id=notes>Notes</h2><p>These are some notes to help understand the internals of PyTorch, and its Autograd module, and why did we get this error.</p><p><strong>To know more about this, in a more structured way,read this blog post <a href=https://medium.com/@mrityu.jha/understanding-the-grad-of-autograd-fc8d266fd6cf target=_blank rel="noopener noreffer">here</a></strong></p><ul><li><p><code>requires_grad</code> tells PyTorch, if it should save the forward results, to use it to calculate the gradients.</p></li><li><p>All tensors with <code>requires_grad</code> is <code>False</code> are leafs tensors.</p></li><li><p>Tensors with <code>requires_grad</code> is <code>True</code> are leafs only if they are created by the user, and not as a result by a mathematical expression for example.</p></li><li><p>When a tensor is created as a result of an operation on a few tensors, then it would be <code>leaf</code> if and only if all the tensor used to generate it has <code>requires_grad</code> set to <code>False</code>, as only then, the <code>requires_grad</code> for this tensor will be set to <code>False</code>, and as we mentioned above, it will be a leaf.</p></li><li><p>For a tensor to have <code>tensor.grad</code> populated, it must have <code>requires_grad</code> to be <code>True</code>, and it must be a <code>leaf</code>.</p></li><li><p>You can&rsquo;t change the <code>requires_grad</code> for <code>non-leaf</code> tensors. They are set automatically according the tensors used in the operation creating them</p></li><li><p>PyTorch doesn&rsquo;t allow updates(in-place) on leaf tensors if the <code>requires_grad</code> is set to <code>True</code>, as it causes troubles in the backward pass.</p></li><li><p>What does it mean, that Autograd will not populate the gradient for a tensor?</p><ul><li>Here&rsquo;s what the backward graph does while calculating and propagating the gradient:<ul><li>If this tensor has <code>requires_grad</code> set to <code>False</code>, then this tensor is not part of the backward graph, and it will do nothing.</li><li>If the tensor has <code>requires_grad</code> set to <code>True</code>, and <code>is_leaf</code> set to <code>False</code>, this means that this is an <code>intermediate</code> tensor that came off an operation. In this case, Autograd doesn&rsquo;t populate the <code>grad</code> attribute for this tensor, and just propagate the gradient to the operation that generated this tensor (to <code>grad_fn</code>.)</li><li>Lastly, if the tensor is a <code>leaf</code>, and has <code>requires_grad</code> set to <code>True</code>, then Autograd calculates the accumulated grad value, and puts it in <code>grad</code> field for the tensor.</li></ul></li><li>There&rsquo;s an amazing video explaining this <a href="https://www.youtube.com/watch?v=MswxJw-8PvE" target=_blank rel="noopener noreffer">here</a></li></ul></li></ul><p>Now for the Solutions:</p><p>There are two solutions for this:</p><ol><li>call <code>a.retain_grad()</code>, which tells PyTorch to keep the grad for this tensor anyway.<ul><li>Note: this solution is not fast.</li></ul></li><li>remove <code>requires_grad</code> from the initialization of the tensor, and then do it explicitly in a separate step.</li><li>wrap the weight update under <code>torch.no_grad</code>, and I will explain why we need this step below.</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>5.0</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>a</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl><span class=n>c</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span> <span class=o>+=</span> <span class=o>+</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>a</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=normal-training>Normal training</h2><p>Here&rsquo;s another script for a training a network with just one neuron. Our normal training system, would be something like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Input</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># weight</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>3.0</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># bias</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># forward pass</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span> <span class=o>=</span> <span class=n>a</span> <span class=o>*</span> <span class=n>w</span> <span class=o>+</span>  <span class=n>b</span>
</span></span><span class=line><span class=cl>    <span class=c1># backward pass</span>
</span></span><span class=line><span class=cl>    <span class=n>c</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># weight update</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>=</span>   <span class=n>w</span> <span class=o>+</span>  <span class=mf>0.1</span><span class=o>*</span><span class=n>w</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><p>This will not work!!</p><p>As we said, the weight update line, will produce an <code>intermediate</code> tensor, which the <code>grad</code> will not be populated for.
That&rsquo;s why we all wrap our weight updating steps under <code>torch.no_grad()</code> context, and do in-place calculation,which tells PyTorch not to keep track of these operations in the grad calculations, and thus the weights tensor will remain a <code>leaf</code> tensor.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>w</span> <span class=o>+=</span>   <span class=mf>0.1</span><span class=o>*</span><span class=n>w</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>so we use <code>torch.no_grad()</code> for two main reasons:<ol><li>we don&rsquo;t need to include the weight update operation in the gradient graph.<ul><li>although, we zero our gradient before every backward pass, but this weight update step will add a new branch in the gradient calculations for the weights tensors,and this will mess with the calculations.</li></ul></li><li>we need to be able to do <code>in-place</code> operation, and so, our tensor will remain a <code>leaf</code>, and Autograd will keep populating its gradient.</li></ol></li></ul><h2 id=conclusion>Conclusion</h2><p>At last, we usually, use pre-built optimizers for the training step, or we even use a <code>trainer</code>, which make the things way easier.
This was just a try to go back to the basics and trying to understand the roots of the problem.</p><h2 id=references>References</h2><ul><li><a href=https://PyTorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf target=_blank rel="noopener noreffer">https://PyTorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf</a></li><li><a href="https://www.youtube.com/watch?v=MswxJw-8PvE" target=_blank rel="noopener noreffer">https://www.youtube.com/watch?v=MswxJw-8PvE</a></li><li><a href=https://medium.com/@mrityu.jha/understanding-the-grad-of-Autograd-fc8d266fd6cf target=_blank rel="noopener noreffer">https://medium.com/@mrityu.jha/understanding-the-grad-of-Autograd-fc8d266fd6cf</a></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 01-10-2023</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/blog/pytorch_internals/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://bodasadalla98.github.io/blog/pytorch_internals/ data-title="A glimpse into PyTorch Autograd internals" data-via=bodasadallah data-hashtags=pytorch,python,autograd><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://bodasadalla98.github.io/blog/pytorch_internals/ data-hashtag=pytorch><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://bodasadalla98.github.io/blog/pytorch_internals/ data-title="A glimpse into PyTorch Autograd internals"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://bodasadalla98.github.io/blog/pytorch_internals/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://bodasadalla98.github.io/blog/pytorch_internals/ data-title="A glimpse into PyTorch Autograd internals"><i data-svg-src=/blog/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://bodasadalla98.github.io/blog/pytorch_internals/ data-title="A glimpse into PyTorch Autograd internals"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/blog/tags/pytorch/>pytorch</a>,&nbsp;<a href=/blog/tags/python/>python</a>,&nbsp;<a href=/blog/tags/autograd/>autograd</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/blog/>Home</a></span></section></div><div class=post-nav><a href=/blog/univnet/ class=prev rel=prev title=Univnet><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Univnet</a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.109.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/BodaSadalla98 target=_blank>Boda Sadallah</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/blog/lib/katex/katex.min.css><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/blog/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/blog/lib/lunr/lunr.min.js></script><script type=text/javascript src=/blog/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/blog/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/blog/lib/sharer/sharer.min.js></script><script type=text/javascript src=/blog/lib/katex/katex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript src=/blog/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},cookieconsent:{content:{dismiss:"Got it!",link:"Learn more",message:"This website uses Cookies to improve your experience."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/blog/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/blog/js/theme.min.js></script></body></html>