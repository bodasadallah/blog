<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Distributed_training - Category - Boda Blog</title><link>https://bodasadalla98.github.io/blog/categories/distributed_training/</link><description>Distributed_training - Category - Boda Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Thu, 11 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/categories/distributed_training/" rel="self" type="application/rss+xml"/><item><title>Distributed Training in PyTorch</title><link>https://bodasadalla98.github.io/blog/distributed_training/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><author>Baraa, Boda</author><guid>https://bodasadalla98.github.io/blog/distributed_training/</guid><description><![CDATA[<h1 id="distributed-training">Distributed Training</h1>
<h2 id="why">Why?</h2>
<ul>
<li>
<p>Need more compute power to process large batches in parallel (DDP)</p>
<ul>
<li>Uses collective communication</li>
</ul>
</li>
<li>
<p>Large model that couldnâ€™t be fit in memory of one GPU (RPC)</p>
<ul>
<li>Uses P2P communication</li>
</ul>
</li>
<li>
<p>All of the above XD</p>
</li>
</ul>
<h2 id="ddp-in-pytorch">DDP in Pytorch</h2>
<ul>
<li>Every GPU has a model replica, controlled by a process.</li>
<li>Every process fetches different batch of data.</li>
<li>Forward.</li>
<li>Overlapping between computation of  and communication(broadcast - allreduced) of gradient.</li>
<li>Validation</li>
</ul>
<p align="center"> </p>]]></description></item></channel></rss>