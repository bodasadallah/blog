<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>NLP Specialization - Boda Blog</title><meta name=Description content="This is my personal Blog, I write about random things that comes to my mind!"><meta property="og:url" content="https://bodasadalla98.github.io/blog/coursera_nlp_specialization/">
<meta property="og:site_name" content="Boda Blog"><meta property="og:title" content="NLP Specialization"><meta property="og:description" content="Course 1: Classification and vector Spaces Weak 4 Hashing We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space Locality senstive hashing the idea is to put items that are close in the vector space, in the same hashing buckets
we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-30T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-30T00:00:00+00:00"><meta property="article:tag" content="Deeplearning"><meta property="article:tag" content="Python"><meta property="article:tag" content="NLP"><meta property="og:image" content="https://bodasadalla98.github.io/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://bodasadalla98.github.io/logo.png"><meta name=twitter:title content="NLP Specialization"><meta name=twitter:description content="Course 1: Classification and vector Spaces Weak 4 Hashing We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space Locality senstive hashing the idea is to put items that are close in the vector space, in the same hashing buckets
we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly"><meta name=twitter:site content="@bodasadallah"><meta name=application-name content="Boda Blog"><meta name=apple-mobile-web-app-title content="Boda Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/><link rel=prev href=https://bodasadalla98.github.io/blog/introduction_to_machine_learning/><link rel=next href=https://bodasadalla98.github.io/blog/applied_deep_learning/><link rel=stylesheet href=/blog/css/style.min.css><link rel=preload href=/blog/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/blog/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/blog/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"NLP Specialization","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/bodasadalla98.github.io\/blog\/coursera_nlp_specialization\/"},"genre":"posts","keywords":"deeplearning, python, NLP","wordcount":586,"url":"https:\/\/bodasadalla98.github.io\/blog\/coursera_nlp_specialization\/","datePublished":"2022-01-30T00:00:00+00:00","dateModified":"2022-01-30T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Boda Sadallah"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"dark"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"dark"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/blog/posts/>Posts </a><a class=menu-item href=/blog/tags/>Tags </a><a class=menu-item href=/blog/categories/>Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/blog/ title="Boda Blog"></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/blog/posts/ title>Posts</a><a class=menu-item href=/blog/tags/ title>Tags</a><a class=menu-item href=/blog/categories/ title>Categories</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">NLP Specialization</h1><h2 class=single-subtitle>Notebook for coursera's NLP Specialization</h2><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://twitter.com/bodasadallah title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Boda Sadallah</a></span>&nbsp;<span class=post-category>included in <a href=/blog/categories/deeplearning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Deeplearning</a>&nbsp;<a href=/blog/categories/python/><i class="far fa-folder fa-fw" aria-hidden=true></i>Python</a>&nbsp;<a href=/blog/categories/nlp/><i class="far fa-folder fa-fw" aria-hidden=true></i>NLP</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=01-30-2022>01-30-2022</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;586 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept=true><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#weak-4>Weak 4</a><ul><li><a href=#hashing>Hashing</a><ul><li><a href=#locality-senstive-hashing>Locality senstive hashing</a></li></ul></li></ul></li></ul><ul><li><a href=#week-1-autocorrect>Week 1: Autocorrect</a></li><li><a href=#week-2-pos-using-viterbi-algorithm>Week 2: POS using Viterbi algorithm</a><ul><li><a href=#markov-chains>Markov Chains:</a></li><li><a href=#viterbi-algorithm>Viterbi algorithm</a></li></ul></li><li><a href=#week3-autocomplete>Week3 Autocomplete</a><ul><li><a href=#n-gram-models>N-Gram models:</a></li><li><a href=#preplexity>Preplexity:</a></li><li><a href=#out-of-vocab-words-and-smoothing>out of vocab words and smoothing</a></li></ul></li><li><a href=#week4-word-vectors-using-bag-of-words-method>Week4: Word vectors using Bag of Words method</a><ul><li><a href=#continuous-bag-of-words-model>Continuous Bag-of-Words Model</a><ul><li><a href=#evaluations>Evaluations</a></li></ul></li></ul></li></ul></nav></div></div><div class=content id=content><h1 id=course-1-classification-and-vector-spaces>Course 1: Classification and vector Spaces</h1><h2 id=weak-4>Weak 4</h2><h3 id=hashing>Hashing</h3><ul><li>We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space</li></ul><h4 id=locality-senstive-hashing>Locality senstive hashing</h4><ul><li><p>the idea is to put items that are close in the vector space, in the same hashing buckets</p></li><li><p>we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly</p></li><li><p>but how can we be sure, that the planes that we chose are the perfect set to seperate our space, we can&rsquo;t be sure of that, so we create a multi sets of random planes and every set would get us a different way to seperate our words</p></li></ul><h1 id=course-2-probabilstic-models>Course 2: Probabilstic Models</h1><h2 id=week-1-autocorrect>Week 1: Autocorrect</h2><ul><li>to make a simple auto-correction system, you need to perform 4 simple steps:<ul><li>you need to identify the miss spelled words</li><li>get the n edit distance correct words</li><li>filter these candidates for correct words in the dictionary</li><li>change miss spelled word with one that has the highest probability</li></ul></li></ul><h2 id=week-2-pos-using-viterbi-algorithm>Week 2: POS using Viterbi algorithm</h2><ul><li>Part of Speech Tagging (POS) is the process of assigning a part of speech to a word</li><li>You can use part of speech tagging for:<ul><li>Identifying named entities</li><li>Speech recognition</li><li>Co-reference Resolution</li></ul></li><li>You can use the probabilities of POS tags happening near one another to come up with the most reasonable output.</li></ul><h3 id=markov-chains>Markov Chains:</h3><ul><li>You can use Markov chains to identify the probability of the next word.</li><li>calculate transmission probability</li><li>calculate emission probability</li></ul><h3 id=viterbi-algorithm>Viterbi algorithm</h3><ul><li>calculates a probability for each possible path</li><li>a probability for a given state, is the (transition probability * emission probability)</li><li>total probability of the path, is to product of all states probabilities</li><li>we use a top down dynamic programming algorithm to build the paths matrix</li><li>we use sum of logs instead of product of probabilities, to avoid converging to zero values</li></ul><h2 id=week3-autocomplete>Week3 Autocomplete</h2><h3 id=n-gram-models>N-Gram models:</h3><ul><li>it&rsquo;s a language model that predicts probabilities of sentences depending on the probabilities of their N-grams</li><li>to capture the context of beginning end ending of the sentences, we add start and end tokens to each sentence</li></ul><h3 id=preplexity>Preplexity:</h3><ul><li>a measure to calculate how complex a sentence is</li><li>humans type low preplex sentences</li></ul><h3 id=out-of-vocab-words-and-smoothing>out of vocab words and smoothing</h3><ul><li>we can add UNK token for unseen words in the vocab</li><li>we can apply smoothing of interpolation for unseen Ngrams</li></ul><h2 id=week4-word-vectors-using-bag-of-words-method>Week4: Word vectors using Bag of Words method</h2><ul><li><p>we can use self-supervised learning in predicting the next word, to learn the weight matrix</p></li><li><p>word2vec</p><ul><li>continuous bag-of-words<ul><li>predicts a word in context</li></ul></li><li>continuous skip-gram<ul><li>tries to predict the words surrounding input word</li></ul></li></ul></li><li><p>Global Vectors (GloVe)</p><ul><li>factorizes the corpus word co-occurrence matrix</li></ul></li><li><p>fastText</p><ul><li>based on skip-gram model</li><li>support out-of-vocab words</li></ul></li><li><p>Advanced word embedding methods</p><ul><li>Deep Learning, contextual embedding</li><li>BERT</li><li>ELMO</li><li>GPT-2</li></ul></li></ul><h3 id=continuous-bag-of-words-model>Continuous Bag-of-Words Model</h3><ul><li>you choose a center word, and a context words around it, and try to predict the centered word</li><li>we model the words by one-hot encoding</li><li>then we prepare the input training example feature to be an average of the one-hot vectors of the context words, and the label would be one-hot vector of the center word</li><li>after training the model, the word embedding is one of the weight matrices, or an average of them</li></ul><h4 id=evaluations>Evaluations</h4><ul><li>Intrinsic evaluation<ul><li>test the relationships between words</li><li>Analogies</li><li>Clustering</li><li>Visualizing</li></ul></li><li>Extrinsic evaluation<ul><li>test the embedding on the end task you want to perform (ex. Sentiment Analysis)</li><li>evaluates actual usefulness of embeddings</li><li>time consuming</li><li>more difficult to troubleshoot</li></ul></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 01-30-2022</span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/blog/coursera_nlp_specialization/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/ data-title="NLP Specialization" data-via=bodasadallah data-hashtags=deeplearning,python,NLP><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/ data-hashtag=deeplearning><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/ data-title="NLP Specialization"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Reddit" data-sharer=reddit data-url=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/><i class="fab fa-reddit fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/ data-title="NLP Specialization"><i data-svg-src=/blog/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://bodasadalla98.github.io/blog/coursera_nlp_specialization/ data-title="NLP Specialization"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/blog/tags/deeplearning/>Deeplearning</a>,&nbsp;<a href=/blog/tags/python/>Python</a>,&nbsp;<a href=/blog/tags/nlp/>NLP</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/blog/>Home</a></span></section></div><div class=post-nav><a href=/blog/introduction_to_machine_learning/ class=prev rel=prev title="CS480/680 Intro to Machine Learning"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>CS480/680 Intro to Machine Learning</a>
<a href=/blog/applied_deep_learning/ class=next rel=next title="Applied Deep Learning">Applied Deep Learning<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.135.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/BodaSadalla98 target=_blank>Boda Sadallah</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/blog/lib/katex/katex.min.css><link rel=stylesheet href=/blog/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/blog/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/blog/lib/lunr/lunr.min.js></script><script type=text/javascript src=/blog/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/blog/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/blog/lib/sharer/sharer.min.js></script><script type=text/javascript src=/blog/lib/katex/katex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/blog/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript src=/blog/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},cookieconsent:{content:{dismiss:"Got it!",link:"Learn more",message:"This website uses Cookies to improve your experience."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/blog/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/blog/js/theme.min.js></script></body></html>